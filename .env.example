# =============================================================================
# FastAPI Streamlit LLM Starter - Environment Configuration Examples
# =============================================================================
#
# This file contains example environment variable configurations for the
# FastAPI backend application. Copy this file to .env and customize the
# values for your specific environment.
#
# IMPORTANT: Never commit the actual .env file to version control.
# This .env.example file serves as a template and documentation.


# =============================================================================
# CORE APPLICATION SETTINGS
# =============================================================================

# Application environment (development, staging, production)
ENVIRONMENT=development

# Enable debug mode (true/false)
DEBUG=true

# Application logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO


# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Backend server host (0.0.0.0 for all interfaces, 127.0.0.1 for local only)
BACKEND_HOST=0.0.0.0

# Backend server port
BACKEND_PORT=8000

# Frontend server port (for Streamlit)
FRONTEND_PORT=8501


# =============================================================================
# AI MODEL CONFIGURATION
# =============================================================================

# Google Gemini API key (required for AI text processing features)
GEMINI_API_KEY=your-gemini-api-key-here

# AI model to use (e.g., gemini-2.0-flash-exp, gemini-1.5-pro)
AI_MODEL=gemini-2.0-flash-exp

# AI model temperature (0.0-1.0, higher = more creative/random)
AI_TEMPERATURE=0.7


# =============================================================================
# BATCH PROCESSING CONFIGURATION
# =============================================================================

# Maximum number of requests per batch call
MAX_BATCH_REQUESTS_PER_CALL=50

# AI concurrency limit for batch processing
BATCH_AI_CONCURRENCY_LIMIT=5


# =============================================================================
# INPUT VALIDATION
# =============================================================================

# Maximum length for input sanitization (used by both backend and frontend)
INPUT_MAX_LENGTH=10000


# =============================================================================
# API AUTHENTICATION
# =============================================================================

# Primary API key for authentication (required for all protected endpoints)
API_KEY=your-secure-api-key-here

# Additional API keys for multi-user scenarios (comma-separated)
ADDITIONAL_API_KEYS=key1,key2,key3


# =============================================================================
# CORS CONFIGURATION
# =============================================================================

# Allowed origins for CORS (JSON array format)
# Examples:
# - Development: ["http://localhost:3000", "http://localhost:8501"]
# - Production: ["https://yourdomain.com"]
CORS_ORIGINS=["http://localhost:3000", "http://localhost:8501", "http://127.0.0.1:8501"]

# Legacy format (for backward compatibility) - use CORS_ORIGINS instead
# ALLOWED_ORIGINS=["http://localhost:8501"]

# Allow credentials in CORS requests (true/false)
CORS_CREDENTIALS=true

# Allowed methods for CORS
CORS_METHODS=["GET", "POST", "PUT", "DELETE", "OPTIONS"]

# Allowed headers for CORS
CORS_HEADERS=["*"]


# =============================================================================
# REDIS CONFIGURATION (Optional - used for caching and rate limiting)
# =============================================================================

# Redis connection URL
# If not provided, the application will use in-memory alternatives
# Examples:
# - Local: redis://localhost:6379
# - Remote: redis://username:password@hostname:port/database
# - Docker: redis://redis:6379
REDIS_URL=redis://localhost:6379


# =============================================================================
# RESILIENCE CONFIGURATION (VIA PRESETS)
# =============================================================================
#
# SIMPLE PRESET (RECOMMENDED FOR MOST USERS):
# - 3 retry attempts, 5 failure threshold, 60s recovery
# - Balanced strategy for all operations
# - Best for: General use, testing, staging
#
# DEVELOPMENT PRESET:
# - 2 retry attempts, 3 failure threshold, 30s recovery
# - Aggressive strategy (fast failures for quick feedback)
# - Best for: Local development, CI/CD, debugging
#
# PRODUCTION PRESET:
# - 5 retry attempts, 10 failure threshold, 120s recovery  
# - Conservative default with operation-specific overrides
# - Best for: Production workloads, customer-facing systems

RESILIENCE_PRESET=development

# Advanced: Custom resilience configuration (JSON format)
# Only use if you need configuration beyond what presets provide
# RESILIENCE_CUSTOM_CONFIG={"retry_attempts": 3, "circuit_breaker_threshold": 5}


# =============================================================================
# ENHANCED MIDDLEWARE CONFIGURATION
# =============================================================================

# === Rate Limiting Configuration ===

# Enable/disable rate limiting middleware (true/false)
RATE_LIMITING_ENABLED=true

# Skip rate limiting for health check endpoints (true/false)
RATE_LIMITING_SKIP_HEALTH=true

# Maximum requests allowed per user/endpoint per minute
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# Time window duration for counting requests (seconds)
RATE_LIMIT_WINDOW_SECONDS=60

# Custom rate limits per endpoint (JSON format)
# Example: {"POST /v1/text_processing/summarize": {"requests": 10, "window": 60}}
# CUSTOM_RATE_LIMITS={}

# Custom endpoint classification rules (JSON format)
# Example: {"/v1/text_processing/batch_process": "heavy"}
# CUSTOM_ENDPOINT_RULES={}

# === Request Size Limiting Configuration ===

# Enable/disable request size limiting (true/false)
REQUEST_SIZE_LIMITING_ENABLED=true

# Request size limits by content type (JSON format)
# Sizes in bytes: 1KB=1024, 1MB=1048576, 1GB=1073741824
REQUEST_SIZE_LIMITS={ "default": 2097152, "application/json": 5242880, "multipart/form-data": 52428800 }

# === Compression Configuration ===

# Enable/disable compression middleware (true/false)
COMPRESSION_ENABLED=true

# Minimum response size to compress (bytes)
COMPRESSION_MIN_SIZE=1024

# Compression level (1-9, higher = better compression but slower)
COMPRESSION_LEVEL=6

# Supported compression algorithms (JSON array)
COMPRESSION_ALGORITHMS=["br", "gzip", "deflate"]

# Enable streaming compression for large responses (true/false)
STREAMING_COMPRESSION_ENABLED=true

# Compression Memory Limit
COMPRESSION_MAX_MEMORY=104857600

# === API Versioning Configuration ===

# Enable/disable API versioning middleware (true/false)
API_VERSIONING_ENABLED=true

# Default API version for requests without version specified
DEFAULT_API_VERSION=1.0

# Current API version of the application
CURRENT_API_VERSION=1.0

# Minimum supported API version
MIN_API_VERSION=1.0

# Maximum supported API version
MAX_API_VERSION=1.0

# Supported API Versions Array
API_SUPPORTED_VERSIONS=["1.0"]

# Enable version compatibility transformations (true/false)
API_VERSION_COMPATIBILITY_ENABLED=false

# Enable version usage analytics (true/false)
VERSION_ANALYTICS_ENABLED=true

# API Version Header Name
API_VERSION_HEADER=X-API-Version

# === Security Middleware Configuration ===

# Enable/disable security headers middleware (true/false)
SECURITY_HEADERS_ENABLED=true

# Maximum request size (bytes)
MAX_REQUEST_SIZE=2097152

# Maximum number of headers per request
MAX_HEADERS_COUNT=100

# === Performance Monitoring Configuration ===

# Enable/disable performance monitoring middleware (true/false)
PERFORMANCE_MONITORING_ENABLED=true

# Slow request threshold in milliseconds
SLOW_REQUEST_THRESHOLD=1000

# Enable memory usage monitoring (true/false)
MEMORY_MONITORING_ENABLED=true

# Enable metrics export to external systems (true/false)
METRICS_EXPORT_ENABLED=false

# === Request Logging Configuration ===

# Enable/disable request logging middleware (true/false)
REQUEST_LOGGING_ENABLED=true

# Log sensitive data like API keys (true/false) - NOT recommended for production
LOG_SENSITIVE_DATA=false

# Log request bodies (true/false) - Use with caution due to log size
LOG_REQUEST_BODIES=false

# Log response bodies (true/false) - Use with caution due to log size
LOG_RESPONSE_BODIES=false


# =============================================================================
# INTERNAL API CONFIGURATION
# =============================================================================

# Disable internal API documentation in production (true/false)
# Recommended: true for production, false for development
DISABLE_INTERNAL_DOCS=false


# =============================================================================
# CACHE CONFIGURATION
# =============================================================================

# Cache TTL (Time To Live) in seconds
CACHE_TTL_SECONDS=3600

# Enable cache statistics collection (true/false)
CACHE_STATS_ENABLED=true


# =============================================================================
# PERFORMANCE TUNING
# =============================================================================

# Maximum concurrent requests
MAX_CONCURRENT_REQUESTS=50

# Request timeout in seconds
REQUEST_TIMEOUT_SECONDS=30


# =============================================================================
# CONFIGURATION TEMPLATES
# =============================================================================

# === Development Environment Template ===
# ENVIRONMENT=development
# DEBUG=true
# LOG_LEVEL=DEBUG
# RESILIENCE_PRESET=development
# RATE_LIMITING_ENABLED=true
# COMPRESSION_ENABLED=true
# SECURITY_HEADERS_ENABLED=true
# PERFORMANCE_MONITORING_ENABLED=true
# AI_MODEL=gemini-2.0-flash-exp
# AI_TEMPERATURE=0.7
# BACKEND_HOST=0.0.0.0
# BACKEND_PORT=8000
# FRONTEND_PORT=8501

# === Staging Environment Template ===
# ENVIRONMENT=staging
# DEBUG=false
# LOG_LEVEL=INFO
# RESILIENCE_PRESET=production
# RATE_LIMITING_ENABLED=true
# COMPRESSION_ENABLED=true
# SECURITY_HEADERS_ENABLED=true
# PERFORMANCE_MONITORING_ENABLED=true
# DISABLE_INTERNAL_DOCS=false
# AI_MODEL=gemini-2.0-flash-exp
# AI_TEMPERATURE=0.7
# BACKEND_HOST=0.0.0.0
# BACKEND_PORT=8000
# FRONTEND_PORT=8501

# === Production Environment Template ===
# ENVIRONMENT=production
# DEBUG=false
# LOG_LEVEL=WARNING
# RESILIENCE_PRESET=production
# RATE_LIMITING_ENABLED=true
# COMPRESSION_ENABLED=true
# SECURITY_HEADERS_ENABLED=true
# PERFORMANCE_MONITORING_ENABLED=true
# DISABLE_INTERNAL_DOCS=true
# LOG_SENSITIVE_DATA=false
# LOG_REQUEST_BODIES=false
# LOG_RESPONSE_BODIES=false
# MEMORY_MONITORING_ENABLED=false
# AI_MODEL=gemini-2.0-flash-exp
# AI_TEMPERATURE=0.5
# BACKEND_HOST=0.0.0.0
# BACKEND_PORT=8000
# FRONTEND_PORT=8501


# =============================================================================
# ENVIRONMENT-SPECIFIC NOTES
# =============================================================================

# Development:
# - Use 'development' resilience preset for fast feedback
# - Enable debug logging for troubleshooting
# - Keep internal API docs enabled
# - Enable detailed monitoring for development insights
# - Higher AI temperature for more creative responses during testing

# Staging:
# - Use 'production' resilience preset for realistic testing
# - Reduce logging verbosity
# - Test with internal docs disabled
# - Monitor performance characteristics
# - Production-like AI model settings

# Production:
# - Use 'production' resilience preset for maximum reliability
# - Minimize logging for performance and security
# - Disable internal API documentation
# - Enable only essential monitoring
# - Never log sensitive data or request/response bodies
# - Lower AI temperature for more consistent responses


# =============================================================================
# SECURITY CONSIDERATIONS
# =============================================================================

# 1. Always use strong, unique API keys
# 2. Never commit actual API keys to version control
# 3. Rotate API keys regularly
# 4. Use HTTPS in production
# 5. Set appropriate CORS origins (never use "*" in production)
# 6. Disable debug mode in production
# 7. Set LOG_SENSITIVE_DATA=false in production
# 8. Use environment-specific rate limiting rules
# 9. Enable security headers middleware
# 10. Regularly review and update resilience configurations
# 11. Protect your Gemini API key as it provides access to AI services
# 12. Monitor AI model usage to prevent unexpected costs


# =============================================================================
# TROUBLESHOOTING
# =============================================================================

# Common Issues and Solutions:
#
# 1. Rate limiting too aggressive:
#    - Increase rate limits in CUSTOM_RATE_LIMITS
#    - Use 'development' resilience preset for testing
#
# 2. Compression not working:
#    - Check COMPRESSION_MIN_SIZE is appropriate
#    - Verify COMPRESSION_ALGORITHMS includes supported formats
#
# 3. API versioning issues:
#    - Ensure DEFAULT_API_VERSION matches your API design
#    - Check version headers in client requests
#
# 4. Performance issues:
#    - Reduce COMPRESSION_LEVEL for faster processing
#    - Disable MEMORY_MONITORING_ENABLED if not needed
#    - Tune SLOW_REQUEST_THRESHOLD based on your requirements
#    - Check AI_TEMPERATURE and AI_MODEL settings
#
# 5. Resilience configuration errors:
#    - Use preset configurations (simple, development, production)
#    - Validate custom JSON configurations before deployment
#    - Check logs for configuration validation warnings
#
# 6. AI model issues:
#    - Verify GEMINI_API_KEY is correct and has proper permissions
#    - Check if AI_MODEL is supported by your Gemini API plan
#    - Adjust AI_TEMPERATURE if responses are too random or too deterministic
#
# 7. Server connection issues:
#    - Check BACKEND_HOST and BACKEND_PORT settings
#    - Ensure CORS_ORIGINS includes your frontend URL
#    - Verify Redis connection with REDIS_URL if using caching
#
# 8. Batch processing issues:
#    - Adjust MAX_BATCH_REQUESTS_PER_CALL based on your load
#    - Tune BATCH_AI_CONCURRENCY_LIMIT to balance speed vs resource usage
#
# 9. Input validation errors:
#    - Check INPUT_MAX_LENGTH matches your expected input sizes
#    - Verify content doesn't exceed REQUEST_SIZE_LIMITS