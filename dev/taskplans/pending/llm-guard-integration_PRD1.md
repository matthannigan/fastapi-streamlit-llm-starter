# PRD: `llm-guard` Integration for Advanced AI Security

## Overview

[cite\_start]This document outlines the plan to integrate the `llm-guard` library into the FastAPI starter template, replacing the existing custom, regex-based security modules (`input_sanitizer.py`, `prompt_builder.py`, `response_validator.py`)[cite: 1]. This initiative addresses the need for a more robust, industry-standard security solution that reduces maintenance overhead and introduces new, critical security capabilities.

This integration solves the problem of maintaining a complex, custom-built security system by adopting a specialized, open-source tool. It's for developers using this template who require production-grade AI security without becoming security experts themselves. The value lies in providing a starter that is secure-by-default with advanced features like PII detection and output validation, significantly increasing the template's production readiness.

## Core Features

The integration will introduce the following core features, replacing and enhancing the current security model.

-----

### 1\. Advanced Input Sanitization & Threat Detection

  - **What it does**: Replaces the current regex-based input sanitizer with `llm-guard`'s suite of sophisticated input scanners. This includes ML-powered detection for prompt injection, toxicity, secrets, and malicious code.
  - **Why it's important**: Moves beyond static pattern matching to semantic threat detection, offering superior protection against evolving attack vectors. It also adds new capabilities like secrets detection (e.g., exposed API keys) that do not exist in the current implementation.
  - **How it works**: A new `LLMGuardService` will be created within the infrastructure layer. The `TextProcessorService` will use this service to scan all incoming user text against a configurable set of `llm-guard` input scanners before any processing occurs.

-----

### 2\. Robust Output Validation

  - **What it does**: Implements a new security layer to validate and sanitize responses generated by the LLM *before* they are sent to the end-user. [cite\_start]This fulfills a planned enhancement noted in the existing AI infrastructure documentation. [cite: 1]
  - **Why it's important**: Protects against model misbehavior, data leakage, and ensures the LLM does not generate harmful or off-topic content. It prevents the model from refusing to answer valid prompts or leaking sensitive information from its training data or the prompt itself.
  - **How it works**: The `LLMGuardService` will use `llm-guard`'s output scanners (e.g., `NoRefusal`, `Relevance`, `Sensitive`) to analyze the LLM's response within the `TextProcessorService`. If the output is flagged as unsafe, an exception will be raised or a safe fallback response will be returned.

-----

### 3\. Preset-Based Security Configuration

  - **What it does**: Introduces a new environment variable, `LLM_GUARD_PRESET`, that allows developers to select a security level (`development`, `production`, `strict`) with a single setting. [cite\_start]This mirrors the successful preset patterns already used for cache and resilience configuration. [cite: 1]
  - **Why it's important**: Drastically simplifies security configuration, making it easy for developers to apply best-practice security settings without needing to understand the nuances of each individual scanner. It aligns with the template's core philosophy of simplicity and excellent developer experience.
  - **How it works**: The `LLMGuardService` will read the `LLM_GUARD_PRESET` variable and dynamically load the corresponding set of input and output scanners. Developers can choose a preset that balances security and performance for their specific environment.

-----

### 4\. Deprecation of Custom Security Modules

  - **What it does**: Completely removes `backend/app/infrastructure/ai/input_sanitizer.py` and `backend/app/services/response_validator.py`. The `backend/app/infrastructure/ai/prompt_builder.py` module will be simplified to handle only prompt templating, with all security functions removed.
  - **Why it's important**: Reduces the custom codebase, lowers the maintenance burden, and eliminates the risk of bugs in homegrown security logic. It focuses the template on its core strength: architectural patterns, not implementing low-level security tools.
  - **How it works**: As `llm-guard`'s functionality is integrated into the `TextProcessorService`, the calls to the old custom modules will be removed, and the files will be deleted from the repository.

## User Experience

The "user" of this feature is the **application developer** leveraging the FastAPI starter template.

  - **User Persona**: A developer building an LLM-powered application. They are skilled in application logic but may not be a security expert. They value simplicity, clear patterns, and production-ready tools that "just work."
  - **Key User Flows**:
    1.  **Configuration**: A developer sets `LLM_GUARD_PRESET=production` in their `.env` file. The template automatically activates a strong, pre-configured set of security scanners for both input and output.
    2.  **Development**: The `TextProcessorService` transparently handles all security scanning. The developer can focus on their domain logic, knowing that the infrastructure layer is providing robust protection.
    3.  **Customization**: For a specialized use case, the developer can easily modify the list of scanners loaded by the `LLMGuardService` to add or remove specific protections, giving them fine-grained control when needed.
  - **UI/UX Considerations**: The primary goal is to improve the **Developer Experience (DX)**. The integration should feel seamless. The preset system makes a complex security suite easy to manage, and the removal of custom modules simplifies the codebase, making it easier to understand and extend.

-----

## Technical Architecture

### System Components

1.  **New Module (`app/infrastructure/ai/llm_guard_service.py`)**: This new service will encapsulate all `llm-guard` logic. It will initialize the required input and output scanners based on the active preset and expose simple `scan_input` and `scan_output` methods.
2.  **Configuration (`app/core/config.py`)**: The `Settings` class will be updated with a new `LLM_GUARD_PRESET` variable and logic to manage the different security levels (e.g., `development`, `production`).
3.  **Dependency Injection (`app/dependencies.py`)**: A new dependency provider, `get_llm_guard_service()`, will be created to supply a singleton instance of the `LLMGuardService` to other services.
4.  **Service Refactoring (`app/services/text_processor.py`)**: This service will be the primary consumer. It will be refactored to:
      * Inject the `LLMGuardService`.
      * Call `llm_guard_service.scan_input()` on incoming requests, replacing calls to `sanitize_input_advanced`.
      * Call `llm_guard_service.scan_output()` on LLM responses, replacing calls to the custom `ResponseValidator`.
5.  **Module Deletion**: The following files will be deleted:
      * `backend/app/infrastructure/ai/input_sanitizer.py`
      * `backend/app/services/response_validator.py`
6.  **Module Simplification**: `backend/app/infrastructure/ai/prompt_builder.py` will have its `escape_user_input` function removed, as `llm-guard` provides superior sanitization. Its role will be reduced to pure prompt templating.

### Infrastructure Requirements

  - **New Dependencies**: `llm-guard` will be added to `requirements.txt`. This will also bring in its transitive dependencies, such as `transformers` and `torch`, which will increase the Docker image size.
  - **Environment Variables**: A new variable, `LLM_GUARD_PRESET`, will be added to `.env.example`.

-----

## Development Roadmap

The development will proceed in phases to ensure an incremental and manageable integration. Since backward compatibility is not a goal, we can move directly to the new implementation.

### Phase 1: Foundational Integration & Input Scanning

  - **Scope**:
      - Add `llm-guard` as a project dependency.
      - Create the new `LLMGuardService` in the AI infrastructure layer.
      - Implement the preset configuration system in `app/core/config.py` with `development` and `production` presets.
      - Refactor `TextProcessorService` to use `LLMGuardService` for **input scanning** only.
      - Delete `input_sanitizer.py`.
  - **Goal**: Replace the existing input sanitization with a more robust, ML-powered solution.

### Phase 2: Output Scanning & Full Replacement

  - **Scope**:
      - Extend `LLMGuardService` and `TextProcessorService` to perform **output scanning** on all LLM responses.
      - The `production` preset should include key output scanners like `NoRefusal` and `Sensitive`.
      - Delete the custom `response_validator.py`.
      - Remove security-related functions from `prompt_builder.py`.
  - **Goal**: Introduce a critical new security layer for output validation and complete the replacement of all custom security logic.

### Phase 3: Advanced Features & Documentation

  - **Scope**:
      - Integrate advanced scanners like `Anonymize` (for PII) and `Secrets` into a new `strict` preset.
      - [cite\_start]Update all relevant documentation (`README.md`, `AGENTS.md`, `BACKEND.md`, etc.) to reflect the new `llm-guard`-based architecture. [cite: 1]
      - Add performance benchmarks to measure the latency impact of the scanners.
  - **Goal**: Finalize the feature set and provide comprehensive documentation for developers using the template.

-----

## Logical Dependency Chain

Development will proceed in a logical order to ensure a usable state at each step.

1.  **Foundation First**: The `LLMGuardService` and preset configuration system are the foundational building blocks and must be created first.
2.  **Achieve Usability Quickly**: Integrating input scanning is the next priority. This provides an immediate enhancement over the old system and allows us to remove a major piece of legacy code (`input_sanitizer.py`). The application will be in a usable and improved state at this point.
3.  **Build Upon the Foundation**: With the service in place, adding output scanning is an incremental enhancement that builds on the work from the previous step.
4.  **Pacing and Scope**: Each phase is scoped to be atomic. Phase 1 replaces input security. Phase 2 adds output security. Phase 3 enhances and documents. This allows for clear progress and testing at each stage.

-----

## Risks and Mitigations

  - **Risk**: `llm-guard` and its dependencies (`torch`, `transformers`) will significantly increase the Docker image size and runtime memory usage.
      - **Mitigation**: Clearly document the increased resource requirements. In the documentation, suggest which scanners are lightweight versus resource-intensive. The preset system will offer a `development` preset with only the most essential, lightweight scanners.
  - **Risk**: ML-based scanning will introduce higher latency compared to the current regex-based approach.
      - **Mitigation**: Perform benchmarks to quantify the latency impact for each preset. Publish these benchmarks in the documentation. The preset system will allow users to choose a trade-off between security and performance that suits their needs.
  - **Risk**: Defining the MVP and the default set of scanners for the `production` preset can be challenging.
      - **Mitigation**: The MVP (`production` preset) will focus on the most critical and common threats: `PromptInjection`, `Toxicity`, `NoRefusal`, and `Sensitive`. The system will be designed to be easily extensible, and the documentation will clearly explain how developers can add more scanners.

## Appendix

### Custom Module to `llm-guard` Mapping

| Old Functionality (Custom Modules) | New Scanner (`llm-guard`) |
| :--- | :--- |
| `input_sanitizer.py` (Regex prompt injection) | `PromptInjection` (ML-based) |
| `input_sanitizer.py` (Character filtering for code) | `Code` |
| `input_sanitizer.py` (HTML escaping) | No longer needed; content is analyzed |
| *New Capability* | `Anonymize`, `Toxicity`, `Secrets` |
| `response_validator.py` (System prompt leakage) | `Sensitive` |
| `response_validator.py` (Refusal detection) | `NoRefusal` |
| `prompt_builder.py` (`escape_user_input`) | No longer needed; replaced by `llm-guard` |

### Example `LLM_GUARD_PRESET` Configuration

```ini
# .env file

# For local development: minimal, fast scanners
LLM_GUARD_PRESET=development

# For production: strong set of input/output scanners
# LLM_GUARD_PRESET=production

# For high-security needs: includes PII detection and more
# LLM_GUARD_PRESET=strict
```