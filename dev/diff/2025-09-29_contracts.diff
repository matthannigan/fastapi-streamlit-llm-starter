diff --git a/backend/contracts/api/v1/auth.pyi b/backend/contracts/api/v1/auth.pyi
index 486accb..bbda223 100644
--- a/backend/contracts/api/v1/auth.pyi
+++ b/backend/contracts/api/v1/auth.pyi
@@ -23,11 +23,11 @@ workflows in domain services.
 ## Dependencies & Integration
 
 ### Infrastructure Dependencies
-- `app.infrastructure.security.verify_api_key`: Infrastructure authentication service
+- `app.infrastructure.security.verify_api_key_http`: HTTP-compatible authentication service
 - `app.schemas.ErrorResponse`: Structured error response model
 
 ### FastAPI Dependencies
-- `verify_api_key()`: Required authentication dependency injection
+- `verify_api_key_http()`: HTTP-compatible authentication dependency injection
 
 ## Usage Examples
 
@@ -63,14 +63,14 @@ Customize the authentication logic and responses based on your application needs
 
 from fastapi import APIRouter, Depends
 import logging
-from app.infrastructure.security import verify_api_key
+from app.infrastructure.security import verify_api_key_http
 from app.schemas import ErrorResponse
 
 auth_router = APIRouter(prefix='/auth', tags=['Authentication'])
 
 
 @auth_router.get('/status', responses={401: {'model': ErrorResponse, 'description': 'Authentication Error'}})
-async def auth_status(api_key: str = Depends(verify_api_key)):
+async def auth_status(api_key: str = Depends(verify_api_key_http)):
     """
     Authentication status validation endpoint with secure API key verification and truncation.
     
diff --git a/backend/contracts/api/v1/health.pyi b/backend/contracts/api/v1/health.pyi
index 56213de..21d2501 100644
--- a/backend/contracts/api/v1/health.pyi
+++ b/backend/contracts/api/v1/health.pyi
@@ -88,6 +88,14 @@ This service demonstrates domain-level health checking that:
 
 **Replace in your project** - Customize the health checks and status logic
 based on your specific infrastructure components and monitoring requirements.
+
+## Developer Reference
+
+For comprehensive documentation on the health check infrastructure, including
+advanced usage patterns, performance optimization, testing strategies, and
+production deployment guidance, see:
+
+**📖 `docs/guides/developer/HEALTH_CHECKS.md`**
 """
 
 from fastapi import APIRouter, Depends
diff --git a/backend/contracts/core/config.pyi b/backend/contracts/core/config.pyi
index a5fc07a..27fb159 100644
--- a/backend/contracts/core/config.pyi
+++ b/backend/contracts/core/config.pyi
@@ -23,7 +23,7 @@ preset selections with optional overrides.
 ### Core Design Principles
 - **Preset-First**: Choose configuration presets instead of managing dozens of variables
 - **Override Capable**: Environment variables and JSON overrides for customization
-- **Backward Compatible**: Legacy environment variable support for smooth migration
+- **Configuration Presets**: Simplified preset-based configuration system
 - **Validation-First**: Comprehensive Pydantic validation with clear error messages
 - **Observable Behavior**: Configuration loading behavior is logged and monitorable
 
@@ -42,7 +42,7 @@ preset selections with optional overrides.
 
 ### Resilience Configuration (Preset-Based)
 - **Available Presets**: simple, development, production
-- **Legacy Support**: Automatic detection and migration from individual variables
+- **Preset System**: Intelligent defaults with simple environment variable selection
 - **Operation-Specific**: Different resilience strategies per AI operation type
 - **Custom Overrides**: RESILIENCE_CUSTOM_CONFIG for fine-tuning
 
@@ -105,6 +105,7 @@ cache_config = settings.get_cache_config()
 ```python
 from app.core.config import settings
 from app.core.exceptions import ConfigurationError
+from app.infrastructure.resilience.config_validator import config_validator
 
 try:
     # Load cache configuration with session tracking
@@ -113,10 +114,11 @@ try:
         user_context="production_deployment"
     )
     
-    # Validate custom configuration
-    validation_result = settings.validate_custom_config(custom_json)
-    if not validation_result["is_valid"]:
-        logger.error(f"Configuration errors: {validation_result['errors']}")
+    # Validate custom resilience configuration if needed
+    custom_json = '{"retry_attempts": 5, "circuit_breaker_threshold": 10}'
+    validation_result = config_validator.validate_json_string(custom_json)
+    if not validation_result.is_valid:
+        logger.error(f"Configuration errors: {validation_result.errors}")
         
 except ConfigurationError as e:
     # Handle configuration validation failures
@@ -146,7 +148,7 @@ os.environ['CACHE_REDIS_URL'] = 'redis://prod-cluster:6379'
 
 ### Preset Resolution Behavior
 - Cache presets resolve configuration precedence: Custom JSON > Environment Variables > Preset Defaults
-- Resilience presets auto-detect legacy environment variables for backward compatibility
+- Resilience presets provide simplified configuration with intelligent defaults
 - Invalid preset names raise ConfigurationError with available preset suggestions
 - Fallback presets are automatically applied when primary preset loading fails
 
@@ -165,7 +167,7 @@ os.environ['CACHE_REDIS_URL'] = 'redis://prod-cluster:6379'
 Note:
     This module represents a critical infrastructure component. All configuration changes
     should undergo thorough testing across different environment scenarios to ensure
-    backward compatibility and proper fallback behavior.
+    preset-based configuration and proper fallback behavior.
 """
 
 import os
@@ -213,7 +215,6 @@ class Settings(BaseSettings):
         get_cache_config(): Get complete cache configuration from preset with overrides applied
         get_resilience_config(): Get complete resilience configuration from preset with overrides applied
         get_operation_strategy(): Get resilience strategy for specific operation types
-        validate_custom_config(): Validate custom JSON configuration strings
         get_valid_api_keys(): Get list of all valid API keys (primary + additional)
         
     State Management:
@@ -227,7 +228,7 @@ class Settings(BaseSettings):
     Behavior:
         - Validates all environment variables against type constraints at startup
         - Applies preset-based configuration with override precedence: Custom JSON > Environment Variables > Preset Defaults
-        - Auto-detects legacy resilience environment variables for backward compatibility
+        - Applies preset-based resilience configuration with optional custom overrides
         - Logs all configuration loading actions and applied overrides for debugging
         - Raises ConfigurationError for invalid preset names with available preset suggestions
         - Falls back to 'simple' presets when primary preset loading fails
@@ -274,11 +275,6 @@ class Settings(BaseSettings):
         }
         os.environ['CACHE_CUSTOM_CONFIG'] = json.dumps(custom_cache)
         
-        # Configuration validation
-        validation_result = settings.validate_custom_config(custom_json)
-        if not validation_result['is_valid']:
-            logger.error(f"Configuration errors: {validation_result['errors']}")
-        
         # Session-aware configuration loading
         cache_config = settings.get_cache_config(
             session_id="user_123",
@@ -360,30 +356,6 @@ class Settings(BaseSettings):
         """
         ...
 
-    @field_validator('default_resilience_strategy', 'summarize_resilience_strategy', 'sentiment_resilience_strategy', 'key_points_resilience_strategy', 'questions_resilience_strategy', 'qa_resilience_strategy', mode='before')
-    @classmethod
-    def validate_resilience_strategy(cls, v, info) -> str:
-        """
-        Validate resilience strategy with graceful fallback for environment variables.
-        """
-        ...
-
-    @field_validator('circuit_breaker_failure_threshold', 'circuit_breaker_recovery_timeout', 'retry_max_attempts', 'retry_max_delay', mode='before')
-    @classmethod
-    def validate_positive_integers(cls, v, info) -> int:
-        """
-        Validate positive integers with strict validation for completely invalid values.
-        """
-        ...
-
-    @field_validator('retry_exponential_multiplier', 'retry_exponential_min', 'retry_exponential_max', 'retry_jitter_max')
-    @classmethod
-    def validate_positive_floats(cls, v: float, info) -> float:
-        """
-        Validate positive floats with fallback for invalid values.
-        """
-        ...
-
     @field_validator('health_check_timeout_ms', 'health_check_ai_model_timeout_ms', 'health_check_cache_timeout_ms', 'health_check_resilience_timeout_ms', 'health_check_retry_count')
     @classmethod
     def validate_health_check_numbers(cls, v: int | float, info) -> int:
@@ -426,7 +398,7 @@ class Settings(BaseSettings):
             
         Behavior:
             - Resolves cache_preset field to load base configuration from preset system
-            - Maps 'testing' preset alias to 'development' preset for backward compatibility
+            - Maps 'testing' preset alias to 'development' preset for convenience
             - Applies CACHE_REDIS_URL environment variable override if present
             - Applies ENABLE_AI_CACHE environment variable override if present (true/false/1/0)
             - Applies CACHE_CUSTOM_CONFIG JSON overrides if provided and valid
@@ -466,12 +438,11 @@ class Settings(BaseSettings):
 
     def get_resilience_config(self, session_id: Optional[str] = None, user_context: Optional[str] = None):
         """
-        Get complete resilience configuration from preset or legacy settings with monitoring.
+        Get complete resilience configuration from preset with custom overrides.
         
-        This is the main entry point for resilience configuration that implements both
-        preset-based architecture and legacy environment variable compatibility. It
-        automatically detects the appropriate configuration source and provides
-        comprehensive monitoring and error handling with fallback behavior.
+        This is the main entry point for resilience configuration that implements the
+        preset-based architecture. It automatically resolves configuration from presets
+        and provides comprehensive monitoring and error handling with fallback behavior.
         
         Args:
             session_id: Optional session identifier for monitoring and analytics (default: None)
@@ -484,43 +455,32 @@ class Settings(BaseSettings):
             - circuit_breaker_config: CircuitBreakerConfig with failure thresholds and recovery timeouts
             - enable_circuit_breaker: Boolean flag for circuit breaker pattern activation
             - enable_retry: Boolean flag for retry mechanism activation
-            
+        
         Raises:
             ConfigurationError: When both primary preset loading and fallback preset loading fail
-            
+        
         Behavior:
-            - Auto-detects legacy environment variables and prioritizes them for backward compatibility
             - Resolves resilience_preset field to load base configuration from preset system
             - Applies resilience_custom_config JSON overrides if provided and valid
             - Falls back to 'simple' preset if primary preset loading fails
             - Logs all configuration resolution steps and applied overrides for debugging
             - Records preset usage metrics and monitoring data for analytics
-            - Ignores custom config in legacy mode to maintain strict backward compatibility
             - Provides session and user context tracking for operational monitoring
             - Returns identical configuration for identical parameters and environment state
-            - Caches legacy environment variable detection to avoid repeated filesystem checks
-            
+        
         Examples:
             >>> # Basic preset-based configuration
             >>> config = settings.get_resilience_config()
             >>> assert config.strategy.name in ["CONSERVATIVE", "BALANCED", "AGGRESSIVE"]
             >>> assert config.retry_config.max_attempts > 0
-            
+        
             >>> # Configuration with session tracking
             >>> config = settings.get_resilience_config(
             ...     session_id="api_request_123",
             ...     user_context="text_processing_endpoint"
             ... )
             >>> assert config.circuit_breaker_config.failure_threshold > 0
-            
-            >>> # Legacy environment variable mode
-            >>> import os
-            >>> os.environ['RETRY_MAX_ATTEMPTS'] = '5'
-            >>> os.environ['CIRCUIT_BREAKER_FAILURE_THRESHOLD'] = '10'
-            >>> config = settings.get_resilience_config()
-            >>> assert config.retry_config.max_attempts == 5
-            >>> assert config.circuit_breaker_config.failure_threshold == 10
-            
+        
             >>> # Custom JSON configuration mode
             >>> settings.resilience_custom_config = '{"retry_attempts": 7, "circuit_breaker_threshold": 15}'
             >>> config = settings.get_resilience_config()
@@ -581,62 +541,6 @@ class Settings(BaseSettings):
         """
         ...
 
-    def validate_resilience_custom_config(self, json_string: Optional[str] = None) -> dict:
-        """
-        Validate custom resilience configuration JSON string against schema.
-        
-        This method provides comprehensive validation of custom resilience configuration
-        JSON strings, ensuring they conform to the expected schema and contain valid
-        values before application to the resilience system.
-        
-        Args:
-            json_string: JSON string to validate (default: None).
-                        If None, validates the current resilience_custom_config field value.
-                        If empty string or None, returns success with warning message.
-        
-        Returns:
-            Dictionary with validation results containing:
-            - "is_valid": Boolean indicating overall validation success
-            - "errors": List of error messages for validation failures
-            - "warnings": List of warning messages for non-critical issues
-            
-        Behavior:
-            - Uses resilience_custom_config field value when json_string parameter is None
-            - Returns successful validation with warning when no configuration provided
-            - Parses JSON string and validates against resilience configuration schema
-            - Checks value types, ranges, and allowed enumeration values
-            - Validates nested configuration objects and their relationships
-            - Provides detailed error messages with field names and expected values
-            - Catches and reports JSON parsing errors with helpful error descriptions
-            - Does not modify any configuration state during validation
-            
-        Examples:
-            >>> # Validate current configuration
-            >>> result = settings.validate_custom_config()
-            >>> assert result["is_valid"] is True
-            >>> assert "No custom configuration" in result["warnings"][0]
-            
-            >>> # Validate valid JSON configuration
-            >>> valid_config = '{"retry_attempts": 5, "circuit_breaker_threshold": 10}'
-            >>> result = settings.validate_custom_config(valid_config)
-            >>> assert result["is_valid"] is True
-            >>> assert len(result["errors"]) == 0
-            
-            >>> # Validate invalid JSON configuration
-            >>> invalid_config = '{"retry_attempts": "not_a_number"}'
-            >>> result = settings.validate_custom_config(invalid_config)
-            >>> assert result["is_valid"] is False
-            >>> assert len(result["errors"]) > 0
-            >>> assert "retry_attempts" in str(result["errors"])
-            
-            >>> # Handle JSON parsing errors
-            >>> malformed_json = '{"retry_attempts": 5,}'  # Trailing comma
-            >>> result = settings.validate_custom_config(malformed_json)
-            >>> assert result["is_valid"] is False
-            >>> assert "Validation error" in result["errors"][0]
-        """
-        ...
-
     def get_valid_api_keys(self) -> List[str]:
         """
         Get list of all valid API keys for authentication.
diff --git a/backend/contracts/core/environment/__init__.pyi b/backend/contracts/core/environment/__init__.pyi
new file mode 100644
index 0000000..14332a9
--- /dev/null
+++ b/backend/contracts/core/environment/__init__.pyi
@@ -0,0 +1,59 @@
+"""
+Unified Environment Detection Service
+
+This module provides centralized environment detection capabilities for all backend
+infrastructure services, eliminating code duplication and providing consistent
+environment classification across cache, resilience, security, and other systems.
+
+## Architecture Position
+
+```
+┌─────────────────────────────────────────────────────────────┐
+│                    Application Services                     │
+├─────────────────────────────────────────────────────────────┤
+│  Security Auth  │  Cache Presets  │  Resilience Config      │
+│     (NEW)       │   (EXISTING)    │    (EXISTING)           │
+├─────────────────────────────────────────────────────────────┤
+│           Unified Environment Detection Service             │
+│                        (NEW)                                │
+├─────────────────────────────────────────────────────────────┤
+│              Environment Variables & System                 │
+└─────────────────────────────────────────────────────────────┘
+```
+
+## Key Features
+
+- **Centralized Detection**: Single source of truth for environment classification
+- **Confidence Scoring**: Provides confidence levels and reasoning for decisions  
+- **Extensible Patterns**: Configurable patterns for custom deployment scenarios
+- **Context-Aware**: Supports feature-specific context (AI, security, cache)
+- **Fallback Strategies**: Robust fallback detection for edge cases
+- **Integration Ready**: Drop-in replacement for existing detection logic
+
+## Usage Examples
+
+```python
+# Basic environment detection
+detector = EnvironmentDetector()
+env_info = detector.detect_environment()
+print(f"Environment: {env_info.environment} (confidence: {env_info.confidence})")
+
+# Feature-specific detection
+ai_env = detector.detect_with_context(FeatureContext.AI_ENABLED)
+security_env = detector.detect_with_context(FeatureContext.SECURITY_ENFORCEMENT)
+
+# Integration with existing systems
+cache_preset = cache_manager.recommend_preset(env_info.environment)
+resilience_preset = resilience_manager.recommend_preset(env_info.environment)
+auth_config = security_auth.configure_for_environment(env_info)
+```
+
+Re-exports for backward compatibility while organizing into submodules.
+"""
+
+from .enums import Environment, FeatureContext
+from .models import EnvironmentSignal, EnvironmentInfo, DetectionConfig
+from .detector import EnvironmentDetector
+from .api import environment_detector, get_environment_info, is_production_environment, is_development_environment
+
+__all__ = ['Environment', 'FeatureContext', 'EnvironmentSignal', 'EnvironmentInfo', 'DetectionConfig', 'EnvironmentDetector', 'environment_detector', 'get_environment_info', 'is_production_environment', 'is_development_environment']
diff --git a/backend/contracts/core/environment/api.pyi b/backend/contracts/core/environment/api.pyi
new file mode 100644
index 0000000..da7c240
--- /dev/null
+++ b/backend/contracts/core/environment/api.pyi
@@ -0,0 +1,150 @@
+"""
+Module-level convenience functions.
+
+Contains the global detector instance and convenience functions for easy access
+to environment detection functionality without needing to create detector instances.
+"""
+
+from .enums import Environment, FeatureContext
+from .models import EnvironmentInfo
+from .detector import EnvironmentDetector
+
+
+def get_environment_info(feature_context: FeatureContext = FeatureContext.DEFAULT) -> EnvironmentInfo:
+    """
+    Convenient function to get environment information using the global detector.
+    
+    Provides easy access to environment detection without needing to create
+    an EnvironmentDetector instance. Uses the global environment_detector
+    instance with default configuration for consistent detection across the application.
+    
+    Args:
+        feature_context: Feature context for specialized detection logic.
+                        Defaults to FeatureContext.DEFAULT for standard detection.
+                        Use specific contexts like AI_ENABLED or SECURITY_ENFORCEMENT
+                        for feature-aware detection.
+    
+    Returns:
+        EnvironmentInfo containing:
+        - environment: Detected Environment enum value
+        - confidence: Detection confidence score (0.0-1.0)
+        - reasoning: Human-readable explanation of detection
+        - feature_context: The feature context used
+        - metadata: Feature-specific configuration hints
+        - additional_signals: All detection signals collected
+    
+    Raises:
+        ValidationError: If feature_context is not a valid FeatureContext enum value
+    
+    Behavior:
+        - Uses global environment_detector instance for consistent results
+        - Performs full environment detection with confidence scoring
+        - Applies feature-specific context when specified
+        - Caches detection results for performance optimization
+        - Thread-safe for concurrent access across services
+    
+    Examples:
+        >>> # Basic environment detection
+        >>> env_info = get_environment_info()
+        >>> if env_info.environment == Environment.PRODUCTION:
+        ...     enable_production_logging()
+        >>>
+        >>> # Feature-aware detection
+        >>> ai_env = get_environment_info(FeatureContext.AI_ENABLED)
+        >>> if ai_env.metadata.get('ai_prefix'):
+        ...     cache_prefix = ai_env.metadata['ai_prefix']
+        >>>
+        >>> # Confidence-based decisions
+        >>> security_env = get_environment_info(FeatureContext.SECURITY_ENFORCEMENT)
+        >>> if security_env.confidence > 0.8:
+        ...     enforce_strict_security_policies()
+        >>> else:
+        ...     logger.warning(f"Uncertain environment detection: {security_env.reasoning}")
+    """
+    ...
+
+
+def is_production_environment(feature_context: FeatureContext = FeatureContext.DEFAULT) -> bool:
+    """
+    Check if running in production environment with confidence threshold.
+    
+    Convenience function for production environment checks with built-in
+    confidence validation. Uses reasonable confidence threshold to avoid
+    false positives that could affect production configurations.
+    
+    Args:
+        feature_context: Feature context for specialized detection logic.
+                        Defaults to FeatureContext.DEFAULT for standard detection.
+                        Use SECURITY_ENFORCEMENT context for stricter production detection.
+    
+    Returns:
+        True if production environment is detected with confidence > 0.60,
+        False otherwise. The 0.60 threshold balances reliability with
+        sensitivity to avoid false production configurations.
+    
+    Behavior:
+        - Performs environment detection with specified feature context
+        - Requires both Environment.PRODUCTION and confidence > 0.60
+        - Returns False for uncertain or non-production detections
+        - Uses same detection logic as get_environment_info()
+    
+    Examples:
+        >>> # Basic production check
+        >>> if is_production_environment():
+        ...     configure_production_logging()
+        ...     enable_performance_monitoring()
+        >>>
+        >>> # Security-aware production check
+        >>> if is_production_environment(FeatureContext.SECURITY_ENFORCEMENT):
+        ...     enforce_authentication_requirements()
+        ...     enable_audit_logging()
+        >>>
+        >>> # Combined with manual confidence check
+        >>> env_info = get_environment_info()
+        >>> if is_production_environment() and env_info.confidence > 0.9:
+        ...     enable_strict_production_features()
+    """
+    ...
+
+
+def is_development_environment(feature_context: FeatureContext = FeatureContext.DEFAULT) -> bool:
+    """
+    Check if running in development environment with confidence threshold.
+    
+    Convenience function for development environment checks with built-in
+    confidence validation. Useful for enabling development-specific features
+    like debug logging, hot reloading, or relaxed security settings.
+    
+    Args:
+        feature_context: Feature context for specialized detection logic.
+                        Defaults to FeatureContext.DEFAULT for standard detection.
+                        Use specific contexts for feature-aware development detection.
+    
+    Returns:
+        True if development environment is detected with confidence > 0.60,
+        False otherwise. The 0.60 threshold ensures reasonable confidence
+        while allowing for development environment variations.
+    
+    Behavior:
+        - Performs environment detection with specified feature context
+        - Requires both Environment.DEVELOPMENT and confidence > 0.60
+        - Returns False for uncertain or non-development detections
+        - Uses same detection logic as get_environment_info()
+    
+    Examples:
+        >>> # Basic development check
+        >>> if is_development_environment():
+        ...     enable_debug_logging()
+        ...     configure_hot_reloading()
+        >>>
+        >>> # AI development features
+        >>> if is_development_environment(FeatureContext.AI_ENABLED):
+        ...     use_development_ai_models()
+        ...     enable_ai_debug_logging()
+        >>>
+        >>> # Development-specific cache settings
+        >>> if is_development_environment(FeatureContext.CACHE_OPTIMIZATION):
+        ...     use_memory_cache_only()
+        ...     set_short_cache_ttls()
+    """
+    ...
diff --git a/backend/contracts/core/environment/detector.pyi b/backend/contracts/core/environment/detector.pyi
new file mode 100644
index 0000000..9401b08
--- /dev/null
+++ b/backend/contracts/core/environment/detector.pyi
@@ -0,0 +1,251 @@
+"""
+Main environment detection implementation.
+
+Contains the core EnvironmentDetector class that orchestrates environment detection
+using signals from various sources and applies confidence scoring.
+"""
+
+import logging
+from typing import Optional, Dict, Any
+from .enums import FeatureContext
+from .models import DetectionConfig, EnvironmentInfo, EnvironmentSignal
+from .patterns import collect_detection_signals
+from .feature_contexts import apply_feature_context, determine_environment
+
+
+class EnvironmentDetector:
+    """
+    Unified environment detection service for consistent infrastructure configuration.
+    
+    Provides centralized environment classification across all backend infrastructure
+    services including cache, resilience, security, and monitoring systems. Uses
+    confidence scoring, feature-specific context, and extensible pattern matching
+    to ensure reliable environment detection in diverse deployment scenarios.
+    
+    Public Methods:
+        detect_environment(): Detect environment with optional feature context
+        detect_with_context(): Detect environment with specific feature context
+        get_environment_summary(): Get comprehensive detection summary with all signals
+    
+    State Management:
+        - Maintains signal cache for performance optimization
+        - Thread-safe for concurrent access across infrastructure services
+        - Immutable configuration after initialization
+    
+    Behavior:
+        - Collects environment signals from variables, patterns, and system indicators
+        - Applies confidence scoring with conflict resolution
+        - Supports feature-specific overrides for specialized detection
+        - Provides fallback detection when no signals are found
+        - Logs detection decisions for debugging and monitoring
+    
+    Usage:
+        # Basic environment detection
+        detector = EnvironmentDetector()
+        env_info = detector.detect_environment()
+    
+        if env_info.environment == Environment.PRODUCTION:
+            configure_production_services()
+        elif env_info.confidence < 0.7:
+            logger.warning(f"Low confidence detection: {env_info.reasoning}")
+    
+        # Feature-aware detection for AI services
+        ai_env = detector.detect_with_context(FeatureContext.AI_ENABLED)
+        if ai_env.metadata.get('ai_prefix'):
+            cache_prefix = ai_env.metadata['ai_prefix']
+    
+        # Custom configuration for specialized deployment
+        config = DetectionConfig(
+            production_patterns=[r'.*live.*', r'.*prod.*'],
+            feature_contexts={
+                FeatureContext.SECURITY_ENFORCEMENT: {
+                    'environment_var': 'FORCE_SECURE_MODE',
+                    'production_override': True
+                }
+            }
+        )
+        detector = EnvironmentDetector(config)
+    
+        # Debugging detection issues
+        summary = detector.get_environment_summary()
+        print(f"Detected: {summary['detected_environment']} ({summary['confidence']:.2f})")
+        for signal in summary['all_signals']:
+            print(f"  - {signal['source']}: {signal['reasoning']}")
+    """
+
+    def __init__(self, config: Optional[DetectionConfig] = None):
+        """
+        Initialize environment detector with configuration and caching.
+        
+        Args:
+            config: Optional detection configuration with patterns and precedence.
+                   Uses DetectionConfig() defaults if not provided.
+        
+        Behavior:
+            - Creates signal cache for performance optimization
+            - Validates configuration patterns are well-formed regex
+            - Logs initialization for debugging and monitoring
+            - Stores immutable configuration for thread safety
+        
+        Examples:
+            >>> # Basic initialization with defaults
+            >>> detector = EnvironmentDetector()
+            >>> assert detector.config is not None
+        
+            >>> # Custom configuration
+            >>> config = DetectionConfig(
+            ...     env_var_precedence=['CUSTOM_ENV', 'ENVIRONMENT'],
+            ...     production_patterns=[r'.*prod.*', r'.*live.*']
+            ... )
+            >>> detector = EnvironmentDetector(config)
+        """
+        ...
+
+    def detect_environment(self, feature_context: FeatureContext = FeatureContext.DEFAULT) -> EnvironmentInfo:
+        """
+        Detect environment with optional feature-specific context.
+        
+        Primary entry point for environment detection. Collects signals from
+        environment variables, system indicators, and hostname patterns, then
+        applies confidence scoring and feature-specific overrides.
+        
+        Args:
+            feature_context: Feature-specific context for specialized detection.
+                           Defaults to FeatureContext.DEFAULT for standard detection.
+                           Use specific contexts like AI_ENABLED or SECURITY_ENFORCEMENT
+                           for feature-aware detection.
+        
+        Returns:
+            EnvironmentInfo containing:
+            - environment: Detected Environment enum value
+            - confidence: Float from 0.0-1.0 indicating detection confidence
+            - reasoning: Human-readable explanation of detection decision
+            - detected_by: Primary signal source that determined the environment
+            - feature_context: The feature context used in detection
+            - additional_signals: All signals collected during detection
+            - metadata: Feature-specific metadata and configuration hints
+        
+        Behavior:
+            - Collects environment signals from all configured sources
+            - Applies feature-specific context overrides when specified
+            - Uses confidence scoring to resolve conflicting signals
+            - Returns development environment as fallback when no signals found
+            - Caches detection results for performance optimization
+            - Logs detection decisions for debugging and monitoring
+        
+        Examples:
+            >>> detector = EnvironmentDetector()
+            >>>
+            >>> # Basic detection
+            >>> env_info = detector.detect_environment()
+            >>> assert env_info.environment in Environment
+            >>> assert 0.0 <= env_info.confidence <= 1.0
+            >>>
+            >>> # Feature-aware detection
+            >>> ai_env = detector.detect_environment(FeatureContext.AI_ENABLED)
+            >>> if ai_env.metadata.get('ai_prefix'):
+            ...     cache_key = f"{ai_env.metadata['ai_prefix']}cache-key"
+            >>>
+            >>> # High confidence check
+            >>> if env_info.confidence > 0.8:
+            ...     configure_production_features()
+            >>> else:
+            ...     logger.warning(f"Low confidence: {env_info.reasoning}")
+        """
+        ...
+
+    def detect_with_context(self, feature_context: FeatureContext) -> EnvironmentInfo:
+        """
+        Detect environment with specific feature context and specialized logic.
+        
+        Performs feature-aware environment detection that considers specific
+        infrastructure requirements. May override standard detection logic
+        based on feature-specific environment variables and configuration.
+        
+        Args:
+            feature_context: Specific feature context for specialized detection.
+                           Must be a valid FeatureContext enum value.
+                           Different contexts apply different detection rules:
+                           - AI_ENABLED: Checks ENABLE_AI_CACHE, may add 'ai-' prefix
+                           - SECURITY_ENFORCEMENT: May override to production if ENFORCE_AUTH=true
+                           - DEFAULT: Standard detection without feature-specific overrides
+        
+        Returns:
+            EnvironmentInfo with feature-aware detection results containing:
+            - environment: Final determined environment (may be overridden by feature context)
+            - confidence: Confidence score considering feature-specific signals
+            - feature_context: The specific feature context used
+            - metadata: Feature-specific configuration hints and overrides
+            - additional_signals: All signals including feature-specific ones
+        
+        Behavior:
+            - Collects standard environment detection signals
+            - Applies feature-specific environment variable checks
+            - May override environment based on feature requirements
+            - Adds feature-specific metadata for configuration hints
+            - Combines confidence scores from multiple signal sources
+            - Logs feature-specific detection decisions
+        
+        Examples:
+            >>> detector = EnvironmentDetector()
+            >>>
+            >>> # Security enforcement may override to production
+            >>> security_env = detector.detect_with_context(FeatureContext.SECURITY_ENFORCEMENT)
+            >>> if security_env.environment == Environment.PRODUCTION:
+            ...     enforce_authentication_requirements()
+            >>>
+            >>> # AI context provides cache prefix hints
+            >>> ai_env = detector.detect_with_context(FeatureContext.AI_ENABLED)
+            >>> cache_prefix = ai_env.metadata.get('ai_prefix', '')
+            >>> cache_key = f"{cache_prefix}summarize:{text_hash}"
+            >>>
+            >>> # Feature-specific confidence assessment
+            >>> if ai_env.confidence > 0.9 and 'enable_ai_cache_enabled' in ai_env.metadata:
+            ...     use_ai_optimized_cache_settings()
+        """
+        ...
+
+    def get_environment_summary(self) -> Dict[str, Any]:
+        """
+        Get comprehensive environment detection summary with all signals and metadata.
+        
+        Provides detailed information about environment detection including
+        all collected signals, confidence scores, and metadata. Useful for
+        debugging detection issues and understanding how the environment was determined.
+        
+        Returns:
+            Dictionary containing comprehensive detection information:
+            - 'detected_environment': Final environment name as string
+            - 'confidence': Overall confidence score (0.0-1.0)
+            - 'reasoning': Human-readable explanation of detection decision
+            - 'detected_by': Primary detection mechanism
+            - 'all_signals': List of all signals with source, value, confidence
+            - 'metadata': Feature-specific metadata and configuration hints
+        
+        Behavior:
+            - Performs full environment detection with default context
+            - Formats all signals for human-readable output
+            - Includes both primary and additional signals
+            - Preserves original signal confidence scores
+            - Provides structured data for programmatic analysis
+        
+        Examples:
+            >>> detector = EnvironmentDetector()
+            >>> summary = detector.get_environment_summary()
+            >>>
+            >>> # Check detection results
+            >>> print(f"Environment: {summary['detected_environment']}")
+            >>> print(f"Confidence: {summary['confidence']:.2f}")
+            >>> print(f"Reasoning: {summary['reasoning']}")
+            >>>
+            >>> # Analyze all signals
+            >>> for signal in summary['all_signals']:
+            ...     print(f"  {signal['source']}: {signal['reasoning']}")
+            >>>
+            >>> # Debug low confidence detection
+            >>> if summary['confidence'] < 0.7:
+            ...     logger.warning(f"Low confidence detection: {summary['reasoning']}")
+            ...     for signal in summary['all_signals']:
+            ...         logger.info(f"Signal: {signal['source']} -> {signal['environment']} ({signal['confidence']})")
+        """
+        ...
diff --git a/backend/contracts/core/environment/enums.pyi b/backend/contracts/core/environment/enums.pyi
new file mode 100644
index 0000000..503074d
--- /dev/null
+++ b/backend/contracts/core/environment/enums.pyi
@@ -0,0 +1,63 @@
+"""
+Environment and feature context enumerations.
+
+Contains the core enums for environment classification and feature-specific contexts
+used throughout the environment detection system.
+"""
+
+from enum import Enum
+
+
+class Environment(str, Enum):
+    """
+    Standard environment classifications for application deployment contexts.
+    
+    Provides consistent environment naming across all backend infrastructure
+    services including cache, resilience, security, and monitoring systems.
+    
+    Values:
+        DEVELOPMENT: Local development and testing environments
+        TESTING: Automated testing and CI environments
+        STAGING: Pre-production environments for integration testing
+        PRODUCTION: Live production environments serving real users
+        UNKNOWN: Fallback when environment cannot be determined
+    
+    Examples:
+        >>> env = Environment.PRODUCTION
+        >>> assert env.value == "production"
+        >>> assert str(env) == "production"
+    
+        >>> # Used in configuration
+        >>> if env == Environment.PRODUCTION:
+        ...     use_redis_cache = True
+    """
+
+    ...
+
+
+class FeatureContext(str, Enum):
+    """
+    Feature-specific context for specialized environment detection.
+    
+    Enables feature-aware environment detection that considers specific
+    infrastructure requirements like AI processing, security enforcement,
+    or cache optimization when determining appropriate environment settings.
+    
+    Values:
+        AI_ENABLED: Context for AI-powered features requiring model access
+        SECURITY_ENFORCEMENT: Context for security-critical features
+        CACHE_OPTIMIZATION: Context for cache-intensive operations
+        RESILIENCE_STRATEGY: Context for resilience pattern selection
+        DEFAULT: Standard environment detection without feature context
+    
+    Examples:
+        >>> # Basic usage
+        >>> context = FeatureContext.AI_ENABLED
+        >>> assert context.value == "ai_enabled"
+    
+        >>> # Used with environment detection
+        >>> env_info = detector.detect_with_context(FeatureContext.SECURITY_ENFORCEMENT)
+        >>> # May return production environment even in dev if security is enforced
+    """
+
+    ...
diff --git a/backend/contracts/core/environment/feature_contexts.pyi b/backend/contracts/core/environment/feature_contexts.pyi
new file mode 100644
index 0000000..71f1c0b
--- /dev/null
+++ b/backend/contracts/core/environment/feature_contexts.pyi
@@ -0,0 +1,105 @@
+"""
+Feature-specific context handling.
+
+Contains logic for applying feature-specific context and overrides to environment
+detection, including AI-specific settings, security enforcement, and cache optimization.
+"""
+
+import os
+import logging
+from typing import List, Dict, Any
+from .enums import Environment, FeatureContext
+from .models import EnvironmentSignal, DetectionConfig
+
+
+def apply_feature_context(signals: List[EnvironmentSignal], context: FeatureContext, config: DetectionConfig) -> Dict[str, Any]:
+    """
+    Apply feature-specific context and overrides to detection signals.
+    
+    Processes feature-specific environment variables and configuration
+    to modify detection behavior. May add additional signals or metadata
+    based on feature requirements like AI enabling or security enforcement.
+    
+    Args:
+        signals: Base environment signals from standard detection
+        context: Feature context specifying specialized detection logic
+        config: Detection configuration with feature context settings
+    
+    Returns:
+        Dictionary containing:
+        - 'signals': Original signals list (unmodified)
+        - 'additional_signals': Feature-specific signals to add
+        - 'metadata': Feature-specific metadata and configuration hints
+    
+    Behavior:
+        - Returns original signals unchanged for DEFAULT context
+        - Checks feature-specific environment variables when configured
+        - Adds metadata hints for cache prefixes, security overrides, etc.
+        - May inject additional signals for feature-specific overrides
+        - Preserves original signal list while adding context-specific data
+        - Logs feature-specific detection decisions
+    
+    Examples:
+        >>> config = DetectionConfig()
+        >>> base_signals = [EnvironmentSignal(...)]  # from standard detection
+        >>>
+        >>> # AI context adds cache prefix metadata
+        >>> result = apply_feature_context(base_signals, FeatureContext.AI_ENABLED, config)
+        >>> if result['metadata'].get('ai_prefix'):
+        ...     cache_key = f"{result['metadata']['ai_prefix']}operation-key"
+        >>>
+        >>> # Security context may add production override signal
+        >>> security_result = apply_feature_context(
+        ...     base_signals, FeatureContext.SECURITY_ENFORCEMENT, config
+        ... )
+        >>> additional = security_result['additional_signals']
+        >>> security_overrides = [s for s in additional if s.source == 'security_override']
+    """
+    ...
+
+
+def determine_environment(signals: List[EnvironmentSignal]) -> Dict[str, Any]:
+    """
+    Determine final environment from all collected signals using confidence scoring.
+    
+    Analyzes all environment signals to make a final environment determination.
+    Uses confidence scoring, conflict resolution, and fallback logic to
+    ensure reliable environment detection even with contradictory signals.
+    
+    Args:
+        signals: All environment signals collected from detection sources
+    
+    Returns:
+        Dictionary containing final environment determination:
+        - 'environment': Final Environment enum value
+        - 'confidence': Combined confidence score (0.0-1.0)
+        - 'reasoning': Human-readable explanation of decision
+        - 'detected_by': Primary signal source that determined environment
+    
+    Behavior:
+        - Returns development fallback if no signals provided
+        - Sorts signals by confidence score (highest first)
+        - Uses highest confidence signal as primary determination
+        - Boosts confidence when multiple signals agree
+        - Reduces confidence when high-confidence conflicts exist
+        - Generates comprehensive reasoning including conflicts
+        - Caps combined confidence at 0.98 to indicate uncertainty
+    
+    Examples:
+        >>> # Multiple agreeing signals boost confidence
+        >>> signals = [
+        ...     EnvironmentSignal(..., environment=Environment.PRODUCTION, confidence=0.85),
+        ...     EnvironmentSignal(..., environment=Environment.PRODUCTION, confidence=0.70)
+        ... ]
+        >>> result = determine_environment(signals)
+        >>> assert result['confidence'] > 0.85  # Boosted by agreement
+        >>>
+        >>> # Conflicting signals reduce confidence
+        >>> conflicting_signals = [
+        ...     EnvironmentSignal(..., environment=Environment.PRODUCTION, confidence=0.85),
+        ...     EnvironmentSignal(..., environment=Environment.DEVELOPMENT, confidence=0.75)
+        ... ]
+        >>> result = determine_environment(conflicting_signals)
+        >>> assert result['confidence'] < 0.85  # Reduced by conflict
+    """
+    ...
diff --git a/backend/contracts/core/environment/models.pyi b/backend/contracts/core/environment/models.pyi
new file mode 100644
index 0000000..3eaad9b
--- /dev/null
+++ b/backend/contracts/core/environment/models.pyi
@@ -0,0 +1,116 @@
+"""
+Data models for environment detection.
+
+Contains dataclasses and data structures used in environment detection including
+EnvironmentSignal, EnvironmentInfo, and DetectionConfig.
+"""
+
+from dataclasses import dataclass, field
+from typing import Dict, List, Optional, Any, NamedTuple
+from .enums import Environment, FeatureContext
+
+
+class EnvironmentSignal(NamedTuple):
+    """
+    Single environment detection signal with confidence scoring and reasoning.
+    
+    Represents one piece of evidence used in environment detection, such as
+    an environment variable, system indicator, or hostname pattern match.
+    Each signal includes confidence scoring to enable weighted decision making.
+    
+    Attributes:
+        source: Detection mechanism that generated this signal (e.g., "env_var", "hostname_pattern")
+        value: Raw value that triggered the detection (e.g., "production", "staging.example.com")
+        environment: Environment classification this signal indicates
+        confidence: Confidence score from 0.0-1.0 for this detection
+        reasoning: Human-readable explanation of why this signal indicates the environment
+    
+    Examples:
+        >>> signal = EnvironmentSignal(
+        ...     source="ENVIRONMENT",
+        ...     value="production",
+        ...     environment=Environment.PRODUCTION,
+        ...     confidence=0.95,
+        ...     reasoning="Explicit environment from ENVIRONMENT=production"
+        ... )
+        >>> assert signal.confidence > 0.9
+        >>> assert signal.environment == Environment.PRODUCTION
+    """
+
+    ...
+
+
+@dataclass
+class EnvironmentInfo:
+    """
+    Comprehensive environment detection result with confidence scoring and metadata.
+    
+    Contains the final environment determination along with confidence scoring,
+    reasoning, and supporting evidence. Includes feature-specific context and
+    metadata for advanced use cases like preset selection or security overrides.
+    
+    Attributes:
+        environment: Final determined environment classification
+        confidence: Overall confidence score from 0.0-1.0 for the detection
+        reasoning: Human-readable explanation of the detection decision
+        detected_by: Primary detection mechanism that determined the environment
+        feature_context: Feature-specific context used in detection
+        additional_signals: All environment signals collected during detection
+        metadata: Feature-specific metadata and configuration hints
+    
+    Usage:
+        # Basic environment checking
+        env_info = detector.detect_environment()
+        if env_info.environment == Environment.PRODUCTION and env_info.confidence > 0.8:
+            enable_production_features()
+    
+        # Feature-aware detection
+        ai_env = detector.detect_with_context(FeatureContext.AI_ENABLED)
+        if 'ai_prefix' in ai_env.metadata:
+            cache_key_prefix = ai_env.metadata['ai_prefix']
+    
+        # Debugging detection issues
+        print(f"Environment: {env_info}")
+        print(f"Reasoning: {env_info.reasoning}")
+        for signal in env_info.additional_signals:
+            print(f"  - {signal.source}: {signal.reasoning}")
+    """
+
+    def __str__(self) -> str:
+        ...
+
+
+@dataclass
+class DetectionConfig:
+    """
+    Configuration for environment detection behavior and patterns.
+    
+    Controls how the EnvironmentDetector identifies environments through
+    environment variables, patterns, indicators, and feature-specific overrides.
+    Allows customization of detection logic for different deployment scenarios.
+    
+    Attributes:
+        env_var_precedence: Environment variables checked in priority order
+        development_patterns: Regex patterns indicating development environments
+        staging_patterns: Regex patterns indicating staging environments
+        production_patterns: Regex patterns indicating production environments
+        development_indicators: System indicators suggesting development
+        production_indicators: System indicators suggesting production
+        feature_contexts: Feature-specific configuration overrides
+    
+    Examples:
+        # Custom configuration for specialized deployment
+        config = DetectionConfig(
+            env_var_precedence=['CUSTOM_ENV', 'ENVIRONMENT'],
+            production_patterns=[r'.*live.*', r'.*prod.*']
+        )
+        detector = EnvironmentDetector(config)
+    
+        # Add custom feature context
+        config.feature_contexts[FeatureContext.AI_ENABLED] = {
+            'environment_var': 'ENABLE_AI_FEATURES',
+            'true_values': ['true', '1', 'enabled']
+        }
+    """
+
+    ...
diff --git a/backend/contracts/core/environment/patterns.pyi b/backend/contracts/core/environment/patterns.pyi
new file mode 100644
index 0000000..25b39ce
--- /dev/null
+++ b/backend/contracts/core/environment/patterns.pyi
@@ -0,0 +1,214 @@
+"""
+Pattern matching and system indicator detection.
+
+Contains functions and classes for detecting environment signals from:
+- Environment variables
+- System indicators (file presence, debug flags)
+- Hostname/URL pattern matching
+"""
+
+import os
+import re
+import logging
+from pathlib import Path
+from typing import List
+from .enums import Environment
+from .models import EnvironmentSignal, DetectionConfig
+
+
+def collect_detection_signals(config: DetectionConfig) -> List[EnvironmentSignal]:
+    """
+    Collect all available environment detection signals from configured sources.
+    
+    Gathers environment detection evidence from environment variables,
+    system indicators, and hostname patterns. Signals are collected in
+    priority order with confidence scoring for later resolution.
+    
+    Args:
+        config: Detection configuration with patterns and precedence settings
+        
+    Returns:
+        List of EnvironmentSignal objects, each containing:
+        - source: Detection mechanism (env_var, system_indicator, hostname_pattern)
+        - value: Raw value that triggered the detection
+        - environment: Environment indicated by this signal
+        - confidence: Confidence score for this detection
+        - reasoning: Human-readable explanation
+    
+    Behavior:
+        - Checks environment variables in configured precedence order
+        - Examines system indicators like DEBUG flags and file presence
+        - Applies hostname pattern matching for containerized deployments
+        - Returns empty list if no detection signals are found
+        - Maintains signal order for debugging and analysis
+    
+    Examples:
+        >>> config = DetectionConfig()
+        >>> signals = collect_detection_signals(config)
+        >>>
+        >>> # Examine signal sources
+        >>> env_var_signals = [s for s in signals if s.source in ['ENVIRONMENT', 'NODE_ENV']]
+        >>> pattern_signals = [s for s in signals if s.source == 'hostname_pattern']
+        >>>
+        >>> # Check signal confidence
+        >>> high_confidence = [s for s in signals if s.confidence > 0.8]
+    """
+    ...
+
+
+def detect_from_env_vars(config: DetectionConfig) -> List[EnvironmentSignal]:
+    """
+    Detect environment from environment variables using configured precedence.
+    
+    Checks environment variables in priority order, with ENVIRONMENT having
+    highest precedence and framework-specific variables (NODE_ENV, FLASK_ENV)
+    having lower precedence. Provides direct string-to-environment mapping.
+    
+    Args:
+        config: Detection configuration with env_var_precedence settings
+        
+    Returns:
+        List of EnvironmentSignal objects from environment variable detection.
+        Signals are ordered by precedence with highest confidence for
+        ENVIRONMENT variable and decreasing confidence for others.
+    
+    Behavior:
+        - Iterates through env_var_precedence list in order
+        - Maps common environment values (dev, prod, test, staging) to Environment enums
+        - Assigns confidence scores based on variable precedence
+        - Skips empty or undefined environment variables
+        - Returns empty list if no recognized environment variables found
+    
+    Examples:
+        >>> config = DetectionConfig()
+        >>> # With ENVIRONMENT=production set
+        >>> signals = detect_from_env_vars(config)
+        >>> assert len(signals) >= 1
+        >>> assert signals[0].environment == Environment.PRODUCTION
+        >>> assert signals[0].confidence >= 0.95
+        >>>
+        >>> # With NODE_ENV=development
+        >>> # (lower confidence than ENVIRONMENT)
+        >>> assert any(s.confidence < 0.95 for s in signals if s.source == 'NODE_ENV')
+    """
+    ...
+
+
+def detect_from_system_indicators(config: DetectionConfig) -> List[EnvironmentSignal]:
+    """
+    Detect environment from system indicators and file presence.
+    
+    Examines system-level indicators like DEBUG flags, file presence
+    (e.g., .env, .git, docker-compose files), and production indicators
+    to infer the current environment context.
+    
+    Args:
+        config: Detection configuration with indicator settings
+        
+    Returns:
+        List of EnvironmentSignal objects from system indicator detection.
+        Each signal represents one system indicator with appropriate
+        confidence scoring based on indicator reliability.
+    
+    Behavior:
+        - Checks development indicators (DEBUG=true, .env files, .git directories)
+        - Checks production indicators (DEBUG=false, PRODUCTION=true)
+        - Validates file existence for file-based indicators
+        - Validates environment variable values for variable-based indicators
+        - Assigns moderate confidence scores (0.70-0.75) as these are indirect signals
+        - Returns empty list if no system indicators are found
+    
+    Examples:
+        >>> config = DetectionConfig()
+        >>> # In development environment with .env file
+        >>> signals = detect_from_system_indicators(config)
+        >>> env_file_signals = [s for s in signals if s.value == '.env']
+        >>> if env_file_signals:
+        ...     assert env_file_signals[0].environment == Environment.DEVELOPMENT
+        >>>
+        >>> # With DEBUG=false in production
+        >>> debug_signals = [s for s in signals if 'DEBUG=false' in s.value]
+        >>> if debug_signals:
+        ...     assert debug_signals[0].environment == Environment.PRODUCTION
+    """
+    ...
+
+
+def check_indicator(indicator: str) -> bool:
+    """
+    Check if a system indicator is present in the environment.
+    
+    Validates both environment variable indicators (KEY=value format)
+    and file-based indicators (file paths) to determine if the
+    specified system condition exists.
+    
+    Args:
+        indicator: System indicator to check. Format depends on type:
+                  - Environment variable: "VAR_NAME=expected_value"
+                  - File existence: "path/to/file" or "filename"
+    
+    Returns:
+        True if the indicator condition is met, False otherwise.
+        For environment variables: True if variable equals expected value
+        For files: True if file exists at the specified path
+    
+    Behavior:
+        - Detects indicator type by presence of '=' character
+        - For env vars: performs case-insensitive value comparison
+        - For files: checks file existence using Path.exists()
+        - Returns False for malformed indicators or missing conditions
+        - Does not raise exceptions for invalid paths or variables
+        - Logs warnings for file system access errors but continues gracefully
+    
+    Examples:
+        >>> # Environment variable check
+        >>> # If DEBUG=true is set
+        >>> assert check_indicator("DEBUG=true") == True
+        >>> assert check_indicator("DEBUG=false") == False
+        >>>
+        >>> # File existence check
+        >>> # If .env file exists in current directory
+        >>> assert check_indicator(".env") == True
+        >>> assert check_indicator("nonexistent.file") == False
+    """
+    ...
+
+
+def detect_from_patterns(config: DetectionConfig) -> List[EnvironmentSignal]:
+    """
+    Detect environment from hostname and URL patterns.
+    
+    Applies regex pattern matching to hostname and URL values to identify
+    environment context. Particularly useful in containerized deployments
+    where hostname or service names follow naming conventions.
+    
+    Args:
+        config: Detection configuration with pattern settings
+        
+    Returns:
+        List of EnvironmentSignal objects from pattern matching.
+        Signals include the matched hostname/URL and the pattern that
+        identified the environment context.
+    
+    Behavior:
+        - Retrieves hostname from HOSTNAME environment variable
+        - Applies regex patterns for development, staging, and production
+        - Uses case-insensitive matching for broader compatibility
+        - Stops at first match per pattern category for efficiency
+        - Assigns moderate confidence (0.60-0.70) as patterns can be ambiguous
+        - Returns empty list if no HOSTNAME set or no patterns match
+        - Logs warnings for invalid regex patterns but continues with valid patterns
+    
+    Examples:
+        >>> config = DetectionConfig()
+        >>> # With HOSTNAME=api-prod-01.example.com
+        >>> signals = detect_from_patterns(config)
+        >>> prod_signals = [s for s in signals if s.environment == Environment.PRODUCTION]
+        >>> if prod_signals:
+        ...     assert 'prod' in prod_signals[0].value.lower()
+        >>>
+        >>> # With HOSTNAME=dev-service.local
+        >>> # Should match development patterns
+        >>> dev_signals = [s for s in signals if s.environment == Environment.DEVELOPMENT]
+    """
+    ...
diff --git a/backend/contracts/core/startup/__init__.pyi b/backend/contracts/core/startup/__init__.pyi
new file mode 100644
index 0000000..f027528
--- /dev/null
+++ b/backend/contracts/core/startup/__init__.pyi
@@ -0,0 +1,10 @@
+"""
+Application startup validation and initialization.
+
+This module provides startup validation services for critical application
+components including security, configuration, and infrastructure validation.
+"""
+
+from .redis_security import RedisSecurityValidator, validate_redis_security
+
+__all__ = ['RedisSecurityValidator', 'validate_redis_security']
diff --git a/backend/contracts/core/startup/redis_security.pyi b/backend/contracts/core/startup/redis_security.pyi
new file mode 100644
index 0000000..4af1312
--- /dev/null
+++ b/backend/contracts/core/startup/redis_security.pyi
@@ -0,0 +1,255 @@
+"""
+Redis security validation for application startup.
+
+This module provides fail-fast Redis security validation that ensures secure
+connections are mandatory in production environments while allowing flexibility
+in development contexts.
+
+## Security Philosophy
+
+This implementation follows a security-first approach where:
+- Production environments MUST use TLS encryption and authentication
+- Development environments are encouraged but not required to use TLS
+- Explicit insecure overrides are available with prominent warnings
+- Application fails immediately if security requirements aren't met
+
+## Usage
+
+```python
+from app.core.startup.redis_security import RedisSecurityValidator
+
+# Initialize validator
+validator = RedisSecurityValidator()
+
+# Validate Redis security configuration
+validator.validate_production_security(
+    redis_url="rediss://redis:6380",
+    insecure_override=False
+)
+
+# Comprehensive configuration validation
+validation_report = validator.validate_security_configuration(
+    redis_url="rediss://redis:6380",
+    encryption_key="your-fernet-key",
+    tls_cert_path="/path/to/cert.pem"
+)
+print(validation_report.summary())
+```
+"""
+
+import logging
+import os
+from typing import Optional, Dict, Any, List
+from dataclasses import dataclass, field
+from pathlib import Path
+from datetime import datetime, timedelta
+from app.core.environment import get_environment_info, Environment, FeatureContext
+from app.core.exceptions import ConfigurationError
+
+
+@dataclass
+class SecurityValidationResult:
+    """
+    Results from comprehensive security configuration validation.
+    
+    Attributes:
+        is_valid: Overall validation status
+        tls_status: TLS configuration validation status
+        encryption_status: Encryption key validation status
+        auth_status: Authentication configuration status
+        connectivity_status: Redis connectivity test status
+        warnings: List of security warnings
+        errors: List of validation errors
+        recommendations: Security improvement recommendations
+        certificate_info: TLS certificate information if available
+    """
+
+    def summary(self) -> str:
+        """
+        Generate human-readable validation summary.
+        """
+        ...
+
+
+class RedisSecurityValidator:
+    """
+    Validates Redis security configuration at application startup.
+    
+    This validator enforces security-first Redis connections with environment-aware
+    requirements. Production environments must use secure connections while
+    development environments have flexibility with explicit override support.
+    
+    The validator integrates with the existing environment detection system to
+    provide appropriate security levels for different deployment contexts.
+    """
+
+    def __init__(self):
+        """
+        Initialize the Redis security validator.
+        """
+        ...
+
+    def validate_production_security(self, redis_url: str, insecure_override: bool = False) -> None:
+        """
+        Validate Redis security for production environments.
+        
+        This method enforces TLS encryption and authentication requirements in
+        production while allowing development flexibility. It provides clear
+        error messages and configuration guidance when security requirements
+        are not met.
+        
+        Args:
+            redis_url: Redis connection string to validate
+            insecure_override: Explicit override to allow insecure connections
+                             in production (logs prominent security warning)
+        
+        Raises:
+            ConfigurationError: If production environment lacks required security
+                              and no explicit override is provided
+        
+        Examples:
+            # Production validation (must be secure)
+            validator.validate_production_security("rediss://redis:6380")
+        
+            # Development validation (flexible)
+            validator.validate_production_security("redis://localhost:6379")
+        
+            # Production with override (warning logged)
+            validator.validate_production_security(
+                "redis://internal:6379",
+                insecure_override=True
+            )
+        """
+        ...
+
+    def validate_tls_certificates(self, cert_path: Optional[str] = None, key_path: Optional[str] = None, ca_path: Optional[str] = None) -> Dict[str, Any]:
+        """
+        Validate TLS certificate files and their properties.
+        
+        Args:
+            cert_path: Path to client certificate file
+            key_path: Path to private key file
+            ca_path: Path to CA certificate file
+        
+        Returns:
+            Dictionary with validation results and certificate info
+        
+        Examples:
+            result = validator.validate_tls_certificates(
+                cert_path="/path/to/cert.pem",
+                key_path="/path/to/key.pem",
+                ca_path="/path/to/ca.pem"
+            )
+        """
+        ...
+
+    def validate_encryption_key(self, encryption_key: Optional[str] = None) -> Dict[str, Any]:
+        """
+        Validate encryption key format and strength.
+        
+        Args:
+            encryption_key: Fernet encryption key to validate
+        
+        Returns:
+            Dictionary with validation results
+        
+        Examples:
+            result = validator.validate_encryption_key("your-fernet-key")
+        """
+        ...
+
+    def validate_redis_auth(self, redis_url: str, auth_password: Optional[str] = None) -> Dict[str, Any]:
+        """
+        Validate Redis authentication configuration.
+        
+        Args:
+            redis_url: Redis connection URL
+            auth_password: Redis AUTH password (if not in URL)
+        
+        Returns:
+            Dictionary with validation results
+        
+        Examples:
+            result = validator.validate_redis_auth("redis://user:pass@localhost:6379")
+        """
+        ...
+
+    def validate_security_configuration(self, redis_url: str, encryption_key: Optional[str] = None, tls_cert_path: Optional[str] = None, tls_key_path: Optional[str] = None, tls_ca_path: Optional[str] = None, auth_password: Optional[str] = None, test_connectivity: bool = False) -> SecurityValidationResult:
+        """
+        Perform comprehensive security configuration validation.
+        
+        This method validates all aspects of Redis security configuration including
+        TLS, encryption, authentication, and optionally connectivity testing.
+        
+        Args:
+            redis_url: Redis connection URL
+            encryption_key: Fernet encryption key
+            tls_cert_path: Path to TLS client certificate
+            tls_key_path: Path to TLS private key
+            tls_ca_path: Path to TLS CA certificate
+            auth_password: Redis authentication password
+            test_connectivity: Whether to test actual Redis connectivity
+        
+        Returns:
+            SecurityValidationResult with detailed validation information
+        
+        Examples:
+            # Full validation
+            result = validator.validate_security_configuration(
+                redis_url="rediss://redis:6380",
+                encryption_key=os.getenv("REDIS_ENCRYPTION_KEY"),
+                tls_cert_path="/path/to/cert.pem",
+                test_connectivity=True
+            )
+            print(result.summary())
+        """
+        ...
+
+    def validate_startup_security(self, redis_url: str, insecure_override: Optional[bool] = None) -> None:
+        """
+        Comprehensive Redis security validation for application startup.
+        
+        This is the main entry point for startup security validation. It performs
+        environment-aware security validation and provides clear feedback on
+        security status.
+        
+        Args:
+            redis_url: Redis connection string to validate
+            insecure_override: Optional explicit override for insecure connections
+                             If None, will be determined from environment variables
+        
+        Raises:
+            ConfigurationError: If security requirements are not met
+        
+        Examples:
+            # Standard startup validation
+            validator.validate_startup_security("rediss://redis:6380")
+        
+            # With explicit override
+            validator.validate_startup_security("redis://redis:6379", insecure_override=True)
+        """
+        ...
+
+
+def validate_redis_security(redis_url: str, insecure_override: Optional[bool] = None) -> None:
+    """
+    Convenience function for Redis security validation.
+    
+    This function provides a simple interface for startup security validation
+    without requiring explicit validator instantiation.
+    
+    Args:
+        redis_url: Redis connection string to validate
+        insecure_override: Optional explicit override for insecure connections
+    
+    Raises:
+        ConfigurationError: If security requirements are not met
+    
+    Examples:
+        # Simple validation
+        validate_redis_security("rediss://redis:6380")
+    
+        # With override
+        validate_redis_security("redis://redis:6379", insecure_override=True)
+    """
+    ...
diff --git a/backend/contracts/infrastructure/__init__.pyi b/backend/contracts/infrastructure/__init__.pyi
index 3a9d5b0..8c7c5d0 100644
--- a/backend/contracts/infrastructure/__init__.pyi
+++ b/backend/contracts/infrastructure/__init__.pyi
@@ -210,11 +210,10 @@ HEALTH_CHECK_TIMEOUT_MS=500
 
 ## Migration & Compatibility
 
-### Backward Compatibility
-The infrastructure maintains backward compatibility while adding new features:
-- **Legacy API Support**: Existing code continues to work without modification
-- **Gradual Migration**: New features can be adopted incrementally
-- **Configuration Migration**: Tools for migrating legacy configurations
+### Preset-Based Configuration
+The infrastructure uses preset-based configuration for simplified setup:
+- **Modern Configuration**: Preset-based approach replaces complex environment variable configurations
+- **Gradual Adoption**: New features can be adopted incrementally through preset selection
 - **Version Management**: Clear versioning for infrastructure components
 
 ### Future Roadmap
diff --git a/backend/contracts/infrastructure/cache/__init__.pyi b/backend/contracts/infrastructure/cache/__init__.pyi
index 4406d51..bd3aafb 100644
--- a/backend/contracts/infrastructure/cache/__init__.pyi
+++ b/backend/contracts/infrastructure/cache/__init__.pyi
@@ -1,26 +1,36 @@
 """
-**Comprehensive cache infrastructure with multiple implementations and monitoring.**
+**Security-first cache infrastructure with comprehensive Redis and in-memory implementations.**
 
 This module serves as the central entry point for all cache-related functionality,
-providing multiple cache implementations, configuration management, and comprehensive
-monitoring capabilities for both web and AI applications.
+providing multiple cache implementations with mandatory security, configuration management,
+and comprehensive monitoring capabilities for both web and AI applications.
+
+## Security-First Architecture
+
+**All Redis connections enforce mandatory security:**
+- **TLS Encryption**: Required for all Redis connections (rediss:// protocol)
+- **Authentication**: Strong password authentication mandatory
+- **Data Encryption**: Always-on Fernet encryption for cached data
+- **Environment-Aware**: Security levels adapt per environment (development, staging, production)
 
 ## Directory Structure
 
 The cache module is organized into specialized components:
 
 - **Core Implementations**: `base.py`, `memory.py`, `redis_generic.py`, `redis_ai.py`
-- **Configuration**: `config.py`, `ai_config.py`, `cache_presets.py`, `dependencies.py`
-- **Utilities**: `factory.py`, `key_generator.py`, `parameter_mapping.py`
-- **Advanced Features**: `monitoring.py`, `security.py`
+- **Security & Configuration**: `security.py`, `config.py`, `ai_config.py`, `cache_presets.py`
+- **Utilities**: `factory.py`, `key_generator.py`, `parameter_mapping.py`, `dependencies.py`
+- **Monitoring**: `monitoring.py`
 
 ## Main Components
 
 - **CacheInterface**: Abstract base class for all cache implementations
-- **AIResponseCache**: AI-optimized Redis cache with intelligent key generation
-- **GenericRedisCache**: Flexible Redis cache with L1 memory cache
+- **AIResponseCache**: AI-optimized Redis cache with security and intelligent key generation
+- **GenericRedisCache**: Flexible Redis cache with mandatory security and L1 memory cache
 - **InMemoryCache**: High-performance in-memory cache with TTL and LRU eviction
-- **CacheFactory**: Explicit cache creation with environment-optimized defaults
+- **SecurityConfig**: Mandatory security configuration with TLS and encryption
+- **RedisCacheSecurityManager**: Security validation and enforcement
+- **CacheFactory**: Explicit cache creation with environment-optimized secure defaults
 - **CacheConfig**: Comprehensive configuration management with preset system
 - **CachePerformanceMonitor**: Real-time monitoring and analytics
 - **FastAPI Dependencies**: Complete dependency injection with lifecycle management
@@ -28,19 +38,32 @@ The cache module is organized into specialized components:
 ## Quick Start
 
 ```python
-# Preset-based configuration (recommended)
+# Preset-based secure configuration (recommended)
 from app.infrastructure.cache.dependencies import get_cache_config
 from app.infrastructure.cache import CacheFactory
 
-config = get_cache_config()  # Uses CACHE_PRESET environment variable
+# Uses CACHE_PRESET environment variable with automatic secure Redis setup
+config = get_cache_config()
 cache = CacheFactory.create_cache_from_config(config)
 
-# Standard cache operations
+# Standard cache operations with transparent encryption
 await cache.set("key", {"data": "value"}, ttl=3600)
 result = await cache.get("key")
 ```
 
+## Security Setup
+
+For one-command secure Redis setup:
+```bash
+# Automated secure setup (generates certificates, passwords, encryption keys)
+./scripts/setup-secure-redis.sh
+
+# Or manually generate configuration
+python scripts/generate-secure-env.py --environment production
+```
+
 See the component README.md for comprehensive usage examples and configuration details.
+See docs/guides/infrastructure/cache/security.md for security architecture and setup.
 """
 
 from .base import CacheInterface
diff --git a/backend/contracts/infrastructure/cache/ai_config.pyi b/backend/contracts/infrastructure/cache/ai_config.pyi
index d3bb6f0..65acf53 100644
--- a/backend/contracts/infrastructure/cache/ai_config.pyi
+++ b/backend/contracts/infrastructure/cache/ai_config.pyi
@@ -3,7 +3,7 @@ AI Response Cache Configuration Module
 
 This module provides comprehensive configuration management for the AI Response Cache
 with advanced validation, factory methods, and environment integration. It supports
-the Phase 2 cache refactoring architecture where AIResponseCache inherits from
+the cache architecture where AIResponseCache inherits from
 GenericRedisCache, enabling clean parameter separation and robust configuration.
 
 ## Classes
@@ -47,7 +47,7 @@ Comprehensive configuration dataclass for AI cache settings with:
 - `compression_threshold`: Size threshold for automatic compression (0-1MB)
 - `compression_level`: Zlib compression level for bandwidth optimization (1-9)
 - `performance_monitor`: CachePerformanceMonitor instance for metrics collection
-- `security_config`: Optional security configuration for encrypted connections
+- `security_config`: DEPRECATED - Security is now automatic via GenericRedisCache inheritance
 
 ### AI-Specific Parameters (Unique to AI Response Caching)
 - `text_hash_threshold`: Character threshold for text hashing optimization (1-100000)
@@ -122,20 +122,6 @@ os.environ['CACHE_CUSTOM_CONFIG'] = '''{
 }'''
 ```
 
-### Legacy Environment-Based Configuration (DEPRECATED)
-```python
-# DEPRECATED: Individual environment variables no longer supported
-# Use CACHE_PRESET with preset-based configuration instead
-
-# OLD WAY (no longer works):
-# os.environ['AI_CACHE_DEFAULT_TTL'] = '7200'
-# os.environ['AI_CACHE_MEMORY_CACHE_SIZE'] = '200'
-
-# NEW WAY (recommended):
-os.environ['CACHE_PRESET'] = 'ai-development'
-cache_config = settings.get_cache_config()
-```
-
 ### Configuration Presets
 ```python
 # Development configuration
@@ -202,39 +188,6 @@ This configuration system integrates seamlessly with the cache inheritance archi
 4. **Performance Optimization**: Includes intelligent recommendations for cache tuning
 5. **Environment Flexibility**: Supports Docker, Kubernetes, and cloud deployments
 
-## Migration Guide
-
-### From Legacy Configuration
-```python
-# Old direct initialization
-cache = AIResponseCache(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    memory_cache_size=100
-)
-
-# New configuration-based approach
-config = AIResponseCacheConfig(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    memory_cache_size=100
-)
-kwargs = config.to_ai_cache_kwargs()
-cache = AIResponseCache(**kwargs)
-```
-
-### Environment Variable Migration
-```bash
-# Preset-based configuration (RECOMMENDED)
-export CACHE_PRESET="ai-development"
-export CACHE_REDIS_URL="redis://localhost:6379"  # Optional override
-export ENABLE_AI_CACHE="true"  # Optional override
-
-# Legacy environment variables (DEPRECATED - no longer supported)
-# export AI_CACHE_REDIS_URL="redis://localhost:6379"
-# export AI_CACHE_DEFAULT_TTL="3600"
-# export AI_CACHE_MEMORY_CACHE_SIZE="100"
-```
 
 ## Performance Considerations
 
@@ -300,7 +253,7 @@ class AIResponseCacheConfig:
         hash_algorithm: str hash algorithm for text processing (default: 'sha256')
         operation_ttls: Dict[str, int] operation-specific TTL overrides
         performance_monitor: Optional cache performance monitor instance
-        security_config: Optional security configuration for encrypted connections
+        security_config: DEPRECATED - Security is now automatic via GenericRedisCache inheritance
     
     Public Methods:
         validate(): Comprehensive validation with detailed error reporting
diff --git a/backend/contracts/infrastructure/cache/encryption.pyi b/backend/contracts/infrastructure/cache/encryption.pyi
new file mode 100644
index 0000000..4e7219c
--- /dev/null
+++ b/backend/contracts/infrastructure/cache/encryption.pyi
@@ -0,0 +1,247 @@
+"""
+Application-layer encryption for Redis cache data.
+
+This module provides mandatory Fernet encryption for all cached data with transparent
+operation. It ensures that all data stored in Redis is encrypted at rest using
+application-layer encryption, providing an additional security layer beyond TLS.
+
+## Security Features
+
+- **Always-On Encryption**: All cache data is encrypted by default, no opt-out available
+- **Fernet Encryption**: Uses industry-standard symmetric encryption from cryptography library
+- **Key Management**: Secure encryption key management with environment-aware generation
+- **Performance Optimized**: Efficient encryption/decryption with minimal overhead
+- **Error Handling**: Graceful handling of encryption failures and key rotation scenarios
+- **Transparent Operation**: Seamless integration with existing cache operations
+
+## Usage
+
+```python
+from app.infrastructure.cache.encryption import EncryptedCacheLayer
+
+# Initialize with encryption key
+encryption = EncryptedCacheLayer("your-fernet-key")
+
+# Encrypt data before storage
+encrypted_data = encryption.encrypt_cache_data({"key": "value"})
+
+# Decrypt data after retrieval
+original_data = encryption.decrypt_cache_data(encrypted_data)
+
+# Check encryption status
+if encryption.is_enabled:
+    print("Data encryption is active")
+```
+"""
+
+import logging
+import json
+import time
+from typing import Dict, Any, Optional, Union
+from app.core.exceptions import ConfigurationError
+
+
+class EncryptedCacheLayer:
+    """
+    Handles encryption/decryption of sensitive cache data.
+    
+    This class provides application-layer encryption for all cached data using
+    Fernet symmetric encryption. It ensures that sensitive data is encrypted
+    before storage and decrypted transparently on retrieval.
+    
+    The encryption layer is designed to be always-on and provides performance
+    monitoring to ensure encryption overhead stays within acceptable limits.
+    """
+
+    def __init__(self, encryption_key: Optional[str] = None, performance_monitoring: bool = True):
+        """
+        Initialize the encrypted cache layer.
+        
+        Args:
+            encryption_key: Fernet encryption key in base64 format.
+                          If None, encryption will be disabled (not recommended).
+            performance_monitoring: Enable performance monitoring for encryption operations
+        
+        Raises:
+            ConfigurationError: If encryption key is invalid or cryptography is unavailable
+        
+        Examples:
+            # Standard initialization
+            encryption = EncryptedCacheLayer("your-fernet-key")
+        
+            # Without performance monitoring
+            encryption = EncryptedCacheLayer("your-fernet-key", performance_monitoring=False)
+        
+        Note:
+            This implementation follows the security-first approach where encryption
+            is mandatory. Passing None for encryption_key will log warnings and
+            should only be used in testing scenarios.
+        """
+        ...
+
+    def encrypt_cache_data(self, data: Dict[str, Any]) -> bytes:
+        """
+        Encrypt cache data if encryption is enabled.
+        
+        This method serializes the data to JSON and encrypts it using Fernet.
+        If encryption is disabled, it returns the JSON-serialized data as bytes
+        with a warning.
+        
+        Args:
+            data: Dictionary data to encrypt
+        
+        Returns:
+            Encrypted data as bytes
+        
+        Raises:
+            ConfigurationError: If encryption fails or data cannot be serialized
+        
+        Examples:
+            # Encrypt user data
+            encrypted = encryption.encrypt_cache_data({"user_id": 123, "name": "Alice"})
+        
+            # Encrypt AI response
+            ai_data = {"response": "Generated text", "model": "gpt-4", "timestamp": 1234567890}
+            encrypted = encryption.encrypt_cache_data(ai_data)
+        
+        Performance:
+            Typical encryption adds <5ms overhead for data <1KB.
+            Larger datasets may see proportional increases.
+        """
+        ...
+
+    def decrypt_cache_data(self, encrypted_data: bytes) -> Dict[str, Any]:
+        """
+        Decrypt cache data if encryption is enabled.
+        
+        This method decrypts the encrypted bytes using Fernet and deserializes
+        the result from JSON. It includes fallback handling for data that was
+        stored without encryption.
+        
+        Args:
+            encrypted_data: Encrypted data bytes from cache
+        
+        Returns:
+            Decrypted data as dictionary
+        
+        Raises:
+            ConfigurationError: If decryption fails or data cannot be deserialized
+        
+        Examples:
+            # Decrypt user data
+            user_data = encryption.decrypt_cache_data(encrypted_bytes)
+            print(user_data["name"])  # "Alice"
+        
+            # Decrypt AI response
+            ai_response = encryption.decrypt_cache_data(encrypted_ai_data)
+            print(ai_response["response"])
+        
+        Performance:
+            Typical decryption adds <3ms overhead for data <1KB.
+            Performance is generally better than encryption.
+        """
+        ...
+
+    @property
+    def is_enabled(self) -> bool:
+        """
+        Check if encryption is enabled.
+        
+        Returns:
+            True if encryption is active, False otherwise
+        
+        Examples:
+            if encryption.is_enabled:
+                print("Cache data is encrypted")
+            else:
+                print("WARNING: Cache data is not encrypted!")
+        """
+        ...
+
+    def get_performance_stats(self) -> Dict[str, Any]:
+        """
+        Get encryption performance statistics.
+        
+        Returns detailed performance metrics for monitoring and optimization.
+        
+        Returns:
+            Dictionary containing performance metrics
+        
+        Examples:
+            stats = encryption.get_performance_stats()
+            print(f"Average encryption time: {stats['avg_encryption_time']:.3f}ms")
+            print(f"Total operations: {stats['total_operations']}")
+        """
+        ...
+
+    def reset_performance_stats(self) -> None:
+        """
+        Reset performance statistics.
+        
+        Useful for testing or monitoring specific time periods.
+        
+        Examples:
+            encryption.reset_performance_stats()
+            # ... perform operations ...
+            stats = encryption.get_performance_stats()
+        """
+        ...
+
+    @classmethod
+    def create_with_generated_key(cls, **kwargs) -> 'EncryptedCacheLayer':
+        """
+        Create EncryptedCacheLayer with a generated encryption key.
+        
+        This is a convenience method for development and testing. For production,
+        use proper key management with environment variables or key management services.
+        
+        Args:
+            **kwargs: Additional arguments passed to __init__
+        
+        Returns:
+            EncryptedCacheLayer instance with generated key
+        
+        Examples:
+            # For development/testing only
+            encryption = EncryptedCacheLayer.create_with_generated_key()
+        
+            # With additional options
+            encryption = EncryptedCacheLayer.create_with_generated_key(
+                performance_monitoring=False
+            )
+        
+        Warning:
+            This method generates a new key each time. Data encrypted with
+            one key cannot be decrypted with another key.
+        """
+        ...
+
+
+def create_encryption_layer_from_env() -> EncryptedCacheLayer:
+    """
+    Create EncryptedCacheLayer from environment variables.
+    
+    This function creates an encryption layer using the REDIS_ENCRYPTION_KEY
+    environment variable. If the key is not available, it logs an error and
+    creates a layer without encryption (not recommended for production).
+    
+    Returns:
+        EncryptedCacheLayer instance
+    
+    Environment Variables:
+        REDIS_ENCRYPTION_KEY: Base64-encoded Fernet encryption key
+    
+    Examples:
+        # Create from environment
+        encryption = create_encryption_layer_from_env()
+    
+        # Use in cache implementation
+        class MyCache:
+            def __init__(self):
+                self.encryption = create_encryption_layer_from_env()
+    
+    Note:
+        This function follows the security-first approach and will create
+        an encryption layer even without a key, but with appropriate warnings.
+    """
+    ...
diff --git a/backend/contracts/infrastructure/cache/factory.pyi b/backend/contracts/infrastructure/cache/factory.pyi
index 505cee6..f7723c8 100644
--- a/backend/contracts/infrastructure/cache/factory.pyi
+++ b/backend/contracts/infrastructure/cache/factory.pyi
@@ -51,18 +51,7 @@ config = {
 cache = await factory.create_cache_from_config(config)
 ```
 
-## Factory vs Direct Instantiation Patterns
-
-### When to Use Factory Methods (Recommended)
-
-**✅ Use factory methods for:**
-
-1. **Application Initialization** - When setting up cache at application startup
-2. **Environment-Specific Configurations** - Different settings per deployment environment
-3. **Cross-Team Development** - Provides consistent, validated configurations
-4. **Preset-Based Configuration** - Leverage optimized defaults for specific use cases
-5. **Production Deployments** - Built-in error handling and graceful fallback
-6. **Security-Conscious Applications** - Integrated security configuration validation
+## Factory Benefits
 
 **Factory Method Benefits:**
 - **Optimized Defaults**: Environment-specific presets (web, AI, testing)
@@ -74,10 +63,10 @@ cache = await factory.create_cache_from_config(config)
 
 **Example - Application Startup:**
 ```python
-# ✅ Recommended: Factory method with environment-specific defaults
+# Recommended: Factory method with environment-specific defaults
 async def setup_cache_layer():
     factory = CacheFactory()
-    
+
     # Production web application
     cache = await factory.for_web_app(
         redis_url=settings.redis_url,
@@ -85,289 +74,12 @@ async def setup_cache_layer():
         security_config=security_config,
         fail_on_connection_error=True
     )
-    
-    return cache
-```
-
-### When to Use Direct Instantiation
-
-**⚠️ Use direct instantiation for:**
 
-1. **Fine-Grained Control** - When you need specific parameter combinations
-2. **Custom Cache Implementations** - Building specialized cache behaviors
-3. **Testing Scenarios** - When you need exact control over cache behavior
-4. **Library/Framework Development** - When building reusable cache components
-5. **Performance-Critical Paths** - When you need to bypass factory validation overhead
-6. **Legacy Code Migration** - Maintaining compatibility during migration
-
-**Direct Instantiation Considerations:**
-- **Manual Parameter Management**: You handle all parameter validation
-- **No Automatic Fallback**: Must implement your own error handling
-- **Security Configuration**: Must manually configure security settings
-- **Connection Management**: Must handle Redis connection testing yourself
-
-**Example - Custom Cache Implementation:**
-```python
-# ⚠️ Direct instantiation: When you need fine-grained control
-async def setup_custom_cache():
-    # Custom AIResponseCache with specific parameters
-    cache = AIResponseCache(
-        redis_url="redis://custom-host:6380/5",
-        default_ttl=7200,
-        text_hash_threshold=2000,
-        compression_threshold=500,
-        compression_level=9,
-        l1_cache_size=300,
-        enable_l1_cache=True,
-        operation_ttls={"custom_op": 1800},
-        performance_monitor=custom_monitor,
-        security_config=custom_security,
-        fail_on_connection_error=True
-    )
-    
-    # Manual connection handling (no automatic fallback)
-    connected = await cache.connect()
-    if not connected:
-        raise InfrastructureError("Cache connection required for this service")
-    
     return cache
 ```
 
-### Migration Path: Direct → Factory
-
-**Migrating from direct instantiation to factory methods:**
-
-```python
-# Before: Direct instantiation
-cache = AIResponseCache(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    text_hash_threshold=1000,
-    memory_cache_size=150,  # deprecated parameter
-    compression_threshold=1000
-)
-
-# After: Factory method with equivalent configuration
-factory = CacheFactory()
-cache = await factory.for_ai_app(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    text_hash_threshold=1000,
-    l1_cache_size=150,  # modern parameter
-    compression_threshold=1000
-)
-```
-
-### Best Practices Summary
-
-**🎯 Recommended Approach:**
-1. **Start with factory methods** for 90% of use cases
-2. **Use environment-specific factory methods** (`for_web_app`, `for_ai_app`, `for_testing`)
-3. **Leverage configuration-based creation** for dynamic environments
-4. **Reserve direct instantiation** for specialized requirements
-5. **Always handle connection failures** regardless of approach
-6. **Use security configuration** in production environments
-
-**🔒 Security Considerations:**
-- Factory methods include built-in security configuration validation
-- Direct instantiation requires manual security setup
-- Always use `security_config` parameter in production
-- Consider `fail_on_connection_error=True` for production deployments
-
 This design provides optimized defaults for different application types.
 
-## Legacy Parameter Migration Guide
-
-### Overview
-
-This guide helps migrate from legacy parameter names to modern equivalents while maintaining backward compatibility. All legacy parameters are still supported but will generate deprecation warnings.
-
-### Parameter Migration Table
-
-| Legacy Parameter | Modern Parameter | Notes |
-|-----------------|------------------|--------|
-| `memory_cache_size` | `l1_cache_size` | Controls L1 memory cache size |
-| `redis_cache_size` | Not applicable | No longer needed, Redis handles capacity |
-| `cache_size` | `l1_cache_size` | Generic cache size parameter |
-
-### Migration Examples
-
-#### AIResponseCache Parameter Migration
-
-**Before (Legacy):**
-```python
-# ⚠️ Uses deprecated parameters
-cache = AIResponseCache(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    memory_cache_size=200,  # DEPRECATED
-    compression_threshold=1000
-)
-```
-
-**After (Modern):**
-```python
-# ✅ Uses current parameters
-cache = AIResponseCache(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    l1_cache_size=200,  # Modern parameter
-    compression_threshold=1000
-)
-
-# Or use factory method (recommended)
-factory = CacheFactory()
-cache = await factory.for_ai_app(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    l1_cache_size=200,
-    compression_threshold=1000
-)
-```
-
-#### Mixed Legacy/Modern Parameters
-
-**Current (Backward Compatible):**
-```python
-# ✅ Mixing legacy and modern parameters works
-cache = AIResponseCache(
-    redis_url="redis://localhost:6379",
-    default_ttl=3600,
-    memory_cache_size=200,  # Legacy (generates warning)
-    l1_cache_size=300,      # Modern (takes precedence)
-    compression_threshold=1000
-)
-# Result: l1_cache_size=300 (modern parameter wins)
-```
-
-### Migration Strategy
-
-#### Phase 1: Identify Legacy Usage
-
-**Search for legacy parameters in your codebase:**
-```bash
-# Find legacy parameter usage
-grep -r "memory_cache_size" --include="*.py" .
-grep -r "redis_cache_size" --include="*.py" .
-grep -r "cache_size.*=" --include="*.py" .
-```
-
-#### Phase 2: Replace with Modern Equivalents
-
-**Parameter Replacement Rules:**
-1. **Replace `memory_cache_size`** → **`l1_cache_size`**
-2. **Remove `redis_cache_size`** (no longer needed)
-3. **Update constructor calls** to use modern parameters
-4. **Consider factory methods** for new code
-
-#### Phase 3: Validate Migration
-
-**Test backward compatibility:**
-
-.. code-block:: python
-
-    import warnings
-    import pytest
-
-    def test_legacy_parameter_compatibility():
-        '''Test that legacy parameters still work with warnings.'''
-        with warnings.catch_warnings(record=True) as w:
-            warnings.simplefilter("always")
-            
-            # Legacy parameter should work
-            cache = AIResponseCache(
-                redis_url="redis://localhost:6379",
-                memory_cache_size=100  # Legacy parameter
-            )
-            
-            # Should generate deprecation warning
-            assert len(w) == 1
-            assert "deprecated" in str(w[0].message).lower()
-            
-            # Should still function correctly
-            assert cache.l1_cache_size == 100
-
-### Deprecation Timeline
-
-| Version | Status | Action Required |
-|---------|--------|-----------------|
-| Current | **Backward Compatible** | Legacy parameters work with warnings |
-| Next Minor | **Deprecation Warnings** | Update code to use modern parameters |
-| Next Major | **Legacy Removal** | Legacy parameters will be removed |
-
-### Automated Migration Script
-
-**Use this script to automatically migrate parameters:**
-
-.. code-block:: python
-
-    #!/usr/bin/env python3
-    '''
-    Automatic migration script for cache parameter updates.
-    Run from project root: python scripts/migrate_cache_parameters.py
-    '''
-
-    import re
-    import os
-    from pathlib import Path
-
-    def migrate_file(file_path: Path):
-        '''Migrate cache parameters in a single file.'''
-        content = file_path.read_text()
-        original_content = content
-        
-        # Replace memory_cache_size with l1_cache_size
-        content = re.sub(
-            r'\bmemory_cache_size\s*=',
-            'l1_cache_size=',
-            content
-        )
-        
-        # Remove redis_cache_size parameters
-        content = re.sub(
-            r',?\s*redis_cache_size\s*=[^,\n)]+',
-            '',
-            content
-        )
-        
-        if content != original_content:
-            file_path.write_text(content)
-            print(f"Updated: {file_path}")
-            return True
-        return False
-
-    def main():
-        '''Migrate all Python files in the project.'''
-        python_files = list(Path('.').rglob('*.py'))
-        updated_count = 0
-        
-        for file_path in python_files:
-            if 'venv' in str(file_path) or '__pycache__' in str(file_path):
-                continue
-                
-            if migrate_file(file_path):
-                updated_count += 1
-        
-        print(f"Migration complete. Updated {updated_count} files.")
-
-    if __name__ == "__main__":
-        main()
-
-### Best Practices for New Code
-
-**✅ Recommended Patterns:**
-1. **Use factory methods** instead of direct instantiation
-2. **Use modern parameter names** (`l1_cache_size` not `memory_cache_size`)
-3. **Include security configuration** for production deployments
-4. **Handle connection failures** with appropriate error handling
-5. **Use preset-based configuration** where possible
-
-**❌ Avoid These Patterns:**
-1. **Don't use legacy parameters** in new code
-2. **Don't ignore deprecation warnings** in development
-3. **Don't mix legacy and modern parameters** unnecessarily
-4. **Don't skip error handling** for connection failures
-
 Performance Considerations:
     - Factory methods execute in <10ms for typical configurations
     - Redis connection validation adds 5-50ms depending on network latency
diff --git a/backend/contracts/infrastructure/cache/manager.pyi b/backend/contracts/infrastructure/cache/manager.pyi
new file mode 100644
index 0000000..92efd87
--- /dev/null
+++ b/backend/contracts/infrastructure/cache/manager.pyi
@@ -0,0 +1,255 @@
+"""
+Intelligent cache management with secure Redis → Memory fallback.
+
+This module provides the CacheManager class that implements intelligent cache
+management with automatic fallback from secure Redis to memory cache when Redis
+is unavailable. It follows the security-first architecture where all cache
+operations are secure by default.
+
+## Key Features
+
+- **Secure Redis First**: Always attempts secure Redis connection first
+- **Graceful Fallback**: Automatic fallback to memory cache when Redis unavailable
+- **Transparent API**: Unified interface regardless of backend cache type
+- **Performance Monitoring**: Comprehensive metrics for both cache backends
+- **Cache Type Indicator**: Monitoring endpoints report active cache type
+- **Security Built-in**: All Redis operations use mandatory security features
+
+## Usage
+
+```python
+from app.infrastructure.cache.manager import CacheManager
+
+# Create manager with automatic backend selection
+manager = CacheManager()
+
+# Use unified cache interface
+await manager.set("key", {"data": "value"}, ttl=3600)
+value = await manager.get("key")
+await manager.delete("key")
+
+# Check active cache backend
+cache_type = manager.cache_type  # "redis_secure" or "memory"
+```
+"""
+
+import logging
+from typing import Any, Optional, Dict
+from app.core.exceptions import ConfigurationError
+
+
+class CacheManager:
+    """
+    Intelligent cache management with secure Redis → Memory fallback.
+    
+    This class provides a unified interface for cache operations that automatically
+    selects the best available cache backend. It always attempts to use secure
+    Redis first, with graceful fallback to memory cache when Redis is unavailable.
+    
+    The security-first architecture ensures that all Redis connections are secure
+    and all cached data is encrypted at rest.
+    """
+
+    def __init__(self, redis_url: Optional[str] = None):
+        """
+        Initialize cache manager with automatic backend selection.
+        
+        Args:
+            redis_url: Optional Redis connection URL. If None, uses environment defaults.
+        
+        Examples:
+            # Automatic configuration
+            manager = CacheManager()
+        
+            # Custom Redis URL
+            manager = CacheManager("rediss://production:6380")
+        
+        Note:
+            The manager attempts secure Redis connection first, then falls back
+            to memory cache if Redis is unavailable. This ensures the application
+            continues functioning even without Redis.
+        """
+        ...
+
+    async def get(self, key: str) -> Any:
+        """
+        Get value from cache with transparent backend selection.
+        
+        Args:
+            key: Cache key to retrieve
+        
+        Returns:
+            Cached value or None if not found
+        
+        Examples:
+            # Get user data
+            user_data = await manager.get("user:123")
+        
+            # Check if key exists
+            if await manager.get("session:abc") is not None:
+                print("Session is active")
+        
+        Note:
+            Security is handled transparently - Redis data is automatically
+            decrypted, memory cache data is returned as-is.
+        """
+        ...
+
+    async def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:
+        """
+        Set value in cache with transparent backend selection.
+        
+        Args:
+            key: Cache key to store
+            value: Value to cache (must be serializable)
+            ttl: Time-to-live in seconds (optional)
+        
+        Examples:
+            # Store user data
+            await manager.set("user:123", {"name": "Alice", "role": "admin"}, ttl=3600)
+        
+            # Store AI response
+            await manager.set("ai:query:hash", {
+                "response": "Generated content",
+                "model": "gpt-4",
+                "timestamp": time.time()
+            })
+        
+        Note:
+            Security is handled transparently - Redis data is automatically
+            encrypted before storage, memory cache stores data as-is.
+        """
+        ...
+
+    async def delete(self, key: str) -> bool:
+        """
+        Delete key from cache with transparent backend selection.
+        
+        Args:
+            key: Cache key to delete
+        
+        Returns:
+            True if key was deleted, False if not found
+        
+        Examples:
+            # Delete user session
+            deleted = await manager.delete("session:abc")
+        
+            # Clear AI cache
+            await manager.delete("ai:query:hash")
+        """
+        ...
+
+    async def clear(self) -> None:
+        """
+        Clear all cached data.
+        
+        Examples:
+            # Clear all cache data
+            await manager.clear()
+        
+        Warning:
+            This operation clears ALL cached data. Use with caution.
+        """
+        ...
+
+    async def exists(self, key: str) -> bool:
+        """
+        Check if key exists in cache.
+        
+        Args:
+            key: Cache key to check
+        
+        Returns:
+            True if key exists, False otherwise
+        
+        Examples:
+            # Check if user session exists
+            if await manager.exists("session:abc"):
+                print("User is logged in")
+        """
+        ...
+
+    async def connect(self) -> bool:
+        """
+        Establish connection to cache backend.
+        
+        Returns:
+            True if connection successful, False otherwise
+        
+        Examples:
+            # Ensure cache connection
+            if await manager.connect():
+                print("Cache is ready")
+            else:
+                print("Cache connection failed")
+        
+        Note:
+            For Redis cache, this establishes secure TLS connection.
+            For memory cache, this always returns True.
+        """
+        ...
+
+    async def disconnect(self) -> None:
+        """
+        Disconnect from cache backend.
+        
+        Examples:
+            # Cleanup on application shutdown
+            await manager.disconnect()
+        
+        Note:
+            Properly closes Redis connections to prevent resource leaks.
+            Memory cache cleanup is automatic.
+        """
+        ...
+
+    def get_cache_info(self) -> Dict[str, Any]:
+        """
+        Get information about active cache backend.
+        
+        Returns:
+            Dictionary with cache backend information
+        
+        Examples:
+            # Get cache status
+            info = manager.get_cache_info()
+            print(f"Using {info['cache_type']} cache")
+            print(f"Security enabled: {info['security_enabled']}")
+        
+        Returns:
+            Dictionary containing:
+            - cache_type: "redis_secure" or "memory"
+            - security_enabled: Boolean indicating encryption status
+            - backend_info: Backend-specific information
+        """
+        ...
+
+    async def health_check(self) -> Dict[str, Any]:
+        """
+        Perform health check on cache backend.
+        
+        Returns:
+            Dictionary with health check results
+        
+        Examples:
+            # Check cache health
+            health = await manager.health_check()
+            if health["healthy"]:
+                print("Cache is operational")
+            else:
+                print(f"Cache issues: {health['errors']}")
+        """
+        ...
+
+    def __str__(self) -> str:
+        """
+        String representation of cache manager.
+        """
+        ...
+
+    def __repr__(self) -> str:
+        """
+        Detailed string representation of cache manager.
+        """
+        ...
diff --git a/backend/contracts/infrastructure/cache/memory.pyi b/backend/contracts/infrastructure/cache/memory.pyi
index 7ca3fc9..f31fb4a 100644
--- a/backend/contracts/infrastructure/cache/memory.pyi
+++ b/backend/contracts/infrastructure/cache/memory.pyi
@@ -71,6 +71,60 @@ active_keys = [k for k in keys_to_check if await cache.exists(k)]
 cache.clear()  # Clear all entries
 ```
 
+## Usage Patterns
+
+### Factory Method (Recommended for Most Cases)
+
+**Use factory methods for consistent configuration and testing patterns:**
+
+```python
+from app.infrastructure.cache import CacheFactory
+
+factory = CacheFactory()
+
+# Testing scenarios - recommended approach
+test_cache = await factory.for_testing(
+    use_memory_cache=True,
+    default_ttl=60,  # 1 minute for tests
+    l1_cache_size=50
+)
+
+# Development environments
+dev_cache = await factory.for_web_app(
+    redis_url="redis://localhost:6379",
+    fail_on_connection_error=False  # Falls back to InMemoryCache
+)
+```
+
+**Factory Method Benefits:**
+- Consistent test environment configurations
+- Automatic fallback behavior when Redis unavailable
+- Environment-optimized defaults
+- Built-in parameter validation
+
+### Direct Instantiation (Appropriate for)
+
+**Use direct instantiation for simple applications and exact control scenarios:**
+
+```python
+# Testing scenarios requiring exact control
+cache = InMemoryCache(default_ttl=1800, max_size=500)
+
+# Simple applications with basic cache needs
+cache = InMemoryCache(default_ttl=7200, max_size=2000)
+
+# Fallback cache implementations
+cache = InMemoryCache(default_ttl=3600, max_size=1000)
+```
+
+**Use direct instantiation when:**
+- Building simple applications with basic caching needs
+- Testing scenarios requiring exact behavioral control
+- Implementing fallback cache strategies
+- Developing specialized cache components
+
+**📖 For comprehensive factory usage patterns and configuration examples, see [Cache Usage Guide](../../../docs/guides/infrastructure/cache/usage-guide.md).**
+
 Performance Characteristics:
 ----------------------------
 - **Get Operations**: O(1) average case, O(n) worst case during cleanup
diff --git a/backend/contracts/infrastructure/cache/parameter_mapping.pyi b/backend/contracts/infrastructure/cache/parameter_mapping.pyi
index 2b536d0..00af37d 100644
--- a/backend/contracts/infrastructure/cache/parameter_mapping.pyi
+++ b/backend/contracts/infrastructure/cache/parameter_mapping.pyi
@@ -41,9 +41,8 @@ ai_params = {
         ...     print("All parameters are compatible")
 
 Architecture Context:
-    This module is part of Phase 2 of the cache refactoring project, enabling
-    the inheritance structure where AIResponseCache extends GenericRedisCache
-    while maintaining backward compatibility and clear parameter separation.
+    This module provides parameter mapping for cache inheritance, separating
+    AI-specific parameters from generic Redis parameters for clean inheritance.
 
 Dependencies:
     - dataclasses: For ValidationResult structure
@@ -135,32 +134,17 @@ class ValidationResult:
 
 class CacheParameterMapper:
     """
-    Advanced parameter mapping system enabling clean cache inheritance architecture.
+    Parameter mapping system for cache inheritance architecture.
     
-    Provides comprehensive parameter separation, mapping, and validation to support
-    AIResponseCache inheritance from GenericRedisCache. Handles parameter classification,
-    name mapping, value transformation, and compatibility validation with detailed reporting.
+    Provides parameter separation, mapping, and validation to support
+    AIResponseCache inheritance from GenericRedisCache. Separates AI-specific
+    parameters from generic Redis parameters and validates configuration.
     
-    Attributes:
-        ai_specific_params: Set[str] parameters unique to AI cache functionality
-        generic_params: Set[str] parameters inherited by generic Redis cache
-        parameter_mappings: Dict[str, str] AI-to-generic parameter name mappings
-        value_transformers: Dict[str, Callable] parameter value transformation functions
-    
-    Public Methods:
-        map_ai_to_generic_params(): Separate and map AI parameters to generic equivalents
-        validate_parameter_compatibility(): Comprehensive parameter validation with reporting
-        get_parameter_classification(): Classify parameters by type and inheritance level
-        transform_parameter_values(): Apply value transformations for compatibility
-    
-    State Management:
-        - Immutable parameter classification for consistent behavior
-        - Thread-safe mapping operations for concurrent cache initialization
-        - Comprehensive validation with detailed error context
-        - Extensible architecture for custom parameter types
+    Key Methods:
+        map_ai_to_generic_params(): Separate AI parameters from generic parameters
+        validate_parameter_compatibility(): Validate parameter configuration
     
     Usage:
-        # Basic parameter mapping for inheritance
         mapper = CacheParameterMapper()
         ai_params = {
             'redis_url': 'redis://localhost:6379',
@@ -168,37 +152,8 @@ class CacheParameterMapper:
             'memory_cache_size': 100
         }
     
-        generic_params, ai_only = mapper.map_ai_to_generic_params(ai_params)
-    
-        # Validation with detailed reporting
-        result = mapper.validate_parameter_compatibility(ai_params)
-        if not result.is_valid:
-            for error in result.errors:
-                logger.error(f"Parameter error: {error}")
-    
-        # Production usage with error handling
-        try:
-            generic_config = mapper.create_generic_config(ai_params)
-            ai_config = mapper.extract_ai_specific_config(ai_params)
-            cache = AIResponseCache(generic_config, ai_config)
-        except ValueError as e:
-            logger.error(f"Parameter mapping failed: {e}")
-    
-    Parameter Categories:
-        1. Generic Redis Parameters: Shared between all Redis cache implementations
-        2. AI-Specific Parameters: Unique to AI response caching functionality
-        3. Mapped Parameters: AI parameters that map to generic equivalents
-        4. Conflicting Parameters: Parameters that have different meanings in each context
-    
-    Example:
-        >>> mapper = CacheParameterMapper()
-        >>> ai_params = {
-        ...     'redis_url': 'redis://localhost:6379',
-        ...     'memory_cache_size': 100,
-        ...     'text_hash_threshold': 1000
-        ... }
-        >>> generic_params, ai_specific = mapper.map_ai_to_generic_params(ai_params)
-        >>> validation = mapper.validate_parameter_compatibility(ai_params)
+        generic_params, ai_specific = mapper.map_ai_to_generic_params(ai_params)
+        validation = mapper.validate_parameter_compatibility(ai_params)
     """
 
     def __init__(self):
diff --git a/backend/contracts/infrastructure/cache/redis_ai.pyi b/backend/contracts/infrastructure/cache/redis_ai.pyi
index 2ce1c3c..4df8e4c 100644
--- a/backend/contracts/infrastructure/cache/redis_ai.pyi
+++ b/backend/contracts/infrastructure/cache/redis_ai.pyi
@@ -26,11 +26,71 @@ Inheritance pattern:
 - **AIResponseCache**: Adds AI-specific optimizations and intelligent features
 - **CacheParameterMapper**: Handles parameter validation and mapping
 
-## Usage
+## Usage Patterns
+
+### Factory Method (Recommended)
+
+**Most AI applications should use factory methods for AI-optimized defaults and intelligent features:**
 
 ```python
-cache = AIResponseCache(redis_url="redis://localhost:6379")
-await cache.connect()
+from app.infrastructure.cache import CacheFactory
+
+factory = CacheFactory()
+
+# AI applications - recommended approach
+cache = await factory.for_ai_app(
+    redis_url="redis://localhost:6379",
+    default_ttl=3600,  # 1 hour
+    operation_ttls={
+        "summarize": 7200,  # 2 hours - summaries are stable
+        "sentiment": 86400,  # 24 hours - sentiment rarely changes
+        "translate": 14400   # 4 hours - translations moderately stable
+    }
+)
+
+# Production AI cache with security
+from app.infrastructure.security import SecurityConfig
+security_config = SecurityConfig(
+    redis_auth="ai-cache-password",
+    use_tls=True,
+    verify_certificates=True
+)
+cache = await factory.for_ai_app(
+    redis_url="rediss://ai-production:6380",
+    security_config=security_config,
+    text_hash_threshold=1000,
+    fail_on_connection_error=True
+)
+```
+
+**Factory Method Benefits for AI Applications:**
+- AI-optimized defaults with operation-specific TTLs
+- Intelligent text hashing for large inputs
+- Enhanced compression for AI response storage
+- AI-specific monitoring and performance analytics
+- Automatic fallback with graceful degradation
+
+### Direct Instantiation (Advanced AI Use Cases)
+
+**Use direct instantiation for specialized AI cache configurations:**
+
+```python
+cache = AIResponseCache(
+    redis_url="redis://localhost:6379",
+    default_ttl=3600,
+    text_hash_threshold=500,
+    operation_ttls={
+        "custom_ai_operation": 1800,
+        "specialized_nlp": 7200
+    },
+    compression_threshold=1000,
+    compression_level=8
+)
+
+# Manual connection handling required
+connected = await cache.connect()
+if not connected:
+    raise InfrastructureError("AI cache connection required")
 
 # AI response caching with intelligent key generation
 key = cache.build_key(
@@ -39,9 +99,15 @@ key = cache.build_key(
     options={"max_length": 100}
 )
 await cache.set(key, {"summary": "Brief summary"}, ttl=3600)
-        ...         'large': 30000
-        ...     }
-        ... )
+```
+
+**Use direct instantiation when:**
+- Building custom AI cache implementations with specialized features
+- Requiring exact AI parameter combinations not supported by factory methods
+- Developing AI-specific frameworks or libraries
+- Migrating legacy AI systems with custom configurations
+
+**📖 For comprehensive factory usage patterns and AI-specific configuration examples, see [Cache Usage Guide](../../../docs/guides/infrastructure/cache/usage-guide.md).**
 
 Dependencies:
     Required:
@@ -81,81 +147,105 @@ from app.infrastructure.cache.monitoring import CachePerformanceMonitor
 
 class AIResponseCache(GenericRedisCache):
     """
-    AI Response Cache with enhanced inheritance architecture.
+    AI Response Cache with automatic security inheritance.
+    
+    This secure-first implementation inherits all security features from GenericRedisCache
+    automatically, including TLS encryption, authentication, and data encryption at rest.
+    All AI-specific functionality is preserved while security is handled transparently.
+    
+    ## Key Features
+    
+    - **Automatic Security**: Inherits TLS, authentication, and encryption from GenericRedisCache
+    - **AI-Optimized**: Specialized features for AI response caching and text processing
+    - **Clean Architecture**: Simplified parameters focus on AI-specific functionality
+    - **Backward Compatible**: Existing API preserved with deprecation warnings for security params
+    - **Enhanced Performance**: AI-specific callbacks and metrics collection
+    
+    ## Automatic Security Features
     
-    This refactored implementation properly inherits from GenericRedisCache while
-    maintaining all AI-specific functionality. It uses CacheParameterMapper for
-    clean parameter separation and provides comprehensive AI metrics collection.
+    All security features are enabled automatically without configuration:
+    - **TLS Encryption**: Secure Redis connections with certificate validation
+    - **Authentication**: Automatic Redis authentication and ACL support
+    - **Data Encryption**: Transparent Fernet encryption for all cached data
+    - **Security Validation**: Automatic environment-aware security enforcement
     
-    ### Key Improvements
-    - Clean inheritance from GenericRedisCache for core functionality
-    - Proper parameter mapping using CacheParameterMapper
-    - AI-specific callbacks integrated with generic cache events
-    - Maintains backward compatibility with existing API
-    - Enhanced error handling with custom exceptions
+    ## AI-Specific Parameters
     
-    ### Parameters
-    All original AIResponseCache parameters are supported with automatic mapping:
-    - `redis_url` (str): Redis connection URL
+    Focus on AI functionality - security is handled automatically:
+    - `redis_url` (str): Redis connection URL (security applied automatically)
     - `default_ttl` (int): Default time-to-live for cache entries
     - `text_hash_threshold` (int): Character threshold for text hashing
     - `hash_algorithm`: Hash algorithm for large texts
     - `compression_threshold` (int): Size threshold for compression
     - `compression_level` (int): Compression level (1-9)
     - `text_size_tiers` (Dict[str, int]): Text categorization thresholds
-    - `memory_cache_size` (int): Maximum L1 cache entries (mapped to l1_cache_size)
-    - `performance_monitor` (CachePerformanceMonitor): Performance monitoring instance
+    - `l1_cache_size` (int): Maximum L1 cache entries
+    - `operation_ttls` (Dict[str, int]): TTL values per AI operation type
     
-    ### Returns
-    A fully functional AIResponseCache instance with enhanced architecture.
+    ## Returns
+    
+    A fully functional AIResponseCache instance with automatic security inheritance.
+    
+    ## Examples
     
-    ### Examples
     ```python
-    # Basic usage (backward compatible)
+    # Simple usage - security is automatic
     cache = AIResponseCache(redis_url="redis://localhost:6379")
     await cache.connect()
     
-    # Advanced configuration
+    # AI-optimized configuration
     cache = AIResponseCache(
         redis_url="redis://production:6379",
         text_hash_threshold=1000,
-        memory_cache_size=200,
-        text_size_tiers={'small': 500, 'medium': 5000, 'large': 50000}
+        l1_cache_size=200,
+        operation_ttls={
+            "summarize": 7200,  # 2 hours
+            "sentiment": 86400, # 24 hours
+        }
     )
+    
+    # All security features are enabled automatically:
+    # - TLS encryption for Redis connections
+    # - Fernet encryption for cached data
+    # - Automatic authentication and validation
     ```
+    
+    Note:
+        Security parameters (security_config, use_tls, etc.) are no longer needed
+        and will generate deprecation warnings. Security is always enabled.
     """
 
-    def __init__(self, redis_url: str = 'redis://redis:6379', default_ttl: int = 3600, text_hash_threshold: int = 1000, hash_algorithm = hashlib.sha256, compression_threshold: int = 1000, compression_level: int = 6, text_size_tiers: Optional[Dict[str, int]] = None, memory_cache_size: Optional[int] = None, l1_cache_size: int = 100, enable_l1_cache: bool = True, performance_monitor: Optional[CachePerformanceMonitor] = None, operation_ttls: Optional[Dict[str, int]] = None, security_config: Optional['SecurityConfig'] = None, fail_on_connection_error: bool = False, **kwargs):
+    def __init__(self, redis_url: str = 'redis://redis:6379', default_ttl: int = 3600, text_hash_threshold: int = 1000, hash_algorithm = hashlib.sha256, compression_threshold: int = 1000, compression_level: int = 6, text_size_tiers: Optional[Dict[str, int]] = None, memory_cache_size: Optional[int] = None, l1_cache_size: int = 100, enable_l1_cache: bool = True, performance_monitor: Optional[CachePerformanceMonitor] = None, operation_ttls: Optional[Dict[str, int]] = None, **kwargs):
         """
-        Initialize AIResponseCache with parameter mapping and inheritance.
+        Initialize AIResponseCache with automatic security inheritance.
         
-        This constructor uses CacheParameterMapper to separate AI-specific parameters
-        from generic Redis parameters, then properly initializes the parent class
-        and sets up AI-specific features.
+        This simplified constructor focuses on AI-specific parameters while
+        automatically inheriting security features from the secure GenericRedisCache.
+        All security features (TLS, authentication, encryption) are enabled automatically.
         
         Args:
-            redis_url: Redis connection URL
+            redis_url: Redis connection URL (security features applied automatically)
             default_ttl: Default time-to-live for cache entries in seconds
             text_hash_threshold: Character count threshold for text hashing
             hash_algorithm: Hash algorithm to use for large texts
             compression_threshold: Size threshold in bytes for compressing cache data
             compression_level: Compression level (1-9, where 9 is highest compression)
             text_size_tiers: Text size tiers for caching strategy optimization
-            memory_cache_size: DEPRECATED. Use l1_cache_size instead. Maximum number of items 
+            memory_cache_size: DEPRECATED. Use l1_cache_size instead. Maximum number of items
                               in the in-memory cache. If provided, overrides l1_cache_size for backward compatibility.
             l1_cache_size: Maximum number of items in the L1 in-memory cache (modern parameter)
             enable_l1_cache: Enable/disable L1 in-memory cache for performance optimization
             performance_monitor: Optional performance monitor for tracking cache metrics
             operation_ttls: TTL values per AI operation type
-            security_config: Optional security configuration for secure Redis connections,
-                           including authentication, TLS encryption, and security validation
-            fail_on_connection_error: If True, raise InfrastructureError when Redis unavailable.
-                                     If False (default), gracefully fallback to memory-only mode.
         
         Raises:
             ConfigurationError: If parameter mapping fails or invalid configuration
             ValidationError: If parameter validation fails
-            InfrastructureError: If Redis connection fails and fail_on_connection_error=True
+            InfrastructureError: If Redis connection fails (automatic fallback to memory cache)
+        
+        Note:
+            Security features (TLS, authentication, encryption) are automatically enabled
+            by the parent GenericRedisCache. No explicit security configuration needed.
         """
         ...
 
diff --git a/backend/contracts/infrastructure/cache/redis_generic.pyi b/backend/contracts/infrastructure/cache/redis_generic.pyi
index 1a8b251..eee3122 100644
--- a/backend/contracts/infrastructure/cache/redis_generic.pyi
+++ b/backend/contracts/infrastructure/cache/redis_generic.pyi
@@ -20,17 +20,73 @@ like AIResponseCache.
 - **Flexible Serialization**: Support for JSON and pickle serialization
 - **Connection Management**: Robust Redis connection handling with retry logic
 
-## Usage
+## Usage Patterns
+
+### Factory Method (Recommended)
+
+**Most applications should use factory methods for optimized defaults and built-in validation:**
 
 ```python
-cache = GenericRedisCache(redis_url="redis://localhost:6379")
-await cache.connect()
+from app.infrastructure.cache import CacheFactory
+
+factory = CacheFactory()
+
+# Web applications - recommended approach
+cache = await factory.for_web_app(
+    redis_url="redis://localhost:6379",
+    default_ttl=1800,  # 30 minutes
+    compression_threshold=2000
+)
+
+# Production with security
+from app.infrastructure.security import SecurityConfig
+security_config = SecurityConfig(redis_auth="secure-password", use_tls=True)
+cache = await factory.for_web_app(
+    redis_url="rediss://production:6380",
+    security_config=security_config,
+    fail_on_connection_error=True
+)
+```
+
+**Factory Method Benefits:**
+- Environment-optimized defaults for web applications
+- Comprehensive parameter validation with detailed error messages
+- Automatic fallback to InMemoryCache when Redis unavailable
+- Built-in security configuration and TLS support
+- Structured error handling with proper context
+
+### Direct Instantiation (Advanced Use Cases)
+
+**Use direct instantiation for fine-grained control or custom implementations:**
+
+```python
+cache = GenericRedisCache(
+    redis_url="redis://localhost:6379",
+    default_ttl=3600,
+    enable_l1_cache=True,
+    l1_cache_size=200,
+    compression_threshold=2000,
+    compression_level=6
+)
+
+# Manual connection handling required
+connected = await cache.connect()
+if not connected:
+    raise InfrastructureError("Redis connection required for this service")
 
 # Standard cache operations
 await cache.set("user:123", {"name": "John", "age": 30}, ttl=3600)
 user_data = await cache.get("user:123")
 await cache.delete("user:123")
 ```
+
+**Use direct instantiation when:**
+- Building custom cache implementations with specialized requirements
+- Requiring exact parameter combinations not supported by factory methods
+- Developing reusable cache components or frameworks
+- Migrating legacy code with specific configuration needs
+
+**📖 For comprehensive factory usage patterns and configuration examples, see [Cache Usage Guide](../../../docs/guides/infrastructure/cache/usage-guide.md).**
 """
 
 import json
@@ -87,7 +143,7 @@ class GenericRedisCache(CacheInterface):
     ```
     """
 
-    def __init__(self, redis_url: str = 'redis://redis:6379', default_ttl: int = 3600, enable_l1_cache: bool = True, l1_cache_size: int = 100, compression_threshold: int = 1000, compression_level: int = 6, performance_monitor: Optional[CachePerformanceMonitor] = None, security_config: Optional['SecurityConfig'] = None, fail_on_connection_error: bool = False, **kwargs):
+    def __init__(self, redis_url: str = 'rediss://redis:6380', default_ttl: int = 3600, enable_l1_cache: bool = True, l1_cache_size: int = 100, compression_threshold: int = 1000, compression_level: int = 6, performance_monitor: Optional[CachePerformanceMonitor] = None, **kwargs):
         ...
 
     def register_callback(self, event: str, callback: Callable):
@@ -353,3 +409,40 @@ class GenericRedisCache(CacheInterface):
         ```
         """
         ...
+
+    @classmethod
+    def create_secure(cls, redis_url: Optional[str] = None) -> 'GenericRedisCache':
+        """
+        Factory method for secure cache creation.
+        
+        Creates a GenericRedisCache instance with automatic security configuration
+        based on environment detection. This is the recommended way to create
+        cache instances in the security-first architecture.
+        
+        Args:
+            redis_url: Optional Redis connection URL. If None, uses secure defaults.
+        
+        Returns:
+            GenericRedisCache instance with mandatory security enabled
+        
+        Examples:
+            # Automatic configuration
+            cache = GenericRedisCache.create_secure()
+        
+            # Custom Redis URL
+            cache = GenericRedisCache.create_secure("rediss://production:6380")
+        
+            # Use with context manager
+            cache = GenericRedisCache.create_secure()
+            async with cache:
+                await cache.set("key", "value")
+                value = await cache.get("key")
+        
+        Note:
+            This method automatically configures:
+            - Environment-aware security settings
+            - TLS encryption and authentication
+            - Application-layer data encryption
+            - Fail-fast security validation
+        """
+        ...
diff --git a/backend/contracts/infrastructure/cache/security.pyi b/backend/contracts/infrastructure/cache/security.pyi
index b5ac681..f3ec22a 100644
--- a/backend/contracts/infrastructure/cache/security.pyi
+++ b/backend/contracts/infrastructure/cache/security.pyi
@@ -45,10 +45,13 @@ import ssl
 import asyncio
 import time
 import inspect
+import secrets
+import string
 from dataclasses import dataclass, field
 from typing import Optional, List, Dict, Any
 from pathlib import Path
 from app.core.exceptions import ConfigurationError
+from app.core.environment import get_environment_info, Environment, FeatureContext
 from app.infrastructure.cache.monitoring import CachePerformanceMonitor
 
 
@@ -126,6 +129,58 @@ class SecurityConfig:
         """
         ...
 
+    @classmethod
+    def create_for_environment(cls, encryption_key: Optional[str] = None) -> 'SecurityConfig':
+        """
+        Create security configuration appropriate for detected environment.
+        
+        This method provides environment-aware security configuration that automatically
+        adapts security settings based on the current deployment environment while
+        maintaining security-first principles.
+        
+        Args:
+            encryption_key: Optional encryption key for data encryption.
+                          If not provided, will be generated automatically.
+        
+        Returns:
+            SecurityConfig: Environment-appropriate security configuration
+        
+        Examples:
+            # Automatic environment detection
+            config = SecurityConfig.create_for_environment()
+        
+            # With custom encryption key
+            config = SecurityConfig.create_for_environment("your-encryption-key")
+        
+        Security-First Principles:
+            - All environments require TLS encryption (no plaintext connections)
+            - Unknown environments default to production-level security (fail-secure)
+        
+        Environment-Specific Settings:
+            - Production: Strong passwords, TLS 1.3, certificate validation REQUIRED
+            - Staging: Production-like security with moderate passwords
+            - Development: TLS required, self-signed certificates acceptable
+            - Testing: TLS required, self-signed certificates acceptable, reduced monitoring
+            - Unknown: Production-level security as fail-safe default
+        """
+        ...
+
+    def validate_mandatory_security_requirements(self) -> None:
+        """
+        Validate that mandatory security requirements are met for production.
+        
+        This method enforces security-first requirements and provides fail-fast
+        validation with clear error messages for security violations.
+        
+        Raises:
+            ConfigurationError: If mandatory security requirements are not met
+        
+        Note:
+            This is stricter than the regular _validate_configuration method
+            and enforces production-grade security standards.
+        """
+        ...
+
 
 @dataclass
 class SecurityValidationResult:
@@ -223,6 +278,63 @@ class RedisCacheSecurityManager:
         """
         ...
 
+    def validate_mandatory_security(self, redis_url: str) -> None:
+        """
+        Validate mandatory security requirements with fail-fast behavior.
+        
+        This method enforces security-first principles by validating that all
+        mandatory security requirements are met before attempting connection.
+        It provides immediate failure with clear error messages.
+        
+        Args:
+            redis_url: Redis connection URL to validate
+        
+        Raises:
+            ConfigurationError: If mandatory security requirements are not met
+        
+        Examples:
+            # Validate before connection
+            manager.validate_mandatory_security("rediss://redis:6380")
+        
+            # Will raise ConfigurationError for insecure URLs in production
+            manager.validate_mandatory_security("redis://redis:6379")  # Fails in production
+        
+        Note:
+            This method is called automatically by create_secure_connection()
+            in security-first mode to ensure no insecure connections are created.
+        """
+        ...
+
+    def create_secure_connection_with_validation(self, redis_url: str = 'redis://localhost:6379') -> Any:
+        """
+        Create secure Redis connection with mandatory security validation.
+        
+        This method provides security-first connection creation with fail-fast
+        validation. It ensures all security requirements are met before attempting
+        to create the connection.
+        
+        Args:
+            redis_url: Redis server URL with authentication and TLS
+        
+        Returns:
+            Configured Redis client with security features enabled
+        
+        Raises:
+            ConfigurationError: If security requirements are not met
+        
+        Examples:
+            # Create secure connection (will validate first)
+            redis = await manager.create_secure_connection_with_validation("rediss://redis:6380")
+        
+            # Will fail fast in production without TLS
+            redis = await manager.create_secure_connection_with_validation("redis://redis:6379")
+        
+        Note:
+            This is the recommended method for security-first applications.
+            Use regular create_secure_connection() for backward compatibility.
+        """
+        ...
+
     async def validate_connection_security(self, redis_client: Any) -> SecurityValidationResult:
         """
         Validate Redis connection security status.
@@ -282,26 +394,76 @@ class RedisCacheSecurityManager:
         ...
 
 
-def create_security_config_from_env() -> Optional[SecurityConfig]:
+def generate_secure_password(length: int) -> str:
     """
-    Create SecurityConfig from environment variables.
+    Generate cryptographically secure password.
     
-    This utility function creates a SecurityConfig instance from common
-    environment variables, making it easy to configure Redis security
-    in containerized environments.
+    Creates a random password using secrets module with a character set that
+    includes letters, digits, and safe special characters suitable for Redis
+    authentication.
     
-    Environment Variables:
-        REDIS_AUTH: Redis AUTH password
-        REDIS_USE_TLS: Enable TLS (true/false)
-        REDIS_TLS_CERT_PATH: Path to TLS certificate
-        REDIS_TLS_KEY_PATH: Path to TLS private key
-        REDIS_TLS_CA_PATH: Path to TLS CA certificate
-        REDIS_ACL_USERNAME: ACL username
-        REDIS_ACL_PASSWORD: ACL password
-        REDIS_VERIFY_CERTIFICATES: Verify certificates (true/false)
-        REDIS_CONNECTION_TIMEOUT: Connection timeout in seconds
+    Args:
+        length: Desired password length (minimum 8 characters recommended)
+    
+    Returns:
+        Cryptographically secure password string
+    
+    Examples:
+        # Generate production password
+        prod_password = generate_secure_password(32)
+    
+        # Generate development password
+        dev_password = generate_secure_password(16)
+    
+        # Generate testing password
+        test_password = generate_secure_password(12)
+    
+    Note:
+        Uses secrets.choice() for cryptographic security rather than random.choice().
+        Character set excludes ambiguous characters like 0, O, l, I to avoid confusion.
+    """
+    ...
+
+
+def create_security_config_from_env() -> SecurityConfig:
+    """
+    Create security configuration from environment variables.
+    
+    This function creates a SecurityConfig instance using environment variables
+    with secure defaults. If required values are missing, it generates them
+    automatically to ensure secure operation.
+    
+    Security-First Approach:
+        1. Tries environment-aware configuration first (auto-detects environment)
+        2. Falls back to explicit environment variable configuration
+        3. Auto-generates secure passwords if not provided (fail-secure)
+        4. Defaults to TLS enabled (security-first principle)
     
     Returns:
-        SecurityConfig instance or None if no security settings found
+        SecurityConfig: Configuration with secure defaults
+    
+    Environment Variables:
+        REDIS_AUTH: Redis authentication password (auto-generated if missing)
+        REDIS_ACL_USERNAME: Redis ACL username (optional)
+        REDIS_ACL_PASSWORD: Redis ACL password (optional)
+        REDIS_TLS_ENABLED: Enable TLS (default: true)
+        REDIS_TLS_CERT_PATH: Path to TLS certificate file
+        REDIS_TLS_KEY_PATH: Path to TLS private key file
+        REDIS_TLS_CA_PATH: Path to CA certificate file
+        REDIS_VERIFY_CERTIFICATES: Verify TLS certificates (default: true)
+        REDIS_CONNECTION_TIMEOUT: Connection timeout in seconds (default: 30)
+        REDIS_MAX_RETRIES: Maximum connection retries (default: 3)
+        REDIS_RETRY_DELAY: Delay between retries in seconds (default: 1.0)
+    
+    Examples:
+        # Create config from environment (auto-detects environment)
+        config = create_security_config_from_env()
+    
+        # Use with security manager
+        manager = RedisCacheSecurityManager(config)
+    
+    Note:
+        This function always returns a valid SecurityConfig. It never returns None,
+        ensuring fail-secure behavior even when no environment variables are set.
     """
     ...
diff --git a/backend/contracts/infrastructure/monitoring/health.pyi b/backend/contracts/infrastructure/monitoring/health.pyi
index f291ea3..d60c503 100644
--- a/backend/contracts/infrastructure/monitoring/health.pyi
+++ b/backend/contracts/infrastructure/monitoring/health.pyi
@@ -1,9 +1,165 @@
 """
 Health Check Infrastructure Module
 
-Async-first, standardized health checking for system components with timeouts,
-graceful degradation, and response time measurement. Used by API endpoints and
-external monitoring systems.
+Comprehensive async-first health monitoring infrastructure providing standardized health
+checking capabilities for all system components including AI services, cache infrastructure,
+resilience systems, and databases. Implements configurable timeout policies, retry mechanisms,
+and graceful degradation patterns for reliable operational monitoring.
+
+## Core Components
+
+### Health Status Management
+- **HealthStatus**: Standardized enumeration for HEALTHY, DEGRADED, UNHEALTHY states
+- **ComponentStatus**: Individual component health information with timing and metadata
+- **SystemHealthStatus**: Aggregated system-wide health status across all components
+
+### Health Check Orchestration
+- **HealthChecker**: Centralized health monitoring service with configurable policies
+- **HealthCheckFunc**: Type alias for async health check function signatures
+- **HealthCheckError**: Base exception hierarchy for health check infrastructure failures
+
+### Built-in Health Checks
+- **check_ai_model_health()**: AI service configuration and availability validation
+- **check_cache_health()**: Cache infrastructure connectivity and operational status
+- **check_resilience_health()**: Resilience system circuit breaker states and stability
+- **check_database_health()**: Database connectivity validation (placeholder implementation)
+
+## Architecture Design
+
+### Async-First Performance
+All health check operations use async/await patterns for non-blocking execution with
+concurrent health monitoring across multiple components. Implements asyncio timeout
+protection and parallel execution for optimal performance under load.
+
+### Configurable Policies
+- **Timeout Management**: Per-component timeout configuration with fallback defaults
+- **Retry Mechanisms**: Configurable retry attempts with exponential backoff patterns
+- **Error Classification**: Distinction between timeout errors (DEGRADED) and failures (UNHEALTHY)
+
+### Operational Integration
+- **Response Time Tracking**: Microsecond-precision timing for performance monitoring
+- **Metadata Collection**: Component-specific diagnostic information for troubleshooting
+- **Logging Integration**: Structured logging for health check failures and timeouts
+
+## Usage Patterns
+
+### Basic Health Monitoring
+```python
+from app.infrastructure.monitoring.health import HealthChecker
+
+# Initialize health checker with configuration
+checker = HealthChecker(
+    default_timeout_ms=2000,
+    retry_count=1,
+    backoff_base_seconds=0.1
+)
+
+# Register component health checks
+checker.register_check("database", check_database_health)
+checker.register_check("cache", check_cache_health)
+checker.register_check("ai_model", check_ai_model_health)
+
+# Execute individual component health check
+status = await checker.check_component("database")
+print(f"Database: {status.status.value} - {status.message}")
+
+# Execute comprehensive system health check
+system_health = await checker.check_all_components()
+if system_health.overall_status == HealthStatus.HEALTHY:
+    print("All systems operational")
+```
+
+### Advanced Configuration
+```python
+# Per-component timeout configuration
+checker = HealthChecker(
+    default_timeout_ms=2000,
+    per_component_timeouts_ms={
+        "database": 5000,      # Longer timeout for database operations
+        "cache": 1000,         # Shorter timeout for cache operations
+        "ai_model": 3000       # Medium timeout for AI services
+    },
+    retry_count=2,
+    backoff_base_seconds=0.5
+)
+
+# Performance monitoring integration
+for component in system_health.components:
+    if component.response_time_ms > 1000:
+        logger.warning(f"Slow health check: {component.name} ({component.response_time_ms:.1f}ms)")
+```
+
+### FastAPI Integration
+```python
+from fastapi import Depends
+from app.dependencies import get_health_checker
+
+@app.get("/health")
+async def system_health(checker: HealthChecker = Depends(get_health_checker)):
+    health = await checker.check_all_components()
+    return {
+        "status": health.overall_status.value,
+        "timestamp": health.timestamp,
+        "components": [
+            {
+                "name": c.name,
+                "status": c.status.value,
+                "message": c.message,
+                "response_time_ms": c.response_time_ms
+            }
+            for c in health.components
+        ]
+    }
+```
+
+## Performance Characteristics
+
+### Execution Efficiency
+- **Concurrent Execution**: All component health checks run in parallel via asyncio.gather
+- **Timeout Protection**: Individual health checks cannot block overall system health monitoring
+- **Memory Efficiency**: Minimal memory allocation with reusable health checker instances
+
+### Reliability Features
+- **Graceful Degradation**: Health monitoring continues even if individual components fail
+- **Error Isolation**: Component failures do not impact other component health checks
+- **Retry Logic**: Configurable retry mechanisms with exponential backoff for transient failures
+
+## Error Handling
+
+### Exception Hierarchy
+- **HealthCheckError**: Base class for health check infrastructure failures
+- **HealthCheckTimeoutError**: Specific timeout-related failures for performance issues
+
+### Failure Classification
+- **HEALTHY**: All components operational with normal response times
+- **DEGRADED**: Components operational but with performance issues or reduced functionality
+- **UNHEALTHY**: Components experiencing critical failures or unavailability
+
+## Integration Points
+
+### Dependency Injection
+Health checker integrates with FastAPI dependency injection system through `get_health_checker()`
+dependency provider, enabling consistent health monitoring across all API endpoints.
+
+### Monitoring Systems
+Component status metadata and timing information provide comprehensive data for external
+monitoring systems, alerting infrastructure, and operational dashboards.
+
+### Cache Service Optimization
+Cache health checks support dependency injection for optimal performance, reusing existing
+cache service connections rather than creating new instances for each health check.
+
+## Template Architecture Benefits
+
+This health check infrastructure serves as a production-ready template component that:
+- **Demonstrates Best Practices**: Showcases professional health monitoring patterns
+- **Provides Reusable Patterns**: Ready-to-use health check infrastructure for any service
+- **Enables Operational Excellence**: Comprehensive monitoring foundation for production deployments
+- **Facilitates Testing**: Well-documented behavior contracts enable comprehensive test coverage
+
+Template users can extend this infrastructure by implementing additional component health
+checks following the established patterns while leveraging the robust orchestration and
+policy management capabilities.
 """
 
 from __future__ import annotations
@@ -57,96 +213,577 @@ class HealthCheckTimeoutError(HealthCheckError):
 
 
 class HealthStatus(Enum):
+    """
+    Health status enumeration for components and systems.
+    
+    Provides standardized health status values for consistent health reporting
+    across all system components and overall system health evaluation.
+    
+    Values:
+        HEALTHY: Component is fully operational with normal performance
+        DEGRADED: Component is operational but with reduced functionality or performance
+        UNHEALTHY: Component is non-operational or experiencing critical failures
+    """
+
     ...
 
 
 @dataclass
 class ComponentStatus:
+    """
+    Health status information for individual system components.
+    
+    Encapsulates comprehensive health information for a single system component including
+    operational status, timing metrics, diagnostic messages, and component-specific metadata
+    for operational monitoring and troubleshooting.
+    
+    Attributes:
+        name: Component identifier for health reporting and monitoring (e.g., "database", "cache")
+        status: Current health status using standardized HealthStatus enumeration
+        message: Human-readable status description or error details for troubleshooting
+        response_time_ms: Health check execution time in milliseconds for performance monitoring
+        metadata: Optional component-specific health information and diagnostic data
+    
+    Usage:
+        # Create component status for healthy service
+        status = ComponentStatus(
+            name="database",
+            status=HealthStatus.HEALTHY,
+            message="Connection successful",
+            response_time_ms=45.2,
+            metadata={"connection_pool": "active", "query_test": "passed"}
+        )
+    
+        # Create status for degraded service
+        degraded_status = ComponentStatus(
+            name="cache",
+            status=HealthStatus.DEGRADED,
+            message="Redis unavailable, using memory fallback",
+            response_time_ms=12.1,
+            metadata={"cache_type": "memory", "redis_error": "connection_timeout"}
+        )
+    """
+
     ...
 
 
 @dataclass
 class SystemHealthStatus:
+    """
+    Aggregated health status for the entire system across all monitored components.
+    
+    Provides comprehensive system-wide health information by aggregating individual
+    component health statuses with timing information for monitoring systems and
+    operational dashboards.
+    
+    Attributes:
+        overall_status: Aggregated system health status using worst-case logic
+        components: List of individual component health statuses for detailed analysis
+        timestamp: Unix timestamp when health check execution completed for caching and monitoring
+    
+    Usage:
+        # Comprehensive system health evaluation
+        system_health = SystemHealthStatus(
+            overall_status=HealthStatus.DEGRADED,
+            components=[
+                ComponentStatus("database", HealthStatus.HEALTHY, "OK"),
+                ComponentStatus("cache", HealthStatus.DEGRADED, "Memory only"),
+                ComponentStatus("ai_model", HealthStatus.HEALTHY, "Configured")
+            ],
+            timestamp=time.time()
+        )
+    
+        # Operational monitoring integration
+        if system_health.overall_status != HealthStatus.HEALTHY:
+            unhealthy_components = [
+                c for c in system_health.components
+                if c.status != HealthStatus.HEALTHY
+            ]
+            alert_operations_team(unhealthy_components)
+    """
+
     ...
 
 
 class HealthChecker:
+    """
+    Centralized health monitoring service for system components with configurable timeouts and retry policies.
+    
+    Provides comprehensive health check orchestration across all application components including
+    AI services, cache infrastructure, resilience systems, and databases. Implements async-first
+    monitoring with configurable timeout policies, retry mechanisms, and graceful degradation
+    to ensure reliable health status reporting under all operational conditions.
+    
+    Attributes:
+        _checks: Registry of component health check functions keyed by component name
+        _default_timeout_ms: Default timeout for health checks when component-specific timeout not configured
+        _per_component_timeouts_ms: Component-specific timeout overrides for specialized monitoring requirements
+        _retry_count: Number of retry attempts for failed health checks before marking component unhealthy
+        _backoff_base_seconds: Base delay for exponential backoff between retry attempts
+    
+    Public Methods:
+        register_check(): Register health check function for a named component
+        check_component(): Execute health check for specific component with timeout and retry handling
+        check_all_components(): Execute all registered health checks concurrently and aggregate results
+    
+    State Management:
+        - Thread-safe registration and execution of health checks
+        - Immutable configuration once instantiated
+        - No persistent state between health check executions
+        - Graceful handling of component registration and deregistration
+    
+    Usage:
+        # Basic health checker initialization
+        checker = HealthChecker(
+            default_timeout_ms=2000,
+            retry_count=1,
+            backoff_base_seconds=0.1
+        )
+    
+        # Register component health checks
+        checker.register_check("database", check_database_health)
+        checker.register_check("cache", check_cache_health)
+        checker.register_check("ai_model", check_ai_model_health)
+    
+        # Execute individual component health check
+        status = await checker.check_component("database")
+        if status.status == HealthStatus.HEALTHY:
+            print(f"Database is healthy: {status.message}")
+        else:
+            print(f"Database issue: {status.message}")
+    
+        # Execute all health checks concurrently
+        system_health = await checker.check_all_components()
+        print(f"Overall system status: {system_health.overall_status}")
+    
+        for component in system_health.components:
+            print(f"{component.name}: {component.status.value} ({component.response_time_ms:.1f}ms)")
+    
+        # Advanced configuration with per-component timeouts
+        checker = HealthChecker(
+            default_timeout_ms=2000,
+            per_component_timeouts_ms={
+                "database": 5000,      # Longer timeout for database
+                "cache": 1000,         # Shorter timeout for cache
+                "ai_model": 3000       # Medium timeout for AI services
+            },
+            retry_count=2,
+            backoff_base_seconds=0.5
+        )
+    
+        # Error handling for health check failures
+        try:
+            status = await checker.check_component("external_service")
+        except ValueError as e:
+            print(f"Component not registered: {e}")
+        except HealthCheckError as e:
+            print(f"Health check infrastructure error: {e}")
+    """
+
     def __init__(self, default_timeout_ms: int = 2000, per_component_timeouts_ms: Optional[Dict[str, int]] = None, retry_count: int = 1, backoff_base_seconds: float = 0.1) -> None:
+        """
+        Initialize health checker with configurable timeout and retry policies.
+        
+        Args:
+            default_timeout_ms: Default timeout for health checks in milliseconds (100-30000, default: 2000).
+                               Used when component-specific timeout not configured.
+            per_component_timeouts_ms: Optional component-specific timeout overrides in milliseconds.
+                                     Keys are component names, values are timeout values.
+            retry_count: Number of retry attempts for failed health checks (0-10, default: 1).
+                        Set to 0 to disable retries for faster failure detection.
+            backoff_base_seconds: Base delay for exponential backoff between retries (0.0-5.0, default: 0.1).
+                                Initial delay doubles with each retry attempt.
+        
+        Behavior:
+            - Validates timeout and retry parameters within reasonable operational bounds
+            - Initializes empty health check registry for component registration
+            - Applies defensive parameter validation to prevent configuration errors
+            - Sets up retry and backoff configuration for reliable health monitoring
+            - Ensures configuration immutability after instantiation for thread safety
+        """
         ...
 
     def register_check(self, name: str, check_func: HealthCheckFunc) -> None:
+        """
+        Register health check function for a named component.
+        
+        Associates a health check function with a component name, enabling monitoring
+        of that component through the health checker infrastructure. The function must
+        be async and return a ComponentStatus with health information.
+        
+        Args:
+            name: Component identifier for health monitoring. Must be non-empty string,
+                 typically uses lowercase with underscores (e.g., "database", "cache", "ai_model").
+            check_func: Async health check function that returns ComponentStatus.
+                       Must be a coroutine function with signature () -> ComponentStatus.
+        
+        Raises:
+            ValueError: When component name is empty, None, or not a string
+            TypeError: When check_func is not an async coroutine function
+        
+        Behavior:
+            - Validates component name is non-empty string for consistent naming
+            - Verifies check function is async coroutine for proper async execution
+            - Stores health check function in internal registry keyed by component name
+            - Overwrites existing registration if component name already registered
+            - Enables immediate health monitoring of registered component
+        
+        Examples:
+            >>> checker = HealthChecker()
+            >>>
+            >>> # Register standard health check
+            >>> checker.register_check("database", check_database_health)
+            >>>
+            >>> # Register custom health check with lambda
+            >>> async def custom_service_check():
+            ...     return ComponentStatus("custom", HealthStatus.HEALTHY, "Service OK")
+            >>> checker.register_check("custom_service", custom_service_check)
+            >>>
+            >>> # Error cases
+            >>> checker.register_check("", check_func)  # Raises ValueError
+            >>> checker.register_check("service", lambda: "not async")  # Raises TypeError
+        """
         ...
 
     async def check_component(self, name: str) -> ComponentStatus:
+        """
+        Execute health check for specific component with timeout and retry handling.
+        
+        Performs health monitoring for a registered component with configurable timeout
+        and retry policies. Implements exponential backoff between retry attempts and
+        provides detailed timing information for performance monitoring.
+        
+        Args:
+            name: Component identifier for health check execution. Must match a previously
+                 registered component name exactly (case-sensitive).
+        
+        Returns:
+            ComponentStatus containing:
+            - name: str, component identifier that was checked
+            - status: HealthStatus, one of HEALTHY, DEGRADED, or UNHEALTHY
+            - message: str, human-readable status description or error details
+            - response_time_ms: float, total execution time including retries
+            - metadata: Optional[Dict], component-specific health information
+        
+        Raises:
+            ValueError: When component name is not registered in health checker
+        
+        Behavior:
+            - Validates component is registered before attempting health check
+            - Applies component-specific timeout if configured, otherwise uses default timeout
+            - Executes health check function with asyncio timeout protection
+            - Retries failed health checks according to configured retry policy
+            - Implements exponential backoff between retry attempts (base * 2^attempt)
+            - Logs warnings for timeout and execution failures for monitoring
+            - Distinguishes timeout errors (DEGRADED) from execution errors (UNHEALTHY)
+            - Tracks total execution time including all retry attempts
+            - Preserves original health check response timing when successful
+        
+        Examples:
+            >>> checker = HealthChecker(retry_count=2, backoff_base_seconds=0.1)
+            >>> checker.register_check("database", check_database_health)
+            >>>
+            >>> # Successful health check
+            >>> status = await checker.check_component("database")
+            >>> assert status.name == "database"
+            >>> assert status.status in [HealthStatus.HEALTHY, HealthStatus.DEGRADED, HealthStatus.UNHEALTHY]
+            >>> assert status.response_time_ms > 0
+            >>>
+            >>> # Component not registered
+            >>> try:
+            ...     await checker.check_component("nonexistent")
+            ... except ValueError as e:
+            ...     assert "not registered" in str(e)
+            >>>
+            >>> # Monitor performance with timeout handling
+            >>> status = await checker.check_component("slow_service")
+            >>> if status.status == HealthStatus.DEGRADED and "timed out" in status.message:
+            ...     print(f"Component {status.name} timeout after {status.response_time_ms:.1f}ms")
+        """
         ...
 
     async def check_all_components(self) -> SystemHealthStatus:
+        """
+        Execute all registered health checks concurrently and aggregate results.
+        
+        Performs comprehensive system health monitoring by executing all registered
+        component health checks in parallel for optimal performance. Aggregates
+        individual component results into overall system health status.
+        
+        Returns:
+            SystemHealthStatus containing:
+            - overall_status: HealthStatus, aggregated system health (HEALTHY, DEGRADED, UNHEALTHY)
+            - components: List[ComponentStatus], individual component health results
+            - timestamp: float, Unix timestamp when health check execution completed
+        
+        Behavior:
+            - Executes all registered health checks concurrently using asyncio.gather
+            - Does not fail if individual components throw exceptions (handles gracefully)
+            - Applies timeout and retry policies to each component independently
+            - Aggregates component statuses using worst-case overall status logic
+            - Returns UNHEALTHY if any component is UNHEALTHY
+            - Returns DEGRADED if any component is DEGRADED (and none are UNHEALTHY)
+            - Returns HEALTHY only if all components are HEALTHY
+            - Includes execution timestamp for monitoring and caching purposes
+            - Preserves individual component response times and metadata
+        
+        Examples:
+            >>> checker = HealthChecker()
+            >>> checker.register_check("database", check_database_health)
+            >>> checker.register_check("cache", check_cache_health)
+            >>> checker.register_check("ai_model", check_ai_model_health)
+            >>>
+            >>> # Execute comprehensive health check
+            >>> system_health = await checker.check_all_components()
+            >>>
+            >>> # Check overall system status
+            >>> if system_health.overall_status == HealthStatus.HEALTHY:
+            ...     print("All systems operational")
+            >>> elif system_health.overall_status == HealthStatus.DEGRADED:
+            ...     print("System operational with degraded components")
+            >>> else:
+            ...     print("System has critical health issues")
+            >>>
+            >>> # Review individual component results
+            >>> for component in system_health.components:
+            ...     print(f"{component.name}: {component.status.value}")
+            ...     if component.status != HealthStatus.HEALTHY:
+            ...         print(f"  Issue: {component.message}")
+            ...         print(f"  Response time: {component.response_time_ms:.1f}ms")
+            >>>
+            >>> # Health check for monitoring systems
+            >>> health_data = {
+            ...     "timestamp": system_health.timestamp,
+            ...     "overall_healthy": system_health.overall_status == HealthStatus.HEALTHY,
+            ...     "component_count": len(system_health.components),
+            ...     "unhealthy_components": [
+            ...         c.name for c in system_health.components
+            ...         if c.status == HealthStatus.UNHEALTHY
+            ...     ]
+            ... }
+        """
         ...
 
 
 async def check_ai_model_health() -> ComponentStatus:
+    """
+    Verify AI model service configuration and availability.
+    
+    Performs basic health validation for AI model services by checking configuration
+    availability and service readiness. Currently validates Gemini API configuration
+    without performing actual model calls for performance optimization.
+    
+    Returns:
+        ComponentStatus containing:
+        - name: str, always "ai_model" for consistent component identification
+        - status: HealthStatus, HEALTHY if API key configured, DEGRADED if missing, UNHEALTHY if check fails
+        - message: str, configuration status description for troubleshooting
+        - response_time_ms: float, health check execution time for performance monitoring
+        - metadata: Dict with provider information and configuration status
+    
+    Behavior:
+        - Validates AI service configuration without making external API calls
+        - Checks for presence of required API keys and configuration parameters
+        - Returns HEALTHY when AI services are properly configured and accessible
+        - Returns DEGRADED when configuration is missing but service structure is intact
+        - Returns UNHEALTHY when health check infrastructure itself fails
+        - Measures and reports health check execution time for monitoring
+        - Includes provider and configuration metadata for operational visibility
+        - Does not perform actual AI model inference for performance optimization
+    
+    Examples:
+        >>> # With valid API key configuration
+        >>> status = await check_ai_model_health()
+        >>> assert status.name == "ai_model"
+        >>> assert status.status == HealthStatus.HEALTHY
+        >>> assert "configured" in status.message
+        >>> assert status.metadata["provider"] == "gemini"
+        >>> assert status.metadata["has_api_key"] is True
+        >>>
+        >>> # Without API key configuration
+        >>> # (simulated by temporarily clearing settings.gemini_api_key)
+        >>> status = await check_ai_model_health()
+        >>> assert status.status == HealthStatus.DEGRADED
+        >>> assert "Missing" in status.message
+        >>> assert status.metadata["has_api_key"] is False
+        >>>
+        >>> # Performance monitoring
+        >>> status = await check_ai_model_health()
+        >>> assert status.response_time_ms > 0
+        >>> if status.response_time_ms > 100:
+        ...     print(f"AI health check slow: {status.response_time_ms:.1f}ms")
+    
+    Note:
+        This health check validates configuration readiness without performing actual
+        AI model calls to maintain fast health check response times. For comprehensive
+        AI service validation including model connectivity and inference capability,
+        consider implementing a separate deep health check endpoint.
+    """
     ...
 
 
-async def check_cache_health() -> ComponentStatus:
+async def check_cache_health(cache_service = None) -> ComponentStatus:
     """
-    Check cache system health and operational status.
+    Check cache system health and operational status using dependency injection for optimal performance.
     
-    ⚠️ PERFORMANCE ISSUE: This function currently creates a new AIResponseCache 
-    instance on every health check call, which is extremely inefficient and wasteful.
-    This causes redundant connection setup, memory allocation, and resource usage.
+    This function provides efficient cache health monitoring by accepting an optional cache service
+    parameter for dependency injection. When a cache service is provided, it reuses existing
+    connections and avoids redundant instantiation, making it ideal for frequent health checks.
+    For backward compatibility, it falls back to creating a new cache service when none is provided.
     
-    REQUIRED FIX: Update to accept cache service as parameter for dependency injection:
-    
-    async def check_cache_health(cache_service: AIResponseCache) -> ComponentStatus:
-        '''Reuse singleton cache service for optimal performance.'''
-        stats = await cache_service.get_cache_stats()  # Reuses existing connections
-        
-    Then update registration in get_health_checker():
-    checker.register_check("cache", lambda: check_cache_health(cache_service))
-    
-    This will eliminate redundant instantiation and significantly improve performance
-    under frequent monitoring scenarios.
+    Args:
+        cache_service: Optional AIResponseCache instance for optimal performance.
+                      When provided, reuses existing connections and avoids instantiation overhead.
+                      If None, creates a new cache service for backward compatibility.
     
     Returns:
         ComponentStatus with cache connectivity and operational status
+    
+    Performance Notes:
+        - ✅ OPTIMAL: When cache_service is provided, no instantiation overhead
+        - ⚠️ FALLBACK: When cache_service is None, creates new service instance (less efficient)
+        - For best performance, use dependency injection in get_health_checker()
     """
     ...
 
 
 async def check_resilience_health() -> ComponentStatus:
+    """
+    Monitor resilience infrastructure health including circuit breaker states and system stability.
+    
+    Evaluates the health of the resilience orchestration system by checking circuit breaker
+    states, failure patterns, and overall resilience infrastructure availability. Provides
+    detailed circuit breaker status information for operational monitoring and alerting.
+    
+    Returns:
+        ComponentStatus containing:
+        - name: str, always "resilience" for consistent component identification
+        - status: HealthStatus, HEALTHY if all circuits closed, DEGRADED if circuits open, UNHEALTHY if system fails
+        - message: str, resilience system status or circuit breaker alert information
+        - response_time_ms: float, health check execution time for performance monitoring
+        - metadata: Dict with detailed circuit breaker states and counts for operational visibility
+    
+    Behavior:
+        - Queries resilience orchestrator for current circuit breaker states and health metrics
+        - Returns HEALTHY when all circuit breakers are closed and system is operational
+        - Returns DEGRADED when circuit breakers are open but resilience system is functional
+        - Returns UNHEALTHY when resilience infrastructure itself is unavailable or failing
+        - Provides detailed circuit breaker status including open, half-open, and total counts
+        - Includes circuit breaker names in metadata for specific failure identification
+        - Measures health check execution time for performance monitoring
+        - Enables differentiation between individual service failures and infrastructure failures
+    
+    Examples:
+        >>> # Healthy resilience system with all circuits closed
+        >>> status = await check_resilience_health()
+        >>> assert status.name == "resilience"
+        >>> assert status.status == HealthStatus.HEALTHY
+        >>> assert "healthy" in status.message.lower()
+        >>> assert status.metadata["total_circuit_breakers"] >= 0
+        >>> assert len(status.metadata["open_circuit_breakers"]) == 0
+        >>>
+        >>> # Degraded system with open circuit breakers
+        >>> # (simulated during external service failures)
+        >>> status = await check_resilience_health()
+        >>> if status.status == HealthStatus.DEGRADED:
+        ...     open_breakers = status.metadata["open_circuit_breakers"]
+        ...     print(f"Circuit breakers open: {open_breakers}")
+        ...     assert "circuit breakers" in status.message.lower()
+        >>>
+        >>> # Monitor circuit breaker recovery
+        >>> status = await check_resilience_health()
+        >>> half_open = status.metadata["half_open_circuit_breakers"]
+        >>> if half_open:
+        ...     print(f"Circuit breakers recovering: {half_open}")
+        >>>
+        >>> # Operational monitoring integration
+        >>> status = await check_resilience_health()
+        >>> resilience_metrics = {
+        ...     "healthy": status.status == HealthStatus.HEALTHY,
+        ...     "open_breakers": len(status.metadata["open_circuit_breakers"]),
+        ...     "total_breakers": status.metadata["total_circuit_breakers"],
+        ...     "response_time": status.response_time_ms
+        ... }
+    
+    Note:
+        Circuit breaker states indicate external service health rather than resilience
+        infrastructure health. Open circuit breakers suggest external service issues
+        but confirm the resilience system is working correctly by preventing cascade failures.
+    """
     ...
 
 
 async def check_database_health() -> ComponentStatus:
     """
-    ⚠️ PLACEHOLDER DATABASE HEALTH CHECK - Always returns healthy!
+    Placeholder database health check for template demonstration purposes.
+    
+    ⚠️ **IMPORTANT**: This is a placeholder implementation that always returns HEALTHY
+    regardless of actual database connectivity. This function serves as a template example
+    and must be replaced with actual database health validation for production use.
     
-    This function is NOT a functional health check. It always returns HEALTHY
-    regardless of actual database state. This could mislead operators about
-    system health.
+    Returns:
+        ComponentStatus containing:
+        - name: str, always "database" for consistent component identification
+        - status: HealthStatus, always HEALTHY (placeholder behavior)
+        - message: str, "Not implemented" to indicate placeholder status
+        - response_time_ms: float, minimal execution time for placeholder operation
+        - metadata: None, no database-specific information available
     
-    Replace with actual database connectivity validation for production:
+    Behavior:
+        - Always returns HEALTHY status regardless of actual database state
+        - Does not perform any actual database connectivity testing
+        - Provides minimal response time measurement for consistency
+        - Serves as template structure for implementing real database health checks
+        - Should be replaced with actual database validation logic for production
     
-    async def check_database_health() -> ComponentStatus:
-        name = "database"
-        start = time.perf_counter()
-        try:
-            async with get_database_connection() as conn:
-                await conn.execute("SELECT 1")  # Test connectivity
-            return ComponentStatus(
-                name=name, status=HealthStatus.HEALTHY,
-                message="Database connection successful",
-                response_time_ms=(time.perf_counter() - start) * 1000.0
-            )
-        except Exception as e:
-            return ComponentStatus(
-                name=name, status=HealthStatus.UNHEALTHY,
-                message=f"Database connection failed: {e}",
-                response_time_ms=(time.perf_counter() - start) * 1000.0
-            )
+    Examples:
+        >>> # Current placeholder behavior
+        >>> status = await check_database_health()
+        >>> assert status.name == "database"
+        >>> assert status.status == HealthStatus.HEALTHY  # Always healthy (placeholder)
+        >>> assert status.message == "Not implemented"
+        >>> assert status.response_time_ms >= 0
     
-    Returns:
-        ComponentStatus: Always HEALTHY with "Not implemented" message
+    Production Implementation Example:
+        Replace this placeholder with actual database health validation:
+    
+        ```python
+        async def check_database_health() -> ComponentStatus:
+            name = "database"
+            start = time.perf_counter()
+            try:
+                async with get_database_connection() as conn:
+                    await conn.execute("SELECT 1")  # Test connectivity
+                    result = await conn.fetchone()
+                    assert result[0] == 1  # Verify query execution
+    
+                return ComponentStatus(
+                    name=name,
+                    status=HealthStatus.HEALTHY,
+                    message="Database connection successful",
+                    response_time_ms=(time.perf_counter() - start) * 1000.0,
+                    metadata={"query_test": "passed", "connection_pool": "active"}
+                )
+            except asyncio.TimeoutError:
+                return ComponentStatus(
+                    name=name,
+                    status=HealthStatus.DEGRADED,
+                    message="Database connection timeout",
+                    response_time_ms=(time.perf_counter() - start) * 1000.0
+                )
+            except Exception as e:
+                return ComponentStatus(
+                    name=name,
+                    status=HealthStatus.UNHEALTHY,
+                    message=f"Database connection failed: {e}",
+                    response_time_ms=(time.perf_counter() - start) * 1000.0
+                )
+        ```
+    
+    Note:
+        This placeholder function is included to demonstrate health check infrastructure
+        patterns without requiring actual database dependencies. Template users should
+        implement proper database connectivity validation based on their specific
+        database technology and connection patterns.
     """
     ...
diff --git a/backend/contracts/infrastructure/resilience/__init__.pyi b/backend/contracts/infrastructure/resilience/__init__.pyi
index 9a024f8..2807b30 100644
--- a/backend/contracts/infrastructure/resilience/__init__.pyi
+++ b/backend/contracts/infrastructure/resilience/__init__.pyi
@@ -14,7 +14,6 @@ flexibility and operational visibility:
 - **Configuration Layer**: Intelligent preset management with dynamic configuration and validation
 - **Orchestration Layer**: High-level decorators and service integration for seamless adoption
 - **Monitoring Layer**: Comprehensive metrics, performance tracking, and alerting system
-- **Migration Layer**: Legacy configuration analysis and automated migration utilities
 
 ## Core Components
 
@@ -39,7 +38,6 @@ Simplified configuration system with intelligent defaults:
 - **Preset System**: Pre-configured strategies (simple, development, production)
 - **Environment Detection**: Automatic environment-specific configuration
 - **Dynamic Updates**: Runtime configuration changes with validation
-- **Legacy Migration**: Automated migration from complex legacy configurations
 - **Validation**: Comprehensive configuration validation with detailed error reporting
 
 ### Orchestration (`orchestrator.py`)
@@ -148,7 +146,6 @@ from .config_presets import ResilienceStrategy, ResilienceConfig, get_default_pr
 from .orchestrator import AIServiceResilience, ai_resilience, with_operation_resilience, with_aggressive_resilience, with_balanced_resilience, with_conservative_resilience, with_critical_resilience
 from .config_validator import ResilienceConfigValidator, ValidationResult, ValidationRateLimiter, RESILIENCE_CONFIG_SCHEMA, CONFIGURATION_TEMPLATES, SECURITY_CONFIG
 from .config_monitoring import ConfigurationMetricsCollector, ConfigurationMetric, ConfigurationUsageStats, ConfigurationAlert, MetricType, AlertLevel
-from .migration_utils import LegacyConfigAnalyzer, ConfigurationMigrator, MigrationRecommendation, MigrationConfidence
 from .performance_benchmarks import ConfigurationPerformanceBenchmark, BenchmarkResult, BenchmarkSuite, PerformanceThreshold
 
-__all__ = ['AIServiceException', 'TransientAIError', 'PermanentAIError', 'RateLimitError', 'ServiceUnavailableError', 'RetryConfig', 'CircuitBreakerConfig', 'ResilienceConfig', 'ResilienceStrategy', 'EnhancedCircuitBreaker', 'ResilienceMetrics', 'AIServiceResilience', 'classify_exception', 'should_retry_on_exception', 'get_default_presets', 'DEFAULT_PRESETS', 'preset_manager', 'ai_resilience', 'with_operation_resilience', 'with_aggressive_resilience', 'with_balanced_resilience', 'with_conservative_resilience', 'with_critical_resilience', 'ResilienceConfigValidator', 'ValidationResult', 'ValidationRateLimiter', 'RESILIENCE_CONFIG_SCHEMA', 'CONFIGURATION_TEMPLATES', 'SECURITY_CONFIG', 'ConfigurationMetricsCollector', 'ConfigurationMetric', 'ConfigurationUsageStats', 'ConfigurationAlert', 'MetricType', 'AlertLevel', 'LegacyConfigAnalyzer', 'ConfigurationMigrator', 'MigrationRecommendation', 'MigrationConfidence', 'ConfigurationPerformanceBenchmark', 'BenchmarkResult', 'BenchmarkSuite', 'PerformanceThreshold', 'preset_manager']
+__all__ = ['AIServiceException', 'TransientAIError', 'PermanentAIError', 'RateLimitError', 'ServiceUnavailableError', 'RetryConfig', 'CircuitBreakerConfig', 'ResilienceConfig', 'ResilienceStrategy', 'EnhancedCircuitBreaker', 'ResilienceMetrics', 'AIServiceResilience', 'classify_exception', 'should_retry_on_exception', 'get_default_presets', 'DEFAULT_PRESETS', 'preset_manager', 'ai_resilience', 'with_operation_resilience', 'with_aggressive_resilience', 'with_balanced_resilience', 'with_conservative_resilience', 'with_critical_resilience', 'ResilienceConfigValidator', 'ValidationResult', 'ValidationRateLimiter', 'RESILIENCE_CONFIG_SCHEMA', 'CONFIGURATION_TEMPLATES', 'SECURITY_CONFIG', 'ConfigurationMetricsCollector', 'ConfigurationMetric', 'ConfigurationUsageStats', 'ConfigurationAlert', 'MetricType', 'AlertLevel', 'ConfigurationPerformanceBenchmark', 'BenchmarkResult', 'BenchmarkSuite', 'PerformanceThreshold', 'preset_manager']
diff --git a/backend/contracts/infrastructure/resilience/migration_utils.pyi b/backend/contracts/infrastructure/resilience/migration_utils.pyi
deleted file mode 100644
index c3e0b03..0000000
--- a/backend/contracts/infrastructure/resilience/migration_utils.pyi
+++ /dev/null
@@ -1,98 +0,0 @@
-"""
-Configuration migration utilities for legacy-to-preset migration.
-
-This module provides tools to analyze legacy resilience configurations
-and recommend appropriate preset replacements with migration guidance.
-"""
-
-import os
-import json
-import logging
-from typing import Dict, List, Optional, Any, NamedTuple
-from dataclasses import dataclass
-from enum import Enum
-
-
-class MigrationConfidence(Enum):
-    """
-    Confidence levels for migration recommendations.
-    """
-
-    ...
-
-
-@dataclass
-class MigrationRecommendation:
-    """
-    Recommendation for migrating from legacy to preset configuration.
-    """
-
-    def __post_init__(self):
-        ...
-
-
-class LegacyConfigAnalyzer:
-    """
-    Analyzer for legacy resilience configuration patterns.
-    """
-
-    def __init__(self):
-        """
-        Initialize the legacy configuration analyzer.
-        """
-        ...
-
-    def detect_legacy_configuration(self, env_vars: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
-        """
-        Detect legacy resilience configuration from environment variables.
-        
-        Args:
-            env_vars: Environment variables to analyze (if None, uses os.environ)
-            
-        Returns:
-            Dictionary of detected legacy configuration values
-        """
-        ...
-
-    def recommend_preset(self, legacy_config: Dict[str, Any]) -> MigrationRecommendation:
-        """
-        Recommend appropriate preset based on legacy configuration analysis.
-        
-        Args:
-            legacy_config: Detected legacy configuration
-            
-        Returns:
-            Migration recommendation with preset, confidence, and guidance
-        """
-        ...
-
-
-class ConfigurationMigrator:
-    """
-    Tool for performing automated configuration migrations.
-    """
-
-    def __init__(self):
-        """
-        Initialize the configuration migrator.
-        """
-        ...
-
-    def analyze_current_environment(self) -> MigrationRecommendation:
-        """
-        Analyze current environment and provide migration recommendation.
-        """
-        ...
-
-    def generate_migration_script(self, recommendation: MigrationRecommendation, output_format: str = 'bash') -> str:
-        """
-        Generate migration script in specified format.
-        
-        Args:
-            recommendation: Migration recommendation to implement
-            output_format: Script format ('bash', 'env', 'docker')
-            
-        Returns:
-            Generated migration script content
-        """
-        ...
diff --git a/backend/contracts/infrastructure/security/__init__.pyi b/backend/contracts/infrastructure/security/__init__.pyi
index 72844f2..4024aef 100644
--- a/backend/contracts/infrastructure/security/__init__.pyi
+++ b/backend/contracts/infrastructure/security/__init__.pyi
@@ -57,14 +57,14 @@ Robust authentication system with flexible configuration:
 ### Basic Authentication
 ```python
 from fastapi import Depends
-from app.infrastructure.security import verify_api_key
+from app.infrastructure.security import verify_api_key_http
 
 @app.get("/protected")
-async def protected_endpoint(api_key: str = Depends(verify_api_key)):
+async def protected_endpoint(api_key: str = Depends(verify_api_key_http)):
     return {"message": "Access granted", "authenticated": True}
 
 @app.post("/admin")
-async def admin_endpoint(api_key: str = Depends(verify_api_key)):
+async def admin_endpoint(api_key: str = Depends(verify_api_key_http)):
     # Automatically authenticated with any valid API key
     return {"message": "Admin access", "user": api_key}
 ```
@@ -102,10 +102,10 @@ async def status_endpoint():
 
 ### Custom Authentication Validation
 ```python
-from app.infrastructure.security import verify_api_key
+from app.infrastructure.security import verify_api_key_http
 from fastapi import HTTPException, Depends
 
-async def admin_only_auth(api_key: str = Depends(verify_api_key)):
+async def admin_only_auth(api_key: str = Depends(verify_api_key_http)):
     # Add additional validation for admin endpoints
     if not is_admin_key(api_key):
         raise HTTPException(status_code=403, detail="Admin access required")
diff --git a/backend/contracts/infrastructure/security/auth.pyi b/backend/contracts/infrastructure/security/auth.pyi
index 66d1f68..a480da3 100644
--- a/backend/contracts/infrastructure/security/auth.pyi
+++ b/backend/contracts/infrastructure/security/auth.pyi
@@ -1,201 +1,282 @@
 """
-Authentication and Authorization Module for FastAPI Applications.
-
-This module provides a comprehensive, extensible authentication system built around
-API key authentication with FastAPI. It supports multiple operation modes, from simple
-development setups to advanced production configurations with user tracking,
-permissions, and request logging.
-
-Key Features:
-    - **API Key Authentication**: Secure Bearer token authentication using configurable API keys
-    - **Multiple Operation Modes**: Simple mode for development, advanced mode for production
-    - **Extensible Architecture**: Built-in extension points for custom authentication logic
-    - **Development Support**: Automatic development mode when no keys are configured
-    - **Test Integration**: Built-in test mode support for automated testing
-    - **User Tracking**: Optional user context and request metadata tracking
-    - **Flexible Configuration**: Environment-based configuration with runtime reloading
-    - **HTTP Exception Compatibility**: Wrapper dependencies that convert custom exceptions to proper HTTP responses
-
-Architecture:
-    The module is structured around four main components:
-    
-    1. **AuthConfig**: Manages authentication configuration and feature flags
-    2. **APIKeyAuth**: Handles API key validation and metadata management
-    3. **FastAPI Dependencies**: Provides authentication dependencies for route protection
-    4. **HTTPException Wrappers**: Converts custom exceptions to FastAPI-compatible HTTP responses
-
-Operation Modes:
-    - **Simple Mode** (default): Basic API key validation without advanced features
-    - **Advanced Mode**: Full feature set including user tracking and permissions
-    - **Development Mode**: Automatic when no API keys configured, allows unauthenticated access
-    - **Test Mode**: Special handling for automated tests with test keys
-
-Configuration:
-    The module supports configuration through environment variables:
-    
-    - `AUTH_MODE`: "simple" (default) or "advanced"
-    - `ENABLE_USER_TRACKING`: Enable user context tracking ("true"/"false")
-    - `ENABLE_REQUEST_LOGGING`: Enable request metadata logging ("true"/"false")
-    - `API_KEY`: Primary API key for authentication
-    - `ADDITIONAL_API_KEYS`: Comma-separated additional API keys
-    - `PYTEST_CURRENT_TEST`: Automatically set by pytest for test mode
-
-Usage Examples:
-    Basic API key protection:
-        ```python
-        from fastapi import FastAPI, Depends
-        from app.infrastructure.security.auth import verify_api_key
-        
-        app = FastAPI()
-        
-        @app.get("/protected")
-        async def protected_endpoint(api_key: str = Depends(verify_api_key)):
-            return {"message": "Access granted", "key": api_key}
-        ```
-    
-    With metadata tracking:
-        ```python
-        from app.infrastructure.security.auth import verify_api_key_with_metadata
-        
-        @app.get("/protected-with-metadata")
-        async def protected_with_metadata(
-            auth_data: dict = Depends(verify_api_key_with_metadata)
-        ):
-            return {
-                "message": "Access granted",
-                "auth_data": auth_data
-            }
-        ```
-    
-    Optional authentication:
-        ```python
-        from app.infrastructure.security.auth import optional_verify_api_key
-        
-        @app.get("/optional-auth")
-        async def optional_auth(api_key: str = Depends(optional_verify_api_key)):
-            if api_key:
-                return {"message": "Authenticated access", "key": api_key}
-            return {"message": "Anonymous access"}
-        ```
-    
-    Manual key verification:
-        ```python
-        from app.infrastructure.security.auth import verify_api_key_string
-        
-        def custom_logic(key: str):
-            if verify_api_key_string(key):
-                return "Valid key"
-            return "Invalid key"
-        ```
-    
-    HTTP Exception compatibility (recommended for FastAPI endpoints):
-        ```python
-        from app.infrastructure.security.auth import verify_api_key_http
-        
-        @app.post("/internal/cache/invalidate")
-        async def protected_endpoint(api_key: str = Depends(verify_api_key_http)):
-            # This dependency automatically converts AuthenticationError to 401 HTTPException
-            # Avoids middleware conflicts and provides clean HTTP responses
-            return {"message": "Authenticated access", "key": api_key}
-        ```
-
-Extension Points:
-    The module provides several extension points for customization:
-    
-    - **Key Metadata**: Add custom metadata to API keys via `_key_metadata`
-    - **Request Context**: Extend `add_request_metadata()` for custom request tracking
-    - **Authentication Logic**: Override `verify_api_key()` for custom validation
-    - **Configuration**: Extend `AuthConfig` for additional configuration options
-
-Security Considerations:
-    - API keys are stored in memory and loaded from environment variables
-    - Invalid authentication attempts are logged with truncated key information
-    - Test mode only activates when `PYTEST_CURRENT_TEST` is set
-    - Development mode warnings are logged when no keys are configured
-    - Bearer token authentication follows RFC 6750 standard
-
-Dependencies:
-    - FastAPI: Web framework and security utilities
-    - Python logging: For authentication event logging
-    - Environment variables: For configuration management
-
-Thread Safety:
-    The module is thread-safe for read operations. Key reloading via `reload_keys()`
-    should be used carefully in multi-threaded environments.
-
-Performance:
-    - API key verification is O(1) using set-based lookups
-    - Metadata operations are O(1) dictionary lookups
-    - Minimal overhead for simple mode operations
-
-Error Handling:
-    The module provides two approaches for error handling:
-    
-    **Standard Dependencies** (verify_api_key, optional_verify_api_key):
-    - Raise `AuthenticationError` custom exceptions
-    - Handled by global exception handlers
-    - May cause middleware conflicts in dependency injection
-    
-    **HTTP Wrapper Dependencies** (verify_api_key_http - recommended):
-    - Convert `AuthenticationError` to `HTTPException` automatically
-    - Return proper `401 Unauthorized` responses with detailed error messages
-    - Include WWW-Authenticate headers and structured error context
-    - Avoid middleware conflicts and provide clean HTTP responses
-    - Preserve original error messages and debugging context
-    
-    Both approaches support graceful degradation in development mode
-
-Version: 1.0.0
-Author: FastAPI LLM Starter Team
-License: MIT
+Environment-Aware Authentication and Authorization Module for FastAPI Applications.
+
+This module provides a comprehensive, production-ready authentication system built around
+API key authentication with environment-aware security enforcement. It integrates with
+the unified environment detection service to provide automatic production security
+validation, development mode support, and extensive operational monitoring.
+
+## 🔐 Key Features
+
+- **🌍 Environment-Aware Security**: Automatic production security enforcement with
+  environment detection integration
+- **🔑 Multi-Key API Authentication**: Secure Bearer token authentication with support
+  for multiple API keys and key rotation
+- **⚙️ Flexible Operation Modes**: Simple development mode to advanced production
+  configurations with user tracking and permissions
+- **🏗️ Extensible Architecture**: Built-in extension points for custom authentication
+  logic and metadata management
+- **🛠️ Development Support**: Automatic development mode when no keys are configured,
+  with clear warnings and guidance
+- **📊 Operational Monitoring**: Comprehensive logging, status endpoints, and
+  authentication event tracking
+- **🔄 HTTP Exception Compatibility**: Wrapper dependencies that convert custom
+  exceptions to proper HTTP responses for middleware compatibility
+
+## 📐 Architecture
+
+The module follows a layered architecture with four main components:
+
+1. **`AuthConfig`** - Environment-based configuration and feature flag management
+2. **`APIKeyAuth`** - Multi-key validation with production security enforcement
+3. **`FastAPI Dependencies`** - Authentication dependencies for route protection
+4. **`HTTP Wrapper Dependencies`** - HTTPException conversion for middleware compatibility
+
+## 🎛️ Operation Modes
+
+### Development Mode
+- **Trigger**: No API keys configured in environment variables
+- **Behavior**: Allows unauthenticated access with warning logs
+- **Use Case**: Local development and testing without security overhead
+
+### Production Mode
+- **Trigger**: Production environment detected OR API keys configured
+- **Behavior**: Mandatory authentication with fail-fast security validation
+- **Use Case**: Production deployments with strict security enforcement
+
+### Advanced Mode
+- **Trigger**: `AUTH_MODE=advanced` environment variable
+- **Behavior**: Full feature set including user tracking and request metadata
+- **Use Case**: Enterprise deployments requiring detailed audit trails
+
+## ⚙️ Configuration
+
+Environment variables for authentication configuration:
+
+```bash
+# Authentication Mode
+AUTH_MODE=simple                    # "simple" (default) or "advanced"
+
+# API Key Configuration
+API_KEY=sk-1234567890abcdef         # Primary API key
+ADDITIONAL_API_KEYS=sk-key2,sk-key3 # Comma-separated additional keys
+
+# Advanced Features (when AUTH_MODE=advanced)
+ENABLE_USER_TRACKING=true           # Enable user context tracking
+ENABLE_REQUEST_LOGGING=true         # Enable request metadata logging
+
+# Environment Detection (automatic)
+ENVIRONMENT=production               # Detected automatically by environment service
+```
+
+## 🚀 Usage Examples
+
+### Basic API Protection (Recommended)
+
+```python
+from fastapi import FastAPI, Depends
+from app.infrastructure.security.auth import verify_api_key_http
+
+app = FastAPI()
+
+@app.get("/protected")
+async def protected_endpoint(api_key: str = Depends(verify_api_key_http)):
+    return {"message": "Access granted", "key_prefix": api_key[:8]}
+```
+
+### Advanced Metadata Tracking
+
+```python
+from app.infrastructure.security.auth import verify_api_key_with_metadata
+
+@app.get("/protected-advanced")
+async def protected_with_metadata(
+    auth_data: dict = Depends(verify_api_key_with_metadata)
+):
+    return {
+        "message": "Access granted",
+        "api_key": auth_data["api_key"],
+        "metadata": auth_data.get("metadata", {})
+    }
+```
+
+### Optional Authentication
+
+```python
+from app.infrastructure.security.auth import optional_verify_api_key
+
+@app.get("/optional-auth")
+async def optional_auth(api_key: str = Depends(optional_verify_api_key)):
+    if api_key and api_key != "development":
+        return {"message": "Authenticated access", "key_prefix": api_key[:8]}
+    return {"message": "Anonymous access"}
+```
+
+### Programmatic Validation
+
+```python
+from app.infrastructure.security.auth import verify_api_key_string
+
+def validate_batch_request(api_key: str, data: list) -> bool:
+    # Validate API key for batch processing
+    if not verify_api_key_string(api_key):
+        raise ValueError("Invalid API key for batch processing")
+    return True
+```
+
+### System Status Monitoring
+
+```python
+from app.infrastructure.security.auth import get_auth_status
+
+@app.get("/internal/auth/status")
+async def auth_status():
+    # Get authentication system status for monitoring
+    return get_auth_status()
+```
+
+## 🔧 Extension Points
+
+The module provides extensive customization capabilities:
+
+- **Key Metadata Management**: Custom per-key metadata via `_key_metadata`
+- **Request Context Tracking**: Extended request metadata via `add_request_metadata()`
+- **Authentication Logic**: Custom validation by extending `APIKeyAuth.verify_api_key()`
+- **Configuration Extension**: Additional options via `AuthConfig` inheritance
+- **Environment Integration**: Custom environment detection via environment service
+
+## 🛡️ Security Features
+
+### Production Security Enforcement
+- **Fail-Fast Validation**: Prevents deployment without proper API key configuration
+- **Environment Detection**: Automatic security mode based on environment confidence
+- **Mandatory Authentication**: Required credentials in production/staging environments
+- **Security Event Logging**: Comprehensive audit trail for authentication events
+
+### Operational Security
+- **Key Rotation Support**: Runtime key reloading via `reload_keys()`
+- **Truncated Logging**: Invalid attempts logged with masked key information
+- **Development Warnings**: Clear guidance when security is disabled
+- **RFC 6750 Compliance**: Standard Bearer token authentication implementation
+
+## ⚡ Performance Characteristics
+
+- **O(1) Key Validation**: Set-based lookups for API key verification
+- **O(1) Metadata Access**: Dictionary-based metadata operations
+- **Minimal Overhead**: Efficient operation in simple mode
+- **Thread-Safe Operations**: Concurrent request handling support
+- **Lazy Environment Detection**: Environment info loaded only when needed
+
+## 🔄 Error Handling Strategies
+
+### Standard Dependencies (`verify_api_key`, `optional_verify_api_key`)
+- Raise `AuthenticationError` custom exceptions with rich context
+- Handled by global exception handlers with structured error responses
+- Include environment detection information and confidence scores
+- May cause middleware conflicts in complex dependency injection scenarios
+
+### HTTP Wrapper Dependencies (`verify_api_key_http` - **Recommended**)
+- Convert `AuthenticationError` to `HTTPException` automatically
+- Return proper `401 Unauthorized` responses with detailed error context
+- Include `WWW-Authenticate` headers for proper HTTP authentication flow
+- Avoid middleware conflicts and provide clean HTTP responses
+- Preserve original error messages and debugging context for troubleshooting
+
+## 🔗 Dependencies
+
+- **FastAPI**: Web framework and security utilities (`fastapi.security.HTTPBearer`)
+- **Environment Detection**: Unified environment service (`app.core.environment`)
+- **Exception Handling**: Custom exception types (`app.core.exceptions`)
+- **Configuration**: Application settings (`app.core.config`)
+- **Python Standard Library**: `os`, `sys`, `logging`, `typing`
+
+## 🔒 Thread Safety
+
+- **Read Operations**: Fully thread-safe for concurrent request handling
+- **Key Validation**: Thread-safe set-based lookups across multiple workers
+- **Configuration Access**: Thread-safe environment variable reading
+- **Key Reloading**: Use `reload_keys()` carefully in multi-threaded environments
+- **Metadata Operations**: Thread-safe dictionary operations with proper locking
+
+## 📝 Version Information
+
+- **Version**: 2.0.0 (Environment-Aware Security Release)
+- **Author**: FastAPI LLM Starter Team
+- **License**: MIT
+- **API Compatibility**: Backward compatible with 1.x authentication contracts
+- **Environment Integration**: Requires unified environment detection service
+
+## 🔗 Related Documentation
+
+- **Environment Detection**: `app.core.environment` - Unified environment detection service
+- **Exception Handling**: `app.core.exceptions` - Custom exception types and handling
+- **FastAPI Security**: `app.api.v1.auth` - Public authentication endpoints
+- **Internal Monitoring**: `app.api.internal.monitoring` - Administrative auth status
 """
 
 import os
 import sys
 import logging
 from typing import Optional, Dict, Any
-from fastapi import Depends, status, HTTPException
+from fastapi import Depends, status, HTTPException, Request
 from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
 from app.core.config import settings
-from app.core.exceptions import AuthenticationError
+from app.core.exceptions import AuthenticationError, ConfigurationError
+from app.core.environment import get_environment_info, FeatureContext, Environment
 
 
 class AuthConfig:
     """
-    Authentication configuration manager with environment-based settings and extensibility hooks.
+    Authentication configuration manager providing environment-based settings and feature control.
     
-    Manages authentication behavior, feature flags, and operation modes through environment
-    variables, providing a flexible foundation for both simple API key validation and
-    advanced user management systems.
+    Manages authentication behavior modes and feature flags through environment variables,
+    supporting both simple API key validation and advanced user management capabilities.
     
     Attributes:
-        simple_mode: bool indicating basic API key validation mode (default)
-        enable_user_tracking: bool for user context and session tracking
-        enable_request_logging: bool for request metadata collection
-        
+        simple_mode: bool indicating basic API key validation mode (default: True)
+        enable_user_tracking: bool for user context and session tracking (default: False)
+        enable_request_logging: bool for request metadata collection (default: False)
+    
     Public Methods:
-        supports_user_context: Property indicating user context availability
-        
+        supports_user_context(): Returns True if advanced user context features are enabled
+        supports_permissions(): Returns True if permission-based access control is supported
+        supports_rate_limiting(): Returns True if rate limiting features are supported
+        get_auth_info(): Returns comprehensive configuration information dictionary
+    
     State Management:
-        - Environment-driven configuration with sensible defaults
-        - Immutable configuration after initialization
-        - Extension points for custom authentication logic
-        - Thread-safe operation for concurrent request handling
-        
+        - Configuration loaded from environment variables at initialization
+        - Immutable state after construction (no runtime configuration changes)
+        - Thread-safe for concurrent access across request handlers
+        - Extensible through inheritance for custom authentication requirements
+    
+    Behavior:
+        - Reads AUTH_MODE environment variable ("simple" or "advanced", default: "simple")
+        - Reads ENABLE_USER_TRACKING environment variable ("true"/"false", default: "false")
+        - Reads ENABLE_REQUEST_LOGGING environment variable ("true"/"false", default: "false")
+        - Provides feature capability checks through property methods
+        - Returns comprehensive configuration summary via get_auth_info()
+        - Supports inheritance for extending authentication capabilities
+    
     Usage:
-        # Default configuration for simple API key authentication
+        # Basic authentication configuration
         config = AuthConfig()
-        
-        # Environment-based configuration
-        # AUTH_MODE=advanced ENABLE_USER_TRACKING=true
+        assert config.simple_mode is True
+        assert config.supports_user_context() is False
+    
+        # Environment-driven advanced configuration
+        # Set: AUTH_MODE=advanced ENABLE_USER_TRACKING=true
         config = AuthConfig()
         if not config.simple_mode:
-            print("Advanced authentication features enabled")
-            
-        # Extension point usage
+            user_features_available = config.supports_user_context()
+    
+        # Configuration inspection
+        auth_info = config.get_auth_info()
+        print(f"Mode: {auth_info['mode']}")
+    
+        # Custom extension
         class CustomAuthConfig(AuthConfig):
-            def supports_permissions(self) -> bool:
-                return True
+            def supports_custom_feature(self) -> bool:
+                return not self.simple_mode
     """
 
     def __init__(self):
@@ -231,43 +312,66 @@ class AuthConfig:
 
 class APIKeyAuth:
     """
-    API key authentication handler with multi-key support and extensible metadata management.
+    API key authentication handler providing multi-key validation and environment-aware security.
     
-    Provides secure API key validation with support for multiple keys, development mode,
-    test integration, and extensibility hooks for advanced authentication requirements.
-    Handles Bearer token extraction, validation, and optional user context management.
+    Manages API key validation with support for multiple keys, production security validation,
+    development mode fallbacks, and extensible metadata management for advanced authentication
+    scenarios. Integrates with environment detection for production security enforcement.
     
     Attributes:
-        config: AuthConfig instance controlling authentication behavior
-        api_keys: Set[str] containing valid API keys for authentication
-        _key_metadata: Dict[str, Dict[str, Any]] extensible metadata per API key
-        
+        config: AuthConfig instance controlling authentication behavior and feature flags
+        api_keys: Set[str] containing all valid API keys loaded from environment variables
+        _key_metadata: Dict[str, Dict[str, Any]] extensible per-key metadata for advanced features
+    
     Public Methods:
-        authenticate(): Primary authentication method with Bearer token validation
-        get_user_context(): Extract user information from authenticated requests
-        add_key_metadata(): Associate metadata with specific API keys
-        
+        verify_api_key(api_key): Validates a single API key string against configured keys
+        get_key_metadata(api_key): Retrieves metadata associated with specific API key
+        add_request_metadata(api_key, request_info): Generates request-specific metadata
+        reload_keys(): Reloads API keys from environment variables
+    
     State Management:
-        - Thread-safe API key validation for concurrent requests
-        - Immutable key set after initialization (reload required for changes)
-        - Extensible metadata system for custom authentication logic
-        - Development and test mode support with automatic fallbacks
-        
+        - API keys loaded from environment variables at initialization
+        - Thread-safe key validation for concurrent request handling
+        - Production security validation enforced during initialization
+        - Immutable key set after initialization (use reload_keys() for updates)
+        - Extensible metadata system for custom authentication requirements
+    
+    Behavior:
+        - Loads API_KEY and ADDITIONAL_API_KEYS from environment variables with whitespace trimming
+        - Validates production environments have API keys configured (fail-fast)
+        - Falls back to production security mode if environment detection fails
+        - Creates default metadata for keys when user tracking is enabled
+        - Supports development mode when no keys are configured
+        - Provides O(1) key validation using set-based lookups
+        - Logs security events and warnings for operational monitoring
+        - Enforces production security requirements via environment detection with fallback
+    
     Usage:
-        # Basic API key authentication
+        # Basic authentication setup
         auth = APIKeyAuth()
-        api_key = await auth.authenticate("Bearer sk-abc123")
-        
-        # Advanced usage with user context
+        is_valid = auth.verify_api_key("sk-1234567890abcdef")
+    
+        # Production deployment (with environment detection)
+        # Automatically validates that API keys are configured in production
+        auth = APIKeyAuth()  # Raises ConfigurationError if no keys in production
+    
+        # Advanced usage with custom configuration
         config = AuthConfig()
         auth = APIKeyAuth(config)
-        if config.supports_user_context:
-            user_info = auth.get_user_context(api_key)
-            
-        # Extension with metadata
-        auth.add_key_metadata("sk-abc123", {
-            "user_id": "user_123",
-            "permissions": ["read", "write"]
+    
+        # Add custom metadata for enhanced features
+        if config.enable_user_tracking:
+            metadata = auth.get_key_metadata("sk-1234567890abcdef")
+            print(f"Key type: {metadata.get('type')}")
+    
+        # Runtime key management
+        auth.reload_keys()  # Refresh from environment variables
+    
+        # Request-specific metadata generation
+        request_data = auth.add_request_metadata("sk-key", {
+            "timestamp": "2024-01-01T00:00:00Z",
+            "endpoint": "/api/process",
+            "method": "POST"
         })
     """
 
@@ -294,28 +398,115 @@ class APIKeyAuth:
 
     def reload_keys(self):
         """
-        Reload API keys from environment (useful for runtime updates).
+        Reload API keys from environment variables with metadata consistency.
+        
+        Performs complete key and metadata refresh from current environment state,
+        ensuring consistent state between api_keys and _key_metadata.
+        
+        Behavior:
+            - Clears existing metadata to prevent orphaned entries
+            - Reloads API keys from current environment variables
+            - Regenerates metadata for new keys when user tracking is enabled
+            - Maintains consistent state between keys and metadata
+            - Logs reload completion for operational monitoring
+        
+        Use Cases:
+            - Runtime key rotation without application restart
+            - Adding or removing API keys during operation
+            - Updating key metadata after configuration changes
+        
+        Thread Safety:
+            Use carefully in multi-threaded environments as this modifies
+            internal state. Consider implementing proper locking if needed.
         """
         ...
 
 
-async def verify_api_key(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> str:
+def get_api_key_from_request(request: Request, bearer_credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> tuple[Optional[str], str]:
     """
-    Dependency to verify API key authentication.
+    Extract API key from either Authorization Bearer or X-API-Key header.
+    
+    Supports both authentication methods:
+    - Authorization: Bearer <key>
+    - X-API-Key: <key>
     
     Args:
-        credentials: HTTP Bearer credentials from the request
-        
+        request: FastAPI Request object containing headers
+        bearer_credentials: HTTP Bearer credentials from Authorization header
+    
     Returns:
-        The verified API key
-        
+        tuple: (api_key: Optional[str], auth_method: str)
+               - api_key: The extracted API key or None if not found
+               - auth_method: "bearer_token", "x_api_key", or "none"
+    """
+    ...
+
+
+async def verify_api_key(request: Request, bearer_credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> str:
+    """
+    Environment-aware FastAPI dependency for API key authentication with production security.
+    
+    Validates API key authentication with environment-aware security enforcement,
+    development mode support, and comprehensive error context for operational debugging.
+    
+    Args:
+        request: FastAPI Request object containing headers for X-API-Key support
+        bearer_credentials: HTTP Bearer authorization credentials from request headers.
+                           Expected format: "Bearer sk-1234567890abcdef" or None for missing auth.
+                           Automatically injected by FastAPI's HTTPBearer security scheme.
+    
+    Returns:
+        str: The validated API key string when authentication succeeds.
+             Returns "development" when no keys configured in development environments.
+             Never returns None - always authenticates or raises exception.
+    
     Raises:
-        AuthenticationError: If authentication fails (missing or invalid API key)
+        AuthenticationError: When authentication fails with detailed context including:
+                           - Missing credentials when API keys are configured
+                           - Invalid API key format or unrecognized key value
+                           - Environment detection information and confidence scores
+                           - Authentication method and credential status for debugging
+    
+    Behavior:
+        - Returns "development" immediately if no API keys are configured (development mode)
+        - Requires valid credentials when API keys are configured in any environment
+        - Validates provided API key against all configured valid keys (O(1) lookup)
+        - Includes environment detection context in all error messages and logging
+        - Logs security events for monitoring: warnings for no keys, invalid attempts
+        - Preserves authentication flow for development, staging, and production environments
+        - Fails fast with clear error messages guiding proper configuration
+        - Thread-safe for concurrent request handling across multiple workers
+    
+    Examples:
+        >>> # FastAPI endpoint with authentication
+        >>> @app.get("/protected")
+        >>> async def protected_route(api_key: str = Depends(verify_api_key)):
+        ...     return {"message": "authenticated", "key_prefix": api_key[:8]}
+    
+        >>> # Development mode (no keys configured)
+        >>> # GET /protected without Authorization header
+        >>> # Returns: api_key = "development"
+    
+        >>> # Production mode with valid Bearer token
+        >>> # GET /protected with "Authorization: Bearer sk-1234567890abcdef"
+        >>> # Returns: api_key = "sk-1234567890abcdef"
+    
+        >>> # Production mode with valid X-API-Key header
+        >>> # GET /protected with "X-API-Key: sk-1234567890abcdef"
+        >>> # Returns: api_key = "sk-1234567890abcdef"
+    
+        >>> # Invalid authentication attempt
+        >>> # GET /protected with "Authorization: Bearer invalid-key"
+        >>> # Raises: AuthenticationError("Invalid API key", context={...})
+    
+        >>> # Missing credentials in production
+        >>> # GET /protected without Authorization or X-API-Key headers (keys configured)
+        >>> # Raises: AuthenticationError("API key required...", context={...})
     """
     ...
 
 
-async def verify_api_key_with_metadata(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> Dict[str, Any]:
+async def verify_api_key_with_metadata(request: Request, bearer_credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> Dict[str, Any]:
     """
     Enhanced dependency that returns API key with metadata (extension point).
     
@@ -325,13 +516,14 @@ async def verify_api_key_with_metadata(credentials: Optional[HTTPAuthorizationCr
     ...
 
 
-async def optional_verify_api_key(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> Optional[str]:
+async def optional_verify_api_key(request: Request, bearer_credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> Optional[str]:
     """
     Optional dependency to verify API key authentication.
     Returns None if no credentials provided, otherwise verifies the key.
     
     Args:
-        credentials: HTTP Bearer credentials from the request
+        request: FastAPI Request object containing headers for X-API-Key support
+        bearer_credentials: HTTP Bearer credentials from the request
         
     Returns:
         The verified API key or None if no credentials provided
@@ -344,23 +536,98 @@ async def optional_verify_api_key(credentials: Optional[HTTPAuthorizationCredent
 
 def verify_api_key_string(api_key: str) -> bool:
     """
-    Manually verify an API key string.
+    Validate an API key string without HTTP request context for programmatic verification.
+    
+    Provides direct API key validation for non-HTTP contexts such as batch processing,
+    background tasks, or programmatic authentication checks without FastAPI dependencies.
     
     Args:
-        api_key: The API key to verify
-        
+        api_key: The API key string to validate.
+                Must be complete API key (e.g., "sk-1234567890abcdef").
+                Empty strings, None, or malformed keys return False.
+    
     Returns:
-        True if valid, False otherwise
+        bool: True if the API key is valid and configured in the system.
+              False if invalid, empty, None, or not found in configured keys.
+              False if no API keys are configured (development mode).
+    
+    Behavior:
+        - Performs O(1) lookup against configured API key set
+        - Returns False immediately for None or empty string inputs
+        - Case-sensitive exact string matching (no normalization)
+        - Thread-safe for concurrent access from multiple contexts
+        - No logging or error raising - silent validation for programmatic use
+        - Works independently of HTTP context or FastAPI dependencies
+    
+    Examples:
+        >>> # Programmatic validation
+        >>> is_valid = verify_api_key_string("sk-1234567890abcdef")
+        >>> assert is_valid is True
+    
+        >>> # Invalid key validation
+        >>> is_valid = verify_api_key_string("invalid-key")
+        >>> assert is_valid is False
+    
+        >>> # Empty or None handling
+        >>> assert verify_api_key_string("") is False
+        >>> assert verify_api_key_string(None) is False
+    
+        >>> # Background task authentication
+        >>> def process_batch(api_key: str, data: List[str]):
+        ...     if not verify_api_key_string(api_key):
+        ...         raise ValueError("Invalid API key for batch processing")
+        ...     return process_data(data)
     """
     ...
 
 
 def get_auth_status() -> Dict[str, Any]:
     """
-    Get current authentication system status and capabilities.
+    Retrieve comprehensive authentication system status and configuration information.
+    
+    Provides operational visibility into authentication system state, configuration,
+    and capabilities for monitoring, debugging, and administrative purposes.
     
     Returns:
-        Dictionary with auth system information
+        Dict[str, Any]: Authentication system status containing:
+                       - auth_config: dict with mode, user tracking, and request logging status
+                       - api_keys_configured: int count of configured API keys
+                       - development_mode: bool indicating if running without authentication
+    
+    Behavior:
+        - Returns current authentication configuration from AuthConfig instance
+        - Counts configured API keys without exposing key values
+        - Determines development mode based on API key configuration
+        - Provides snapshot of current system state (not live monitoring)
+        - Thread-safe for concurrent access from monitoring endpoints
+        - Safe for logging and status endpoints (no sensitive information exposed)
+    
+    Examples:
+        >>> # System status check
+        >>> status = get_auth_status()
+        >>> print(f"Mode: {status['auth_config']['mode']}")
+        >>> print(f"Keys configured: {status['api_keys_configured']}")
+    
+        >>> # Development environment check
+        >>> status = get_auth_status()
+        >>> if status['development_mode']:
+        ...     print("WARNING: Running in development mode without authentication")
+    
+        >>> # Monitoring endpoint usage
+        >>> @app.get("/internal/auth/status")
+        >>> async def auth_status_endpoint():
+        ...     return get_auth_status()
+    
+        >>> # Expected status structure:
+        >>> {
+        ...     "auth_config": {
+        ...         "mode": "simple",
+        ...         "user_tracking": False,
+        ...         "request_logging": False
+        ...     },
+        ...     "api_keys_configured": 3,
+        ...     "development_mode": False
+        ... }
     """
     ...
 
@@ -385,21 +652,72 @@ def supports_feature(feature: str) -> bool:
     ...
 
 
-async def verify_api_key_http(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> str:
+async def verify_api_key_http(request: Request, bearer_credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)) -> str:
     """
-    Dependency wrapper that converts AuthenticationError to HTTPException.
+    FastAPI-compatible authentication dependency with HTTP exception handling.
     
-    This wrapper catches AuthenticationError exceptions from verify_api_key and
-    converts them to HTTPException which FastAPI handles more gracefully with
-    the middleware stack, avoiding "response already started" conflicts.
+    Provides the same authentication functionality as verify_api_key but converts
+    AuthenticationError exceptions to HTTPException for proper FastAPI middleware
+    compatibility and standardized HTTP error responses.
     
     Args:
-        credentials: HTTP Bearer credentials from the request
-        
+        request: FastAPI Request object containing headers for X-API-Key support
+        bearer_credentials: HTTP Bearer authorization credentials from request headers.
+                           Expected format: "Bearer sk-1234567890abcdef" or None for missing auth.
+                           Automatically injected by FastAPI's HTTPBearer security scheme.
+    
     Returns:
-        The verified API key
-        
+        str: The validated API key string when authentication succeeds.
+             Returns "development" when no keys configured in development environments.
+             Never returns None - always authenticates or raises HTTPException.
+    
     Raises:
-        HTTPException: 401 Unauthorized if authentication fails
+        HTTPException: 401 Unauthorized when authentication fails, containing:
+                      - Structured error detail with message and context information
+                      - WWW-Authenticate header for proper HTTP authentication flow
+                      - Environment detection context for operational debugging
+                      - Original exception context preserved for troubleshooting
+    
+    Behavior:
+        - Delegates authentication logic to verify_api_key for consistency
+        - Converts AuthenticationError to HTTPException for middleware compatibility
+        - Preserves all authentication context and environment information
+        - Returns proper HTTP 401 status with WWW-Authenticate header
+        - Provides structured error response format for API clients
+        - Maintains compatibility with FastAPI middleware and error handling
+        - Prevents "response already started" conflicts in middleware stack
+        - Thread-safe for concurrent request processing
+    
+    Examples:
+        >>> # Recommended FastAPI endpoint authentication
+        >>> @app.get("/api/data")
+        >>> async def get_data(api_key: str = Depends(verify_api_key_http)):
+        ...     return {"data": "protected", "authenticated_key": api_key[:8]}
+    
+        >>> # Supports both Bearer token and X-API-Key header:
+        >>> # GET /api/data with "Authorization: Bearer sk-1234567890abcdef"
+        >>> # GET /api/data with "X-API-Key: sk-1234567890abcdef"
+    
+        >>> # HTTP error response for invalid authentication
+        >>> # GET /api/data with "Authorization: Bearer invalid-key" or "X-API-Key: invalid-key"
+        >>> # Returns: 401 Unauthorized
+        >>> # {
+        >>> #   "detail": {
+        >>> #     "message": "Invalid API key",
+        >>> #     "context": {
+        >>> #       "auth_method": "bearer_token",
+        >>> #       "environment": "development",
+        >>> #       "credentials_provided": true
+        >>> #     }
+        >>> #   }
+        >>> # }
+    
+        >>> # Success response with valid authentication
+        >>> # GET /api/data with "Authorization: Bearer sk-1234567890abcdef"
+        >>> # Returns: 200 OK with authenticated content
+    
+        >>> # Missing credentials error response
+        >>> # GET /api/data without Authorization or X-API-Key headers (keys configured)
+        >>> # Returns: 401 Unauthorized with "API key required" message
     """
     ...
diff --git a/backend/contracts/main.pyi b/backend/contracts/main.pyi
index 9310ffa..78a0003 100644
--- a/backend/contracts/main.pyi
+++ b/backend/contracts/main.pyi
@@ -190,6 +190,7 @@ from fastapi.openapi.docs import get_swagger_ui_html
 from app.core.config import settings
 from app.core.middleware import setup_middleware, setup_enhanced_middleware
 from app.infrastructure.security import verify_api_key
+from app.core.environment import is_production_environment, get_environment_info, FeatureContext
 from app.api.v1.health import health_router
 from app.api.v1.auth import auth_router
 from app.api.v1.text_processing import router as text_processing_router
